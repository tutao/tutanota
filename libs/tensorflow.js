/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
const EPSILON_FLOAT32 = 1e-7;
const EPSILON_FLOAT16 = 1e-4;

/** Convenient class for storing tensor-related data. */
class DataStorage {
	constructor(backend, dataMover) {
		this.backend = backend;
		this.dataMover = dataMover;
		this.data = new WeakMap();
		this.dataIdsCount = 0;
	}

	get(dataId) {
		if (!this.data.has(dataId)) {
			this.dataMover.moveData(this.backend, dataId);
		}
		return this.data.get(dataId);
	}

	set(dataId, value) {
		this.dataIdsCount++;
		this.data.set(dataId, value);
	}

	has(dataId) {
		return this.data.has(dataId);
	}

	delete(dataId) {
		this.dataIdsCount--;
		return this.data.delete(dataId);
	}

	numDataIds() {
		return this.dataIdsCount;
	}
}

/**
 * The interface that defines the kernels that should be implemented when
 * adding a new backend. New backends don't need to implement every one of the
 * methods, this can be done gradually (throw an error for unimplemented
 * methods).
 */
class KernelBackend {
	refCount(dataId) {
		return notYetImplemented('refCount');
	}

	incRef(dataId) {
		return notYetImplemented('incRef');
	}

	timerAvailable() {
		return true;
	}

	time(f) {
		return notYetImplemented('time');
	}

	read(dataId) {
		return notYetImplemented('read');
	}

	readSync(dataId) {
		return notYetImplemented('readSync');
	}

	readToGPU(dataId, options) {
		return notYetImplemented('readToGPU');
	}

	numDataIds() {
		return notYetImplemented('numDataIds');
	}

	disposeData(dataId, force) {
		return notYetImplemented('disposeData');
	}

	write(values, shape, dtype) {
		return notYetImplemented('write');
	}

	move(dataId, values, shape, dtype, refCount) {
		return notYetImplemented('move');
	}

	createTensorFromGPUData(values, shape, dtype) {
		return notYetImplemented('createTensorFromGPUData');
	}

	memory() {
		return notYetImplemented('memory');
	}

	/** Returns the highest precision for floats in bits (e.g. 16 or 32) */
	floatPrecision() {
		return notYetImplemented('floatPrecision');
	}

	/** Returns the smallest representable number.  */
	epsilon() {
		return this.floatPrecision() === 32 ? EPSILON_FLOAT32 : EPSILON_FLOAT16;
	}

	dispose() {
		return notYetImplemented('dispose');
	}
}

function notYetImplemented(kernelName) {
	throw new Error(`'${kernelName}' not yet implemented or not found in the registry. ` +
		`This kernel may not be supported by the tfjs backend you have chosen`);
}

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Shuffles the array in-place using Fisher-Yates algorithm.
 *
 * ```js
 * const a = [1, 2, 3, 4, 5];
 * tf.util.shuffle(a);
 * console.log(a);
 * ```
 *
 * @param array The array to shuffle in-place.
 *
 * @doc {heading: 'Util', namespace: 'util'}
 */
// tslint:disable-next-line:no-any
function shuffle(array) {
	let counter = array.length;
	let index = 0;
	// While there are elements in the array
	while (counter > 0) {
		// Pick a random index
		index = (Math.random() * counter) | 0;
		// Decrease counter by 1
		counter--;
		// And swap the last element with it
		swap(array, counter, index);
	}
}

/**
 * Shuffles two arrays in-place the same way using Fisher-Yates algorithm.
 *
 * ```js
 * const a = [1,2,3,4,5];
 * const b = [11,22,33,44,55];
 * tf.util.shuffleCombo(a, b);
 * console.log(a, b);
 * ```
 *
 * @param array The first array to shuffle in-place.
 * @param array2 The second array to shuffle in-place with the same permutation
 *     as the first array.
 *
 * @doc {heading: 'Util', namespace: 'util'}
 */
function shuffleCombo(
// tslint:disable-next-line:no-any
array,
// tslint:disable-next-line:no-any
array2) {
	if (array.length !== array2.length) {
		throw new Error(`Array sizes must match to be shuffled together ` +
			`First array length was ${array.length}` +
			`Second array length was ${array2.length}`);
	}
	let counter = array.length;
	let index = 0;
	// While there are elements in the array
	while (counter > 0) {
		// Pick a random index
		index = (Math.random() * counter) | 0;
		// Decrease counter by 1
		counter--;
		// And swap the last element of each array with it
		swap(array, counter, index);
		swap(array2, counter, index);
	}
}

/** Clamps a value to a specified range. */
function clamp(min, x, max) {
	return Math.max(min, Math.min(x, max));
}

function nearestLargerEven(val) {
	return val % 2 === 0 ? val : val + 1;
}

function swap(object, left, right) {
	const temp = object[left];
	object[left] = object[right];
	object[right] = temp;
}

function sum$2(arr) {
	let sum = 0;
	for (let i = 0; i < arr.length; i++) {
		sum += arr[i];
	}
	return sum;
}

/**
 * Returns a sample from a uniform [a, b) distribution.
 *
 * @param a The minimum support (inclusive).
 * @param b The maximum support (exclusive).
 * @return A pseudorandom number on the half-open interval [a,b).
 */
function randUniform(a, b) {
	const r = Math.random();
	return (b * r) + (1 - r) * a;
}

/** Returns the squared Euclidean distance between two vectors. */
function distSquared(a, b) {
	let result = 0;
	for (let i = 0; i < a.length; i++) {
		const diff = Number(a[i]) - Number(b[i]);
		result += diff * diff;
	}
	return result;
}

/**
 * Asserts that the expression is true. Otherwise throws an error with the
 * provided message.
 *
 * ```js
 * const x = 2;
 * tf.util.assert(x === 2, 'x is not 2');
 * ```
 *
 * @param expr The expression to assert (as a boolean).
 * @param msg A function that returns the message to report when throwing an
 *     error. We use a function for performance reasons.
 *
 * @doc {heading: 'Util', namespace: 'util'}
 */
function assert(expr, msg) {
	if (!expr) {
		throw new Error(typeof msg === 'string' ? msg : msg());
	}
}

function assertShapesMatch(shapeA, shapeB, errorMessagePrefix = '') {
	assert(arraysEqual(shapeA, shapeB), () => errorMessagePrefix + ` Shapes ${shapeA} and ${shapeB} must match`);
}

function assertNonNull(a) {
	assert(a != null, () => `The input to the tensor constructor must be a non-null value.`);
}

/**
 * Returns the size (number of elements) of the tensor given its shape.
 *
 * ```js
 * const shape = [3, 4, 2];
 * const size = tf.util.sizeFromShape(shape);
 * console.log(size);
 * ```
 *
 * @doc {heading: 'Util', namespace: 'util'}
 */
function sizeFromShape(shape) {
	if (shape.length === 0) {
		// Scalar.
		return 1;
	}
	let size = shape[0];
	for (let i = 1; i < shape.length; i++) {
		size *= shape[i];
	}
	return size;
}

function isScalarShape(shape) {
	return shape.length === 0;
}

function arraysEqualWithNull(n1, n2) {
	if (n1 === n2) {
		return true;
	}
	if (n1 == null || n2 == null) {
		return false;
	}
	if (n1.length !== n2.length) {
		return false;
	}
	for (let i = 0; i < n1.length; i++) {
		if (n1[i] !== null && n2[i] !== null && n1[i] !== n2[i]) {
			return false;
		}
	}
	return true;
}

function arraysEqual(n1, n2) {
	if (n1 === n2) {
		return true;
	}
	if (n1 == null || n2 == null) {
		return false;
	}
	if (n1.length !== n2.length) {
		return false;
	}
	for (let i = 0; i < n1.length; i++) {
		if (n1[i] !== n2[i]) {
			return false;
		}
	}
	return true;
}

function isInt(a) {
	return a % 1 === 0;
}

function tanh$1(x) {
	// tslint:disable-next-line:no-any
	if (Math.tanh != null) {
		// tslint:disable-next-line:no-any
		return Math.tanh(x);
	}
	if (x === Infinity) {
		return 1;
	} else if (x === -Infinity) {
		return -1;
	} else {
		const e2x = Math.exp(2 * x);
		return (e2x - 1) / (e2x + 1);
	}
}

function sizeToSquarishShape(size) {
	const width = Math.ceil(Math.sqrt(size));
	return [width, Math.ceil(size / width)];
}

/**
 * Creates a new array with randomized indices to a given quantity.
 *
 * ```js
 * const randomTen = tf.util.createShuffledIndices(10);
 * console.log(randomTen);
 * ```
 *
 * @param number Quantity of how many shuffled indices to create.
 *
 * @doc {heading: 'Util', namespace: 'util'}
 */
function createShuffledIndices(n) {
	const shuffledIndices = new Uint32Array(n);
	for (let i = 0; i < n; ++i) {
		shuffledIndices[i] = i;
	}
	shuffle(shuffledIndices);
	return shuffledIndices;
}

function rightPad(a, size) {
	if (size <= a.length) {
		return a;
	}
	return a + ' '.repeat(size - a.length);
}

function repeatedTry(checkFn, delayFn = (counter) => 0, maxCounter, scheduleFn) {
	return new Promise((resolve, reject) => {
		let tryCount = 0;
		const tryFn = () => {
			if (checkFn()) {
				resolve();
				return;
			}
			tryCount++;
			const nextBackoff = delayFn(tryCount);
			if (maxCounter != null && tryCount >= maxCounter) {
				reject();
				return;
			}
			if (scheduleFn != null) {
				scheduleFn(tryFn, nextBackoff);
			} else {
				// google3 does not allow assigning another variable to setTimeout.
				// Don't refactor this so scheduleFn has a default value of setTimeout.
				setTimeout(tryFn, nextBackoff);
			}
		};
		tryFn();
	});
}

/**
 * Given the full size of the array and a shape that may contain -1 as the
 * implicit dimension, returns the inferred shape where -1 is replaced.
 * E.g. For shape=[2, -1, 3] and size=24, it will return [2, 4, 3].
 *
 * @param shape The shape, which may contain -1 in some dimension.
 * @param size The full size (number of elements) of the array.
 * @return The inferred shape where -1 is replaced with the inferred size.
 */
function inferFromImplicitShape(shape, size) {
	let shapeProd = 1;
	let implicitIdx = -1;
	for (let i = 0; i < shape.length; ++i) {
		if (shape[i] >= 0) {
			shapeProd *= shape[i];
		} else if (shape[i] === -1) {
			if (implicitIdx !== -1) {
				throw Error(`Shapes can only have 1 implicit size. ` +
					`Found -1 at dim ${implicitIdx} and dim ${i}`);
			}
			implicitIdx = i;
		} else if (shape[i] < 0) {
			throw Error(`Shapes can not be < 0. Found ${shape[i]} at dim ${i}`);
		}
	}
	if (implicitIdx === -1) {
		if (size > 0 && size !== shapeProd) {
			throw Error(`Size(${size}) must match the product of shape ${shape}`);
		}
		return shape;
	}
	if (shapeProd === 0) {
		throw Error(`Cannot infer the missing size in [${shape}] when ` +
			`there are 0 elements`);
	}
	if (size % shapeProd !== 0) {
		throw Error(`The implicit shape can't be a fractional number. ` +
			`Got ${size} / ${shapeProd}`);
	}
	const newShape = shape.slice();
	newShape[implicitIdx] = size / shapeProd;
	return newShape;
}

function parseAxisParam(axis, shape) {
	const rank = shape.length;
	// Normalize input
	axis = axis == null ? shape.map((s, i) => i) : [].concat(axis);
	// Check for valid range
	assert(axis.every(ax => ax >= -rank && ax < rank), () => `All values in axis param must be in range [-${rank}, ${rank}) but ` +
		`got axis ${axis}`);
	// Check for only integers
	assert(axis.every(ax => isInt(ax)), () => `All values in axis param must be integers but ` +
		`got axis ${axis}`);
	// Handle negative axis.
	return axis.map(a => a < 0 ? rank + a : a);
}

/** Reduces the shape by removing all dimensions of shape 1. */
function squeezeShape(shape, axis) {
	const newShape = [];
	const keptDims = [];
	const isEmptyArray = axis != null && Array.isArray(axis) && axis.length === 0;
	const axes = (axis == null || isEmptyArray) ?
		null :
		parseAxisParam(axis, shape).sort();
	let j = 0;
	for (let i = 0; i < shape.length; ++i) {
		if (axes != null) {
			if (axes[j] === i && shape[i] !== 1) {
				throw new Error(`Can't squeeze axis ${i} since its dim '${shape[i]}' is not 1`);
			}
			if ((axes[j] == null || axes[j] > i) && shape[i] === 1) {
				newShape.push(shape[i]);
				keptDims.push(i);
			}
			if (axes[j] <= i) {
				j++;
			}
		}
		if (shape[i] !== 1) {
			newShape.push(shape[i]);
			keptDims.push(i);
		}
	}
	return {newShape, keptDims};
}

function getTypedArrayFromDType(dtype, size) {
	return getArrayFromDType(dtype, size);
}

function getArrayFromDType(dtype, size) {
	let values = null;
	if (dtype == null || dtype === 'float32') {
		values = new Float32Array(size);
	} else if (dtype === 'int32') {
		values = new Int32Array(size);
	} else if (dtype === 'bool') {
		values = new Uint8Array(size);
	} else if (dtype === 'string') {
		values = new Array(size);
	} else {
		throw new Error(`Unknown data type ${dtype}`);
	}
	return values;
}

function checkConversionForErrors(vals, dtype) {
	for (let i = 0; i < vals.length; i++) {
		const num = vals[i];
		if (isNaN(num) || !isFinite(num)) {
			throw Error(`A tensor of type ${dtype} being uploaded contains ${num}.`);
		}
	}
}

/** Returns true if the dtype is valid. */
function isValidDtype(dtype) {
	return dtype === 'bool' || dtype === 'complex64' || dtype === 'float32' ||
		dtype === 'int32' || dtype === 'string';
}

/**
 * Returns true if the new type can't encode the old type without loss of
 * precision.
 */
function hasEncodingLoss(oldType, newType) {
	if (newType === 'complex64') {
		return false;
	}
	if (newType === 'float32' && oldType !== 'complex64') {
		return false;
	}
	if (newType === 'int32' && oldType !== 'float32' && oldType !== 'complex64') {
		return false;
	}
	if (newType === 'bool' && oldType === 'bool') {
		return false;
	}
	return true;
}

function bytesPerElement(dtype) {
	if (dtype === 'float32' || dtype === 'int32') {
		return 4;
	} else if (dtype === 'complex64') {
		return 8;
	} else if (dtype === 'bool') {
		return 1;
	} else {
		throw new Error(`Unknown dtype ${dtype}`);
	}
}

/**
 * Returns the approximate number of bytes allocated in the string array - 2
 * bytes per character. Computing the exact bytes for a native string in JS
 * is not possible since it depends on the encoding of the html page that
 * serves the website.
 */
function bytesFromStringArray(arr) {
	if (arr == null) {
		return 0;
	}
	let bytes = 0;
	arr.forEach(x => bytes += x.length);
	return bytes;
}

/** Returns true if the value is a string. */
function isString(value) {
	return typeof value === 'string' || value instanceof String;
}

function isBoolean(value) {
	return typeof value === 'boolean';
}

function isNumber(value) {
	return typeof value === 'number';
}

function inferDtype(values) {
	if (Array.isArray(values)) {
		return inferDtype(values[0]);
	}
	if (values instanceof Float32Array) {
		return 'float32';
	} else if (values instanceof Int32Array || values instanceof Uint8Array ||
		values instanceof Uint8ClampedArray) {
		return 'int32';
	} else if (isNumber(values)) {
		return 'float32';
	} else if (isString(values)) {
		return 'string';
	} else if (isBoolean(values)) {
		return 'bool';
	}
	return 'float32';
}

function isFunction(f) {
	return !!(f && f.constructor && f.call && f.apply);
}

function nearestDivisor(size, start) {
	for (let i = start; i < size; ++i) {
		if (size % i === 0) {
			return i;
		}
	}
	return size;
}

function computeStrides(shape) {
	const rank = shape.length;
	if (rank < 2) {
		return [];
	}
	// Last dimension has implicit stride of 1, thus having D-1 (instead of D)
	// strides.
	const strides = new Array(rank - 1);
	strides[rank - 2] = shape[rank - 1];
	for (let i = rank - 3; i >= 0; --i) {
		strides[i] = strides[i + 1] * shape[i + 1];
	}
	return strides;
}

function createNestedArray(offset, shape, a, isComplex = false) {
	const ret = new Array();
	if (shape.length === 1) {
		const d = shape[0] * (isComplex ? 2 : 1);
		for (let i = 0; i < d; i++) {
			ret[i] = a[offset + i];
		}
	} else {
		const d = shape[0];
		const rest = shape.slice(1);
		const len = rest.reduce((acc, c) => acc * c) * (isComplex ? 2 : 1);
		for (let i = 0; i < d; i++) {
			ret[i] = createNestedArray(offset + i * len, rest, a, isComplex);
		}
	}
	return ret;
}

// Provide a nested array of TypedArray in given shape.
function toNestedArray(shape, a, isComplex = false) {
	if (shape.length === 0) {
		// Scalar type should return a single number.
		return a[0];
	}
	const size = shape.reduce((acc, c) => acc * c) * (isComplex ? 2 : 1);
	if (size === 0) {
		// A tensor with shape zero should be turned into empty list.
		return [];
	}
	if (size !== a.length) {
		throw new Error(`[${shape}] does not match the input size ${a.length}${isComplex ? ' for a complex tensor' : ''}.`);
	}
	return createNestedArray(0, shape, a, isComplex);
}

function convertBackendValuesAndArrayBuffer(data, dtype) {
	// If is type Uint8Array[], return it directly.
	if (Array.isArray(data)) {
		return data;
	}
	if (dtype === 'float32') {
		return data instanceof Float32Array ? data : new Float32Array(data);
	} else if (dtype === 'int32') {
		return data instanceof Int32Array ? data : new Int32Array(data);
	} else if (dtype === 'bool' || dtype === 'string') {
		return Uint8Array.from(new Int32Array(data));
	} else {
		throw new Error(`Unknown dtype ${dtype}`);
	}
}

function makeOnesTypedArray(size, dtype) {
	const array = makeZerosTypedArray(size, dtype);
	for (let i = 0; i < array.length; i++) {
		array[i] = 1;
	}
	return array;
}

function makeZerosTypedArray(size, dtype) {
	if (dtype == null || dtype === 'float32' || dtype === 'complex64') {
		return new Float32Array(size);
	} else if (dtype === 'int32') {
		return new Int32Array(size);
	} else if (dtype === 'bool') {
		return new Uint8Array(size);
	} else {
		throw new Error(`Unknown data type ${dtype}`);
	}
}

/**
 * Make nested `TypedArray` filled with zeros.
 * @param shape The shape information for the nested array.
 * @param dtype dtype of the array element.
 */
function makeZerosNestedTypedArray(shape, dtype) {
	const size = shape.reduce((prev, curr) => prev * curr, 1);
	if (dtype == null || dtype === 'float32') {
		return toNestedArray(shape, new Float32Array(size));
	} else if (dtype === 'int32') {
		return toNestedArray(shape, new Int32Array(size));
	} else if (dtype === 'bool') {
		return toNestedArray(shape, new Uint8Array(size));
	} else {
		throw new Error(`Unknown data type ${dtype}`);
	}
}

function assertNonNegativeIntegerDimensions(shape) {
	shape.forEach(dimSize => {
		assert(Number.isInteger(dimSize) && dimSize >= 0, () => `Tensor must have a shape comprised of positive integers but got ` +
			`shape [${shape}].`);
	});
}

/**
 * Computes flat index for a given location (multidimentionsal index) in a
 * Tensor/multidimensional array.
 *
 * @param locs Location in the tensor.
 * @param rank Rank of the tensor.
 * @param strides Tensor strides.
 */
function locToIndex(locs, rank, strides) {
	if (rank === 0) {
		return 0;
	} else if (rank === 1) {
		return locs[0];
	}
	let index = locs[locs.length - 1];
	for (let i = 0; i < locs.length - 1; ++i) {
		index += strides[i] * locs[i];
	}
	return index;
}

/**
 * Computes the location (multidimensional index) in a
 * tensor/multidimentional array for a given flat index.
 *
 * @param index Index in flat array.
 * @param rank Rank of tensor.
 * @param strides Strides of tensor.
 */
function indexToLoc(index, rank, strides) {
	if (rank === 0) {
		return [];
	} else if (rank === 1) {
		return [index];
	}
	const locs = new Array(rank);
	for (let i = 0; i < locs.length - 1; ++i) {
		locs[i] = Math.floor(index / strides[i]);
		index -= locs[i] * strides[i];
	}
	locs[locs.length - 1] = index;
	return locs;
}

/**
 * This method asserts whether an object is a Promise instance.
 * @param object
 */
// tslint:disable-next-line: no-any
function isPromise(object) {
	//  We chose to not use 'obj instanceOf Promise' for two reasons:
	//  1. It only reliably works for es6 Promise, not other Promise
	//  implementations.
	//  2. It doesn't work with framework that uses zone.js. zone.js monkey
	//  patch the async calls, so it is possible the obj (patched) is
	//  comparing to a pre-patched Promise.
	return object && object.then && typeof object.then === 'function';
}

/**
 * @license
 * Copyright 2017 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
// Expects flags from URL in the format ?tfjsflags=FLAG1:1,FLAG2:true.
const TENSORFLOWJS_FLAGS_PREFIX = 'tfjsflags';

/**
 * The environment contains evaluated flags as well as the registered platform.
 * This is always used as a global singleton and can be retrieved with
 * `tf.env()`.
 *
 * @doc {heading: 'Environment'}
 */
class Environment {
	// tslint:disable-next-line: no-any
	constructor(global) {
		this.global = global;
		this.flags = {};
		this.flagRegistry = {};
		this.urlFlags = {};
		// Jasmine spies on this in 'environment_test.ts'
		this.getQueryParams = getQueryParams;
		this.populateURLFlags();
	}

	setPlatform(platformName, platform) {
		if (this.platform != null) {
			if (!(env().getBool('IS_TEST') || env().getBool('PROD'))) {
				console.warn(`Platform ${this.platformName} has already been set. ` +
					`Overwriting the platform with ${platformName}.`);
			}
		}
		this.platformName = platformName;
		this.platform = platform;
	}

	registerFlag(flagName, evaluationFn, setHook) {
		this.flagRegistry[flagName] = {evaluationFn, setHook};
		// Override the flag value from the URL. This has to happen here because
		// the environment is initialized before flags get registered.
		if (this.urlFlags[flagName] != null) {
			const flagValue = this.urlFlags[flagName];
			if (!(env().getBool('IS_TEST') || env().getBool('PROD'))) {
				console.warn(`Setting feature override from URL ${flagName}: ${flagValue}.`);
			}
			this.set(flagName, flagValue);
		}
	}

	async getAsync(flagName) {
		if (flagName in this.flags) {
			return this.flags[flagName];
		}
		this.flags[flagName] = await this.evaluateFlag(flagName);
		return this.flags[flagName];
	}

	get(flagName) {
		if (flagName in this.flags) {
			return this.flags[flagName];
		}
		const flagValue = this.evaluateFlag(flagName);
		if (isPromise(flagValue)) {
			throw new Error(`Flag ${flagName} cannot be synchronously evaluated. ` +
				`Please use getAsync() instead.`);
		}
		this.flags[flagName] = flagValue;
		return this.flags[flagName];
	}

	getNumber(flagName) {
		return this.get(flagName);
	}

	getBool(flagName) {
		return this.get(flagName);
	}

	getString(flagName) {
		return this.get(flagName);
	}

	getFlags() {
		return this.flags;
	}

	// For backwards compatibility.
	get features() {
		return this.flags;
	}

	set(flagName, value) {
		if (this.flagRegistry[flagName] == null) {
			throw new Error(`Cannot set flag ${flagName} as it has not been registered.`);
		}
		this.flags[flagName] = value;
		if (this.flagRegistry[flagName].setHook != null) {
			this.flagRegistry[flagName].setHook(value);
		}
	}

	evaluateFlag(flagName) {
		if (this.flagRegistry[flagName] == null) {
			throw new Error(`Cannot evaluate flag '${flagName}': no evaluation function found.`);
		}
		return this.flagRegistry[flagName].evaluationFn();
	}

	setFlags(flags) {
		this.flags = Object.assign({}, flags);
	}

	reset() {
		this.flags = {};
		this.urlFlags = {};
		this.populateURLFlags();
	}

	populateURLFlags() {
		if (typeof this.global === 'undefined' ||
			typeof this.global.location === 'undefined' ||
			typeof this.global.location.search === 'undefined') {
			return;
		}
		const urlParams = this.getQueryParams(this.global.location.search);
		if (TENSORFLOWJS_FLAGS_PREFIX in urlParams) {
			const keyValues = urlParams[TENSORFLOWJS_FLAGS_PREFIX].split(',');
			keyValues.forEach(keyValue => {
				const [key, value] = keyValue.split(':');
				this.urlFlags[key] = parseValue(key, value);
			});
		}
	}
}

function getQueryParams(queryString) {
	const params = {};
	queryString.replace(/[?&]([^=?&]+)(?:=([^&]*))?/g, (s, ...t) => {
		decodeParam(params, t[0], t[1]);
		return t.join('=');
	});
	return params;
}

function decodeParam(params, name, value) {
	params[decodeURIComponent(name)] = decodeURIComponent(value || '');
}

function parseValue(flagName, value) {
	const lowerCaseValue = value.toLowerCase();
	if (lowerCaseValue === 'true' || lowerCaseValue === 'false') {
		return lowerCaseValue === 'true';
	} else if (`${+lowerCaseValue}` === lowerCaseValue) {
		return +lowerCaseValue;
	} else {
		return value;
	}
}

/**
 * Returns the current environment (a global singleton).
 *
 * The environment object contains the evaluated feature values as well as the
 * active platform.
 *
 * @doc {heading: 'Environment'}
 */
function env() {
	return ENV$1;
}

let ENV$1 = null;

function setEnvironmentGlobal(environment) {
	ENV$1 = environment;
}

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
// Note that the identifier globalNameSpace is scoped to this module, but will
// always resolve to the same global object regardless of how the module is
// resolved.
// tslint:disable-next-line:no-any
let globalNameSpace;

// tslint:disable-next-line:no-any
function getGlobalNamespace() {
	if (globalNameSpace == null) {
		// tslint:disable-next-line:no-any
		let ns;
		if (typeof (window) !== 'undefined') {
			ns = window;
		} else if (typeof (global) !== 'undefined') {
			ns = global;
		} else if (typeof (process) !== 'undefined') {
			ns = process;
		} else if (typeof (self) !== 'undefined') {
			ns = self;
		} else {
			throw new Error('Could not find a global object');
		}
		globalNameSpace = ns;
	}
	return globalNameSpace;
}

// tslint:disable-next-line:no-any
function getGlobalMap() {
	const ns = getGlobalNamespace();
	if (ns._tfGlobals == null) {
		ns._tfGlobals = new Map();
	}
	return ns._tfGlobals;
}

/**
 * Returns a globally accessible 'singleton' object.
 *
 * @param key the name of the object
 * @param init a function to initialize to initialize this object
 *             the first time it is fetched.
 */
function getGlobal(key, init) {
	const globalMap = getGlobalMap();
	if (globalMap.has(key)) {
		return globalMap.get(key);
	} else {
		const singleton = init();
		globalMap.set(key, singleton);
		return globalMap.get(key);
	}
}

const Abs = 'Abs';
const Acos = 'Acos';
const Acosh = 'Acosh';
const Add = 'Add';
const AddN = 'AddN';
const All = 'All';
const Any = 'Any';
const ArgMax = 'ArgMax';
const ArgMin = 'ArgMin';
const Asin = 'Asin';
const Asinh = 'Asinh';
const Atan = 'Atan';
const Atanh = 'Atanh';
const Atan2 = 'Atan2';
const AvgPool = 'AvgPool';
const AvgPoolGrad = 'AvgPoolGrad';
const AvgPool3D = 'AvgPool3D';
const AvgPool3DGrad = 'AvgPool3DGrad';
const BatchMatMul = 'BatchMatMul';
const BatchToSpaceND = 'BatchToSpaceND';
const Bincount = 'Bincount';
const BitwiseAnd = 'BitwiseAnd';
const BroadcastTo = 'BroadcastTo';
const BroadcastArgs = 'BroadcastArgs';
const Cast = 'Cast';
const Ceil = 'Ceil';
const ClipByValue = 'ClipByValue';
const Complex = 'Complex';
const ComplexAbs = 'ComplexAbs';
const Concat = 'Concat';
const Conv2D = 'Conv2D';
const Conv2DBackpropFilter = 'Conv2DBackpropFilter';
const Conv2DBackpropInput = 'Conv2DBackpropInput';
const Conv3D = 'Conv3D';
const Conv3DBackpropFilterV2 = 'Conv3DBackpropFilterV2';
const Conv3DBackpropInputV2 = 'Conv3DBackpropInputV2';
const Cos = 'Cos';
const Cosh = 'Cosh';
const Cumprod = 'Cumprod';
const Cumsum = 'Cumsum';
const CropAndResize = 'CropAndResize';
const DenseBincount = 'DenseBincount';
const DepthToSpace = 'DepthToSpace';
const DepthwiseConv2dNative = 'DepthwiseConv2dNative';
const DepthwiseConv2dNativeBackpropFilter = 'DepthwiseConv2dNativeBackpropFilter';
const DepthwiseConv2dNativeBackpropInput = 'DepthwiseConv2dNativeBackpropInput';
const Diag = 'Diag';
const Dilation2D = 'Dilation2D';
const Dilation2DBackpropInput = 'Dilation2DBackpropInput';
const Dilation2DBackpropFilter = 'Dilation2DBackpropFilter';
const Draw = 'Draw';
const RealDiv = 'RealDiv';
const Einsum = 'Einsum';
const Elu = 'Elu';
const EluGrad = 'EluGrad';
const Erf = 'Erf';
const Equal = 'Equal';
const Exp = 'Exp';
const ExpandDims = 'ExpandDims';
const Expm1 = 'Expm1';
const FFT = 'FFT';
const Fill = 'Fill';
const FlipLeftRight = 'FlipLeftRight';
const Floor = 'Floor';
const FloorDiv = 'FloorDiv';
const FusedBatchNorm = 'FusedBatchNorm';
const GatherV2 = 'GatherV2';
const GatherNd = 'GatherNd';
const Greater = 'Greater';
const GreaterEqual = 'GreaterEqual';
const Identity = 'Identity';
const IFFT = 'IFFT';
const Imag = 'Imag';
const IsFinite = 'IsFinite';
const IsInf = 'IsInf';
const IsNan = 'IsNan';
const LeakyRelu = 'LeakyRelu';
const Less = 'Less';
const LessEqual = 'LessEqual';
const LinSpace = 'LinSpace';
const Log = 'Log';
const Log1p = 'Log1p';
const LogicalAnd = 'LogicalAnd';
const LogicalNot = 'LogicalNot';
const LogicalOr = 'LogicalOr';
const LogicalXor = 'LogicalXor';
const LogSoftmax = 'LogSoftmax';
const LowerBound = 'LowerBound';
const LRN = 'LRN';
const LRNGrad = 'LRNGrad';
const MatrixBandPart = 'MatrixBandPart';
const Max = 'Max';
const Maximum = 'Maximum';
const MaxPool = 'MaxPool';
const MaxPoolGrad = 'MaxPoolGrad';
const MaxPool3D = 'MaxPool3D';
const MaxPool3DGrad = 'MaxPool3DGrad';
const MaxPoolWithArgmax = 'MaxPoolWithArgmax';
const Mean = 'Mean';
const Min = 'Min';
const Minimum = 'Minimum';
const MirrorPad = 'MirrorPad';
const Mod = 'Mod';
const Multinomial = 'Multinomial';
const Multiply = 'Multiply';
const Neg = 'Neg';
const NotEqual = 'NotEqual';
const NonMaxSuppressionV3 = 'NonMaxSuppressionV3';
const NonMaxSuppressionV4 = 'NonMaxSuppressionV4';
const NonMaxSuppressionV5 = 'NonMaxSuppressionV5';
const OnesLike = 'OnesLike';
const OneHot = 'OneHot';
const Pack = 'Pack';
const PadV2 = 'PadV2';
const Pool = 'Pool';
const Pow = 'Pow';
const Prelu = 'Prelu';
const Prod = 'Prod';
const RaggedGather = 'RaggedGather';
const RaggedRange = 'RaggedRange';
const RaggedTensorToTensor = 'RaggedTensorToTensor';
const Range = 'Range';
const Real = 'Real';
const Reciprocal = 'Reciprocal';
const Relu = 'Relu';
const Reshape = 'Reshape';
const ResizeNearestNeighbor = 'ResizeNearestNeighbor';
const ResizeNearestNeighborGrad = 'ResizeNearestNeighborGrad';
const ResizeBilinear = 'ResizeBilinear';
const ResizeBilinearGrad = 'ResizeBilinearGrad';
const Relu6 = 'Relu6';
const Reverse = 'Reverse';
const Round = 'Round';
const Rsqrt = 'Rsqrt';
const ScatterNd = 'ScatterNd';
const TensorScatterUpdate = 'TensorScatterUpdate';
const SearchSorted = 'SearchSorted';
const Select = 'Select';
const Selu = 'Selu';
const Slice = 'Slice';
const Sin = 'Sin';
const Sinh = 'Sinh';
const Sign = 'Sign';
const Sigmoid = 'Sigmoid';
const Softplus = 'Softplus';
const Sqrt = 'Sqrt';
const Sum = 'Sum';
const SpaceToBatchND = 'SpaceToBatchND';
const SplitV = 'SplitV';
const Softmax = 'Softmax';
const SparseFillEmptyRows = 'SparseFillEmptyRows';
const SparseReshape = 'SparseReshape';
const SparseSegmentMean = 'SparseSegmentMean';
const SparseSegmentSum = 'SparseSegmentSum';
const SparseToDense = 'SparseToDense';
const SquaredDifference = 'SquaredDifference';
const Square = 'Square';
const StaticRegexReplace = 'StaticRegexReplace';
const StridedSlice = 'StridedSlice';
const StringNGrams = 'StringNGrams';
const StringSplit = 'StringSplit';
const StringToHashBucketFast = 'StringToHashBucketFast';
const Sub = 'Sub';
const Tan = 'Tan';
const Tanh = 'Tanh';
const Tile = 'Tile';
const TopK = 'TopK';
const Transform = 'Transform';
const Transpose = 'Transpose';
const Unique = 'Unique';
const Unpack = 'Unpack';
const UnsortedSegmentSum = 'UnsortedSegmentSum';
const UpperBound = 'UpperBound';
const ZerosLike = 'ZerosLike';
/**
 * TensorFlow.js-only kernels
 */
const Step = 'Step';
const FromPixels = 'FromPixels';
const RotateWithOffset = 'RotateWithOffset';
const _FusedMatMul = '_FusedMatMul';
const FusedConv2D = 'FusedConv2D';
const FusedDepthwiseConv2D = 'FusedDepthwiseConv2D';

/**
 * @license
 * Copyright 2018 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
function warn(...msg) {
	if (!(env().getBool('IS_TEST') || env().getBool('PROD'))) {
		console.warn(...msg);
	}
}

function log$2(...msg) {
	if (!(env().getBool('IS_TEST') || env().getBool('PROD'))) {
		console.log(...msg);
	}
}

/**
 * @license
 * Copyright 2019 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
const kernelRegistry = getGlobal('kernelRegistry', () => new Map());
const gradRegistry = getGlobal('gradRegistry', () => new Map());

/**
 * Returns the kernel function (code) associated with the provided names.
 *
 * @param kernelName The official name of the kernel.
 * @param backendName The official name of the backend.
 */
function getKernel(kernelName, backendName) {
	const key = makeKey(kernelName, backendName);
	return kernelRegistry.get(key);
}

/**
 * Returns the registered gradient info associated with the provided kernel.
 * @param kernelName The official TF kernel name.
 */
function getGradient(kernelName) {
	return gradRegistry.get(kernelName);
}

function getKernelsForBackend(backendName) {
	const it = kernelRegistry.entries();
	const result = [];
	while (true) {
		const {done, value} = it.next();
		if (done) {
			break;
		}
		const [key, config] = value;
		const [backend,] = key.split('_');
		if (backend === backendName) {
			result.push(config);
		}
	}
	return result;
}

/**
 * Registers the function (forward pass) for the kernel in a global registry.
 *
 * @param config A config object with the following properties:
 * - `kernelName` The official name of the kernel.
 * - `backendName` The official name of the backend.
 * - `kernelFunc` The function to run during the forward pass of the kernel.
 * - `setupFunc` Optional. Gets called once, after the backend initializes.
 * - `disposeFunc` Optional. Gets called once, right before the backend is
 * disposed.
 */
function registerKernel(config) {
	const {kernelName, backendName} = config;
	const key = makeKey(kernelName, backendName);
	if (kernelRegistry.has(key)) {
		warn(`The kernel '${kernelName}' for backend ` +
			`'${backendName}' is already registered`);
	}
	kernelRegistry.set(key, config);
}

/**
 * Registers a gradient function for a given kernel in the global registry,
 * to be used during the back-propagation of that kernel.
 *
 * @param config An object with the following properties:
 * - `kernelName` The name of the kernel that the gradient function is for.
 * - `gradFunc` The function to run during back-propagation.
 */
function registerGradient(config) {
	const {kernelName} = config;
	if (gradRegistry.has(kernelName)) {
		// TODO (yassogba) after 3.0 assess whether we need to keep this gated
		// to debug mode.
		if (env().getBool('DEBUG')) {
			warn(`Overriding the gradient for '${kernelName}'`);
		}
	}
	gradRegistry.set(kernelName, config);
}

/**
 * Removes the kernel function from the registry.
 *
 * @param kernelName The official name of the kernel.
 * @param backendName The official name of the backend.
 *
 */
function unregisterKernel(kernelName, backendName) {
	const key = makeKey(kernelName, backendName);
	if (!kernelRegistry.has(key)) {
		throw new Error(`The kernel '${kernelName}' for backend ` +
			`'${backendName}' is not registered`);
	}
	kernelRegistry.delete(key);
}

/** Removes the registered gradient from the global registry. */
function unregisterGradient(kernelName) {
	if (!gradRegistry.has(kernelName)) {
		throw new Error(`The gradient '${kernelName}' for backend is not registered`);
	}
	gradRegistry.delete(kernelName);
}

/**
 * Finds kernels that have already been registered to a backend and re-registers
 * them for a new backend. Useful for registering custom backends.
 * @param registeredBackendName Already registered backend.
 * @param newBackendName New backend.
 */
function copyRegisteredKernels(registeredBackendName, newBackendName) {
	const kernels = getKernelsForBackend(registeredBackendName);
	kernels.forEach(kernelConfig => {
		const newKernelConfig = Object.assign({}, kernelConfig, {backendName: newBackendName});
		registerKernel(newKernelConfig);
	});
}

function makeKey(kernelName, backendName) {
	return `${backendName}_${kernelName}`;
}

/**
 * @license
 * Copyright 2023 Google LLC.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
function isTypedArrayBrowser(a) {
	return a instanceof Float32Array || a instanceof Int32Array ||
		a instanceof Uint8Array || a instanceof Uint8ClampedArray;
}

//module.exports = Long$1;

/**
 * wasm optimizations, to do native i64 multiplication and divide
 */
var wasm = null;

try {
	wasm = new WebAssembly.Instance(new WebAssembly.Module(new Uint8Array([
		0, 97, 115, 109, 1, 0, 0, 0, 1, 13, 2, 96, 0, 1, 127, 96, 4, 127, 127, 127, 127, 1, 127, 3, 7, 6, 0, 1, 1, 1, 1, 1, 6, 6, 1, 127, 1, 65, 0, 11, 7, 50, 6, 3, 109, 117, 108, 0, 1, 5, 100, 105, 118, 95, 115, 0, 2, 5, 100, 105, 118, 95, 117, 0, 3, 5, 114, 101, 109, 95, 115, 0, 4, 5, 114, 101, 109, 95, 117, 0, 5, 8, 103, 101, 116, 95, 104, 105, 103, 104, 0, 0, 10, 191, 1, 6, 4, 0, 35, 0, 11, 36, 1, 1, 126, 32, 0, 173, 32, 1, 173, 66, 32, 134, 132, 32, 2, 173, 32, 3, 173, 66, 32, 134, 132, 126, 34, 4, 66, 32, 135, 167, 36, 0, 32, 4, 167, 11, 36, 1, 1, 126, 32, 0, 173, 32, 1, 173, 66, 32, 134, 132, 32, 2, 173, 32, 3, 173, 66, 32, 134, 132, 127, 34, 4, 66, 32, 135, 167, 36, 0, 32, 4, 167, 11, 36, 1, 1, 126, 32, 0, 173, 32, 1, 173, 66, 32, 134, 132, 32, 2, 173, 32, 3, 173, 66, 32, 134, 132, 128, 34, 4, 66, 32, 135, 167, 36, 0, 32, 4, 167, 11, 36, 1, 1, 126, 32, 0, 173, 32, 1, 173, 66, 32, 134, 132, 32, 2, 173, 32, 3, 173, 66, 32, 134, 132, 129, 34, 4, 66, 32, 135, 167, 36, 0, 32, 4, 167, 11, 36, 1, 1, 126, 32, 0, 173, 32, 1, 173, 66, 32, 134, 132, 32, 2, 173, 32, 3, 173, 66, 32, 134, 132, 130, 34, 4, 66, 32, 135, 167, 36, 0, 32, 4, 167, 11
	])), {}).exports;
} catch (e) {
	// no wasm support :(
}

/**
 * Constructs a 64 bit two's-complement integer, given its low and high 32 bit values as *signed* integers.
 *  See the from* functions below for more convenient ways of constructing Longs.
 * @exports Long
 * @class A Long class for representing a 64 bit two's-complement integer value.
 * @param {number} low The low (signed) 32 bits of the long
 * @param {number} high The high (signed) 32 bits of the long
 * @param {boolean=} unsigned Whether unsigned or not, defaults to signed
 * @constructor
 */
function Long$1(low, high, unsigned) {

	/**
	 * The low 32 bits as a signed value.
	 * @type {number}
	 */
	this.low = low | 0;

	/**
	 * The high 32 bits as a signed value.
	 * @type {number}
	 */
	this.high = high | 0;

	/**
	 * Whether unsigned or not.
	 * @type {boolean}
	 */
	this.unsigned = !!unsigned;
}

// The internal representation of a long is the two given signed, 32-bit values.
// We use 32-bit pieces because these are the size of integers on which
// Javascript performs bit-operations.  For operations like addition and
// multiplication, we split each number into 16 bit pieces, which can easily be
// multiplied within Javascript's floating-point representation without overflow
// or change in sign.
//
// In the algorithms below, we frequently reduce the negative case to the
// positive case by negating the input(s) and then post-processing the result.
// Note that we must ALWAYS check specially whether those values are MIN_VALUE
// (-2^63) because -MIN_VALUE == MIN_VALUE (since 2^63 cannot be represented as
// a positive number, it overflows back into a negative).  Not handling this
// case would often result in infinite recursion.
//
// Common constant values ZERO, ONE, NEG_ONE, etc. are defined below the from*
// methods on which they depend.

/**
 * An indicator used to reliably determine if an object is a Long or not.
 * @type {boolean}
 * @const
 * @private
 */
Long$1.prototype.__isLong__;

Object.defineProperty(Long$1.prototype, "__isLong__", {value: true});

/**
 * @function
 * @param {*} obj Object
 * @returns {boolean}
 * @inner
 */
function isLong(obj) {
	return (obj && obj["__isLong__"]) === true;
}

/**
 * Tests if the specified object is a Long.
 * @function
 * @param {*} obj Object
 * @returns {boolean}
 */
Long$1.isLong = isLong;

/**
 * A cache of the Long representations of small integer values.
 * @type {!Object}
 * @inner
 */
var INT_CACHE = {};

/**
 * A cache of the Long representations of small unsigned integer values.
 * @type {!Object}
 * @inner
 */
var UINT_CACHE = {};

/**
 * @param {number} value
 * @param {boolean=} unsigned
 * @returns {!Long}
 * @inner
 */
function fromInt(value, unsigned) {
	var obj, cachedObj, cache;
	if (unsigned) {
		value >>>= 0;
		if (cache = (0 <= value && value < 256)) {
			cachedObj = UINT_CACHE[value];
			if (cachedObj)
				return cachedObj;
		}
		obj = fromBits(value, (value | 0) < 0 ? -1 : 0, true);
		if (cache)
			UINT_CACHE[value] = obj;
		return obj;
	} else {
		value |= 0;
		if (cache = (-128 <= value && value < 128)) {
			cachedObj = INT_CACHE[value];
			if (cachedObj)
				return cachedObj;
		}
		obj = fromBits(value, value < 0 ? -1 : 0, false);
		if (cache)
			INT_CACHE[value] = obj;
		return obj;
	}
}

/**
 * Returns a Long representing the given 32 bit integer value.
 * @function
 * @param {number} value The 32 bit integer in question
 * @param {boolean=} unsigned Whether unsigned or not, defaults to signed
 * @returns {!Long} The corresponding Long value
 */
Long$1.fromInt = fromInt;

/**
 * @param {number} value
 * @param {boolean=} unsigned
 * @returns {!Long}
 * @inner
 */
function fromNumber(value, unsigned) {
	if (isNaN(value))
		return unsigned ? UZERO : ZERO;
	if (unsigned) {
		if (value < 0)
			return UZERO;
		if (value >= TWO_PWR_64_DBL)
			return MAX_UNSIGNED_VALUE;
	} else {
		if (value <= -TWO_PWR_63_DBL)
			return MIN_VALUE;
		if (value + 1 >= TWO_PWR_63_DBL)
			return MAX_VALUE;
	}
	if (value < 0)
		return fromNumber(-value, unsigned).neg();
	return fromBits((value % TWO_PWR_32_DBL) | 0, (value / TWO_PWR_32_DBL) | 0, unsigned);
}

/**
 * Returns a Long representing the given value, provided that it is a finite number. Otherwise, zero is returned.
 * @function
 * @param {number} value The number in question
 * @param {boolean=} unsigned Whether unsigned or not, defaults to signed
 * @returns {!Long} The corresponding Long value
 */
Long$1.fromNumber = fromNumber;

/**
 * @param {number} lowBits
 * @param {number} highBits
 * @param {boolean=} unsigned
 * @returns {!Long}
 * @inner
 */
function fromBits(lowBits, highBits, unsigned) {
	return new Long$1(lowBits, highBits, unsigned);
}

/**
 * Returns a Long representing the 64 bit integer that comes by concatenating the given low and high bits. Each is
 *  assumed to use 32 bits.
 * @function
 * @param {number} lowBits The low 32 bits
 * @param {number} highBits The high 32 bits
 * @param {boolean=} unsigned Whether unsigned or not, defaults to signed
 * @returns {!Long} The corresponding Long value
 */
Long$1.fromBits = fromBits;

/**
 * @function
 * @param {number} base
 * @param {number} exponent
 * @returns {number}
 * @inner
 */
var pow_dbl = Math.pow; // Used 4 times (4*8 to 15+4)

/**
 * @param {string} str
 * @param {(boolean|number)=} unsigned
 * @param {number=} radix
 * @returns {!Long}
 * @inner
 */
function fromString(str, unsigned, radix) {
	if (str.length === 0)
		throw Error('empty string');
	if (str === "NaN" || str === "Infinity" || str === "+Infinity" || str === "-Infinity")
		return ZERO;
	if (typeof unsigned === 'number') {
		// For goog.math.long compatibility
		radix = unsigned,
			unsigned = false;
	} else {
		unsigned = !!unsigned;
	}
	radix = radix || 10;
	if (radix < 2 || 36 < radix)
		throw RangeError('radix');

	var p;
	if ((p = str.indexOf('-')) > 0)
		throw Error('interior hyphen');
	else if (p === 0) {
		return fromString(str.substring(1), unsigned, radix).neg();
	}

	// Do several (8) digits each time through the loop, so as to
	// minimize the calls to the very expensive emulated div.
	var radixToPower = fromNumber(pow_dbl(radix, 8));

	var result = ZERO;
	for (var i = 0; i < str.length; i += 8) {
		var size = Math.min(8, str.length - i),
			value = parseInt(str.substring(i, i + size), radix);
		if (size < 8) {
			var power = fromNumber(pow_dbl(radix, size));
			result = result.mul(power).add(fromNumber(value));
		} else {
			result = result.mul(radixToPower);
			result = result.add(fromNumber(value));
		}
	}
	result.unsigned = unsigned;
	return result;
}

/**
 * Returns a Long representation of the given string, written using the specified radix.
 * @function
 * @param {string} str The textual representation of the Long
 * @param {(boolean|number)=} unsigned Whether unsigned or not, defaults to signed
 * @param {number=} radix The radix in which the text is written (2-36), defaults to 10
 * @returns {!Long} The corresponding Long value
 */
Long$1.fromString = fromString;

/**
 * @function
 * @param {!Long|number|string|!{low: number, high: number, unsigned: boolean}} val
 * @param {boolean=} unsigned
 * @returns {!Long}
 * @inner
 */
function fromValue(val, unsigned) {
	if (typeof val === 'number')
		return fromNumber(val, unsigned);
	if (typeof val === 'string')
		return fromString(val, unsigned);
	// Throws for non-objects, converts non-instanceof Long:
	return fromBits(val.low, val.high, typeof unsigned === 'boolean' ? unsigned : val.unsigned);
}

/**
 * Converts the specified value to a Long using the appropriate from* function for its type.
 * @function
 * @param {!Long|number|string|!{low: number, high: number, unsigned: boolean}} val Value
 * @param {boolean=} unsigned Whether unsigned or not, defaults to signed
 * @returns {!Long}
 */
Long$1.fromValue = fromValue;

// NOTE: the compiler should inline these constant values below and then remove these variables, so there should be
// no runtime penalty for these.

/**
 * @type {number}
 * @const
 * @inner
 */
var TWO_PWR_16_DBL = 1 << 16;

/**
 * @type {number}
 * @const
 * @inner
 */
var TWO_PWR_24_DBL = 1 << 24;

/**
 * @type {number}
 * @const
 * @inner
 */
var TWO_PWR_32_DBL = TWO_PWR_16_DBL * TWO_PWR_16_DBL;

/**
 * @type {number}
 * @const
 * @inner
 */
var TWO_PWR_64_DBL = TWO_PWR_32_DBL * TWO_PWR_32_DBL;

/**
 * @type {number}
 * @const
 * @inner
 */
var TWO_PWR_63_DBL = TWO_PWR_64_DBL / 2;

/**
 * @type {!Long}
 * @const
 * @inner
 */
var TWO_PWR_24 = fromInt(TWO_PWR_24_DBL);

/**
 * @type {!Long}
 * @inner
 */
var ZERO = fromInt(0);

/**
 * Signed zero.
 * @type {!Long}
 */
Long$1.ZERO = ZERO;

/**
 * @type {!Long}
 * @inner
 */
var UZERO = fromInt(0, true);

/**
 * Unsigned zero.
 * @type {!Long}
 */
Long$1.UZERO = UZERO;

/**
 * @type {!Long}
 * @inner
 */
var ONE = fromInt(1);

/**
 * Signed one.
 * @type {!Long}
 */
Long$1.ONE = ONE;

/**
 * @type {!Long}
 * @inner
 */
var UONE = fromInt(1, true);

/**
 * Unsigned one.
 * @type {!Long}
 */
Long$1.UONE = UONE;

/**
 * @type {!Long}
 * @inner
 */
var NEG_ONE = fromInt(-1);

/**
 * Signed negative one.
 * @type {!Long}
 */
Long$1.NEG_ONE = NEG_ONE;

/**
 * @type {!Long}
 * @inner
 */
var MAX_VALUE = fromBits(0xFFFFFFFF | 0, 0x7FFFFFFF | 0, false);

/**
 * Maximum signed value.
 * @type {!Long}
 */
Long$1.MAX_VALUE = MAX_VALUE;

/**
 * @type {!Long}
 * @inner
 */
var MAX_UNSIGNED_VALUE = fromBits(0xFFFFFFFF | 0, 0xFFFFFFFF | 0, true);

/**
 * Maximum unsigned value.
 * @type {!Long}
 */
Long$1.MAX_UNSIGNED_VALUE = MAX_UNSIGNED_VALUE;

/**
 * @type {!Long}
 * @inner
 */
var MIN_VALUE = fromBits(0, 0x80000000 | 0, false);

/**
 * Minimum signed value.
 * @type {!Long}
 */
Long$1.MIN_VALUE = MIN_VALUE;

/**
 * @alias Long.prototype
 * @inner
 */
var LongPrototype = Long$1.prototype;

/**
 * Converts the Long to a 32 bit integer, assuming it is a 32 bit integer.
 * @returns {number}
 */
LongPrototype.toInt = function toInt() {
	return this.unsigned ? this.low >>> 0 : this.low;
};

/**
 * Converts the Long to a the nearest floating-point representation of this value (double, 53 bit mantissa).
 * @returns {number}
 */
LongPrototype.toNumber = function toNumber() {
	if (this.unsigned)
		return ((this.high >>> 0) * TWO_PWR_32_DBL) + (this.low >>> 0);
	return this.high * TWO_PWR_32_DBL + (this.low >>> 0);
};

/**
 * Converts the Long to a string written in the specified radix.
 * @param {number=} radix Radix (2-36), defaults to 10
 * @returns {string}
 * @override
 * @throws {RangeError} If `radix` is out of range
 */
LongPrototype.toString = function toString(radix) {
	radix = radix || 10;
	if (radix < 2 || 36 < radix)
		throw RangeError('radix');
	if (this.isZero())
		return '0';
	if (this.isNegative()) { // Unsigned Longs are never negative
		if (this.eq(MIN_VALUE)) {
			// We need to change the Long value before it can be negated, so we remove
			// the bottom-most digit in this base and then recurse to do the rest.
			var radixLong = fromNumber(radix),
				div = this.div(radixLong),
				rem1 = div.mul(radixLong).sub(this);
			return div.toString(radix) + rem1.toInt().toString(radix);
		} else
			return '-' + this.neg().toString(radix);
	}

	// Do several (6) digits each time through the loop, so as to
	// minimize the calls to the very expensive emulated div.
	var radixToPower = fromNumber(pow_dbl(radix, 6), this.unsigned),
		rem = this;
	var result = '';
	while (true) {
		var remDiv = rem.div(radixToPower),
			intval = rem.sub(remDiv.mul(radixToPower)).toInt() >>> 0,
			digits = intval.toString(radix);
		rem = remDiv;
		if (rem.isZero())
			return digits + result;
		else {
			while (digits.length < 6)
				digits = '0' + digits;
			result = '' + digits + result;
		}
	}
};

/**
 * Gets the high 32 bits as a signed integer.
 * @returns {number} Signed high bits
 */
LongPrototype.getHighBits = function getHighBits() {
	return this.high;
};

/**
 * Gets the high 32 bits as an unsigned integer.
 * @returns {number} Unsigned high bits
 */
LongPrototype.getHighBitsUnsigned = function getHighBitsUnsigned() {
	return this.high >>> 0;
};

/**
 * Gets the low 32 bits as a signed integer.
 * @returns {number} Signed low bits
 */
LongPrototype.getLowBits = function getLowBits() {
	return this.low;
};

/**
 * Gets the low 32 bits as an unsigned integer.
 * @returns {number} Unsigned low bits
 */
LongPrototype.getLowBitsUnsigned = function getLowBitsUnsigned() {
	return this.low >>> 0;
};

/**
 * Gets the number of bits needed to represent the absolute value of this Long.
 * @returns {number}
 */
LongPrototype.getNumBitsAbs = function getNumBitsAbs() {
	if (this.isNegative()) // Unsigned Longs are never negative
		return this.eq(MIN_VALUE) ? 64 : this.neg().getNumBitsAbs();
	var val = this.high != 0 ? this.high : this.low;
	for (var bit = 31; bit > 0; bit--)
		if ((val & (1 << bit)) != 0)
			break;
	return this.high != 0 ? bit + 33 : bit + 1;
};

/**
 * Tests if this Long's value equals zero.
 * @returns {boolean}
 */
LongPrototype.isZero = function isZero() {
	return this.high === 0 && this.low === 0;
};

/**
 * Tests if this Long's value equals zero. This is an alias of {@link Long#isZero}.
 * @returns {boolean}
 */
LongPrototype.eqz = LongPrototype.isZero;

/**
 * Tests if this Long's value is negative.
 * @returns {boolean}
 */
LongPrototype.isNegative = function isNegative() {
	return !this.unsigned && this.high < 0;
};

/**
 * Tests if this Long's value is positive.
 * @returns {boolean}
 */
LongPrototype.isPositive = function isPositive() {
	return this.unsigned || this.high >= 0;
};

/**
 * Tests if this Long's value is odd.
 * @returns {boolean}
 */
LongPrototype.isOdd = function isOdd() {
	return (this.low & 1) === 1;
};

/**
 * Tests if this Long's value is even.
 * @returns {boolean}
 */
LongPrototype.isEven = function isEven() {
	return (this.low & 1) === 0;
};

/**
 * Tests if this Long's value equals the specified's.
 * @param {!Long|number|string} other Other value
 * @returns {boolean}
 */
LongPrototype.equals = function equals(other) {
	if (!isLong(other))
		other = fromValue(other);
	if (this.unsigned !== other.unsigned && (this.high >>> 31) === 1 && (other.high >>> 31) === 1)
		return false;
	return this.high === other.high && this.low === other.low;
};

/**
 * Tests if this Long's value equals the specified's. This is an alias of {@link Long#equals}.
 * @function
 * @param {!Long|number|string} other Other value
 * @returns {boolean}
 */
LongPrototype.eq = LongPrototype.equals;

/**
 * Tests if this Long's value differs from the specified's.
 * @param {!Long|number|string} other Other value
 * @returns {boolean}
 */
LongPrototype.notEquals = function notEquals(other) {
	return !this.eq(/* validates */ other);
};

/**
 * Tests if this Long's value differs from the specified's. This is an alias of {@link Long#notEquals}.
 * @function
 * @param {!Long|number|string} other Other value
 * @returns {boolean}
 */
LongPrototype.neq = LongPrototype.notEquals;

/**
 * Tests if this Long's value differs from the specified's. This is an alias of {@link Long#notEquals}.
 * @function
 * @param {!Long|number|string} other Other value
 * @returns {boolean}
 */
LongPrototype.ne = LongPrototype.notEquals;

/**
 * Tests if this Long's value is less than the specified's.
 * @param {!Long|number|string} other Other value
 * @returns {boolean}
 */
LongPrototype.lessThan = function lessThan(other) {
	return this.comp(/* validates */ other) < 0;
};

/**
 * Tests if this Long's value is less than the specified's. This is an alias of {@link Long#lessThan}.
 * @function
 * @param {!Long|number|string} other Other value
 * @returns {boolean}
 */
LongPrototype.lt = LongPrototype.lessThan;

/**
 * Tests if this Long's value is less than or equal the specified's.
 * @param {!Long|number|string} other Other value
 * @returns {boolean}
 */
LongPrototype.lessThanOrEqual = function lessThanOrEqual(other) {
	return this.comp(/* validates */ other) <= 0;
};

/**
 * Tests if this Long's value is less than or equal the specified's. This is an alias of {@link Long#lessThanOrEqual}.
 * @function
 * @param {!Long|number|string} other Other value
 * @returns {boolean}
 */
LongPrototype.lte = LongPrototype.lessThanOrEqual;

/**
 * Tests if this Long's value is less than or equal the specified's. This is an alias of {@link Long#lessThanOrEqual}.
 * @function
 * @param {!Long|number|string} other Other value
 * @returns {boolean}
 */
LongPrototype.le = LongPrototype.lessThanOrEqual;

/**
 * Tests if this Long's value is greater than the specified's.
 * @param {!Long|number|string} other Other value
 * @returns {boolean}
 */
LongPrototype.greaterThan = function greaterThan(other) {
	return this.comp(/* validates */ other) > 0;
};

/**
 * Tests if this Long's value is greater than the specified's. This is an alias of {@link Long#greaterThan}.
 * @function
 * @param {!Long|number|string} other Other value
 * @returns {boolean}
 */
LongPrototype.gt = LongPrototype.greaterThan;

/**
 * Tests if this Long's value is greater than or equal the specified's.
 * @param {!Long|number|string} other Other value
 * @returns {boolean}
 */
LongPrototype.greaterThanOrEqual = function greaterThanOrEqual(other) {
	return this.comp(/* validates */ other) >= 0;
};

/**
 * Tests if this Long's value is greater than or equal the specified's. This is an alias of {@link Long#greaterThanOrEqual}.
 * @function
 * @param {!Long|number|string} other Other value
 * @returns {boolean}
 */
LongPrototype.gte = LongPrototype.greaterThanOrEqual;

/**
 * Tests if this Long's value is greater than or equal the specified's. This is an alias of {@link Long#greaterThanOrEqual}.
 * @function
 * @param {!Long|number|string} other Other value
 * @returns {boolean}
 */
LongPrototype.ge = LongPrototype.greaterThanOrEqual;

/**
 * Compares this Long's value with the specified's.
 * @param {!Long|number|string} other Other value
 * @returns {number} 0 if they are the same, 1 if the this is greater and -1
 *  if the given one is greater
 */
LongPrototype.compare = function compare(other) {
	if (!isLong(other))
		other = fromValue(other);
	if (this.eq(other))
		return 0;
	var thisNeg = this.isNegative(),
		otherNeg = other.isNegative();
	if (thisNeg && !otherNeg)
		return -1;
	if (!thisNeg && otherNeg)
		return 1;
	// At this point the sign bits are the same
	if (!this.unsigned)
		return this.sub(other).isNegative() ? -1 : 1;
	// Both are positive if at least one is unsigned
	return (other.high >>> 0) > (this.high >>> 0) || (other.high === this.high && (other.low >>> 0) > (this.low >>> 0)) ? -1 : 1;
};

/**
 * Compares this Long's value with the specified's. This is an alias of {@link Long#compare}.
 * @function
 * @param {!Long|number|string} other Other value
 * @returns {number} 0 if they are the same, 1 if the this is greater and -1
 *  if the given one is greater
 */
LongPrototype.comp = LongPrototype.compare;

/**
 * Negates this Long's value.
 * @returns {!Long} Negated Long
 */
LongPrototype.negate = function negate() {
	if (!this.unsigned && this.eq(MIN_VALUE))
		return MIN_VALUE;
	return this.not().add(ONE);
};

/**
 * Negates this Long's value. This is an alias of {@link Long#negate}.
 * @function
 * @returns {!Long} Negated Long
 */
LongPrototype.neg = LongPrototype.negate;

/**
 * Returns the sum of this and the specified Long.
 * @param {!Long|number|string} addend Addend
 * @returns {!Long} Sum
 */
LongPrototype.add = function add(addend) {
	if (!isLong(addend))
		addend = fromValue(addend);

	// Divide each number into 4 chunks of 16 bits, and then sum the chunks.

	var a48 = this.high >>> 16;
	var a32 = this.high & 0xFFFF;
	var a16 = this.low >>> 16;
	var a00 = this.low & 0xFFFF;

	var b48 = addend.high >>> 16;
	var b32 = addend.high & 0xFFFF;
	var b16 = addend.low >>> 16;
	var b00 = addend.low & 0xFFFF;

	var c48 = 0, c32 = 0, c16 = 0, c00 = 0;
	c00 += a00 + b00;
	c16 += c00 >>> 16;
	c00 &= 0xFFFF;
	c16 += a16 + b16;
	c32 += c16 >>> 16;
	c16 &= 0xFFFF;
	c32 += a32 + b32;
	c48 += c32 >>> 16;
	c32 &= 0xFFFF;
	c48 += a48 + b48;
	c48 &= 0xFFFF;
	return fromBits((c16 << 16) | c00, (c48 << 16) | c32, this.unsigned);
};

/**
 * Returns the difference of this and the specified Long.
 * @param {!Long|number|string} subtrahend Subtrahend
 * @returns {!Long} Difference
 */
LongPrototype.subtract = function subtract(subtrahend) {
	if (!isLong(subtrahend))
		subtrahend = fromValue(subtrahend);
	return this.add(subtrahend.neg());
};

/**
 * Returns the difference of this and the specified Long. This is an alias of {@link Long#subtract}.
 * @function
 * @param {!Long|number|string} subtrahend Subtrahend
 * @returns {!Long} Difference
 */
LongPrototype.sub = LongPrototype.subtract;

/**
 * Returns the product of this and the specified Long.
 * @param {!Long|number|string} multiplier Multiplier
 * @returns {!Long} Product
 */
LongPrototype.multiply = function multiply(multiplier) {
	if (this.isZero())
		return ZERO;
	if (!isLong(multiplier))
		multiplier = fromValue(multiplier);

	// use wasm support if present
	if (wasm) {
		var low = wasm.mul(this.low,
			this.high,
			multiplier.low,
			multiplier.high);
		return fromBits(low, wasm.get_high(), this.unsigned);
	}

	if (multiplier.isZero())
		return ZERO;
	if (this.eq(MIN_VALUE))
		return multiplier.isOdd() ? MIN_VALUE : ZERO;
	if (multiplier.eq(MIN_VALUE))
		return this.isOdd() ? MIN_VALUE : ZERO;

	if (this.isNegative()) {
		if (multiplier.isNegative())
			return this.neg().mul(multiplier.neg());
		else
			return this.neg().mul(multiplier).neg();
	} else if (multiplier.isNegative())
		return this.mul(multiplier.neg()).neg();

	// If both longs are small, use float multiplication
	if (this.lt(TWO_PWR_24) && multiplier.lt(TWO_PWR_24))
		return fromNumber(this.toNumber() * multiplier.toNumber(), this.unsigned);

	// Divide each long into 4 chunks of 16 bits, and then add up 4x4 products.
	// We can skip products that would overflow.

	var a48 = this.high >>> 16;
	var a32 = this.high & 0xFFFF;
	var a16 = this.low >>> 16;
	var a00 = this.low & 0xFFFF;

	var b48 = multiplier.high >>> 16;
	var b32 = multiplier.high & 0xFFFF;
	var b16 = multiplier.low >>> 16;
	var b00 = multiplier.low & 0xFFFF;

	var c48 = 0, c32 = 0, c16 = 0, c00 = 0;
	c00 += a00 * b00;
	c16 += c00 >>> 16;
	c00 &= 0xFFFF;
	c16 += a16 * b00;
	c32 += c16 >>> 16;
	c16 &= 0xFFFF;
	c16 += a00 * b16;
	c32 += c16 >>> 16;
	c16 &= 0xFFFF;
	c32 += a32 * b00;
	c48 += c32 >>> 16;
	c32 &= 0xFFFF;
	c32 += a16 * b16;
	c48 += c32 >>> 16;
	c32 &= 0xFFFF;
	c32 += a00 * b32;
	c48 += c32 >>> 16;
	c32 &= 0xFFFF;
	c48 += a48 * b00 + a32 * b16 + a16 * b32 + a00 * b48;
	c48 &= 0xFFFF;
	return fromBits((c16 << 16) | c00, (c48 << 16) | c32, this.unsigned);
};

/**
 * Returns the product of this and the specified Long. This is an alias of {@link Long#multiply}.
 * @function
 * @param {!Long|number|string} multiplier Multiplier
 * @returns {!Long} Product
 */
LongPrototype.mul = LongPrototype.multiply;

/**
 * Returns this Long divided by the specified. The result is signed if this Long is signed or
 *  unsigned if this Long is unsigned.
 * @param {!Long|number|string} divisor Divisor
 * @returns {!Long} Quotient
 */
LongPrototype.divide = function divide(divisor) {
	if (!isLong(divisor))
		divisor = fromValue(divisor);
	if (divisor.isZero())
		throw Error('division by zero');

	// use wasm support if present
	if (wasm) {
		// guard against signed division overflow: the largest
		// negative number / -1 would be 1 larger than the largest
		// positive number, due to two's complement.
		if (!this.unsigned &&
			this.high === -2147483648 &&
			divisor.low === -1 && divisor.high === -1) {
			// be consistent with non-wasm code path
			return this;
		}
		var low = (this.unsigned ? wasm.div_u : wasm.div_s)(
			this.low,
			this.high,
			divisor.low,
			divisor.high
		);
		return fromBits(low, wasm.get_high(), this.unsigned);
	}

	if (this.isZero())
		return this.unsigned ? UZERO : ZERO;
	var approx, rem, res;
	if (!this.unsigned) {
		// This section is only relevant for signed longs and is derived from the
		// closure library as a whole.
		if (this.eq(MIN_VALUE)) {
			if (divisor.eq(ONE) || divisor.eq(NEG_ONE))
				return MIN_VALUE;  // recall that -MIN_VALUE == MIN_VALUE
			else if (divisor.eq(MIN_VALUE))
				return ONE;
			else {
				// At this point, we have |other| >= 2, so |this/other| < |MIN_VALUE|.
				var halfThis = this.shr(1);
				approx = halfThis.div(divisor).shl(1);
				if (approx.eq(ZERO)) {
					return divisor.isNegative() ? ONE : NEG_ONE;
				} else {
					rem = this.sub(divisor.mul(approx));
					res = approx.add(rem.div(divisor));
					return res;
				}
			}
		} else if (divisor.eq(MIN_VALUE))
			return this.unsigned ? UZERO : ZERO;
		if (this.isNegative()) {
			if (divisor.isNegative())
				return this.neg().div(divisor.neg());
			return this.neg().div(divisor).neg();
		} else if (divisor.isNegative())
			return this.div(divisor.neg()).neg();
		res = ZERO;
	} else {
		// The algorithm below has not been made for unsigned longs. It's therefore
		// required to take special care of the MSB prior to running it.
		if (!divisor.unsigned)
			divisor = divisor.toUnsigned();
		if (divisor.gt(this))
			return UZERO;
		if (divisor.gt(this.shru(1))) // 15 >>> 1 = 7 ; with divisor = 8 ; true
			return UONE;
		res = UZERO;
	}

	// Repeat the following until the remainder is less than other:  find a
	// floating-point that approximates remainder / other *from below*, add this
	// into the result, and subtract it from the remainder.  It is critical that
	// the approximate value is less than or equal to the real value so that the
	// remainder never becomes negative.
	rem = this;
	while (rem.gte(divisor)) {
		// Approximate the result of division. This may be a little greater or
		// smaller than the actual value.
		approx = Math.max(1, Math.floor(rem.toNumber() / divisor.toNumber()));

		// We will tweak the approximate result by changing it in the 48-th digit or
		// the smallest non-fractional digit, whichever is larger.
		var log2 = Math.ceil(Math.log(approx) / Math.LN2),
			delta = (log2 <= 48) ? 1 : pow_dbl(2, log2 - 48),

			// Decrease the approximation until it is smaller than the remainder.  Note
			// that if it is too large, the product overflows and is negative.
			approxRes = fromNumber(approx),
			approxRem = approxRes.mul(divisor);
		while (approxRem.isNegative() || approxRem.gt(rem)) {
			approx -= delta;
			approxRes = fromNumber(approx, this.unsigned);
			approxRem = approxRes.mul(divisor);
		}

		// We know the answer can't be zero... and actually, zero would cause
		// infinite recursion since we would make no progress.
		if (approxRes.isZero())
			approxRes = ONE;

		res = res.add(approxRes);
		rem = rem.sub(approxRem);
	}
	return res;
};

/**
 * Returns this Long divided by the specified. This is an alias of {@link Long#divide}.
 * @function
 * @param {!Long|number|string} divisor Divisor
 * @returns {!Long} Quotient
 */
LongPrototype.div = LongPrototype.divide;

/**
 * Returns this Long modulo the specified.
 * @param {!Long|number|string} divisor Divisor
 * @returns {!Long} Remainder
 */
LongPrototype.modulo = function modulo(divisor) {
	if (!isLong(divisor))
		divisor = fromValue(divisor);

	// use wasm support if present
	if (wasm) {
		var low = (this.unsigned ? wasm.rem_u : wasm.rem_s)(
			this.low,
			this.high,
			divisor.low,
			divisor.high
		);
		return fromBits(low, wasm.get_high(), this.unsigned);
	}

	return this.sub(this.div(divisor).mul(divisor));
};

/**
 * Returns this Long modulo the specified. This is an alias of {@link Long#modulo}.
 * @function
 * @param {!Long|number|string} divisor Divisor
 * @returns {!Long} Remainder
 */
LongPrototype.mod = LongPrototype.modulo;

/**
 * Returns this Long modulo the specified. This is an alias of {@link Long#modulo}.
 * @function
 * @param {!Long|number|string} divisor Divisor
 * @returns {!Long} Remainder
 */
LongPrototype.rem = LongPrototype.modulo;

/**
 * Returns the bitwise NOT of this Long.
 * @returns {!Long}
 */
LongPrototype.not = function not() {
	return fromBits(~this.low, ~this.high, this.unsigned);
};

/**
 * Returns the bitwise AND of this Long and the specified.
 * @param {!Long|number|string} other Other Long
 * @returns {!Long}
 */
LongPrototype.and = function and(other) {
	if (!isLong(other))
		other = fromValue(other);
	return fromBits(this.low & other.low, this.high & other.high, this.unsigned);
};

/**
 * Returns the bitwise OR of this Long and the specified.
 * @param {!Long|number|string} other Other Long
 * @returns {!Long}
 */
LongPrototype.or = function or(other) {
	if (!isLong(other))
		other = fromValue(other);
	return fromBits(this.low | other.low, this.high | other.high, this.unsigned);
};

/**
 * Returns the bitwise XOR of this Long and the given one.
 * @param {!Long|number|string} other Other Long
 * @returns {!Long}
 */
LongPrototype.xor = function xor(other) {
	if (!isLong(other))
		other = fromValue(other);
	return fromBits(this.low ^ other.low, this.high ^ other.high, this.unsigned);
};

/**
 * Returns this Long with bits shifted to the left by the given amount.
 * @param {number|!Long} numBits Number of bits
 * @returns {!Long} Shifted Long
 */
LongPrototype.shiftLeft = function shiftLeft(numBits) {
	if (isLong(numBits))
		numBits = numBits.toInt();
	if ((numBits &= 63) === 0)
		return this;
	else if (numBits < 32)
		return fromBits(this.low << numBits, (this.high << numBits) | (this.low >>> (32 - numBits)), this.unsigned);
	else
		return fromBits(0, this.low << (numBits - 32), this.unsigned);
};

/**
 * Returns this Long with bits shifted to the left by the given amount. This is an alias of {@link Long#shiftLeft}.
 * @function
 * @param {number|!Long} numBits Number of bits
 * @returns {!Long} Shifted Long
 */
LongPrototype.shl = LongPrototype.shiftLeft;

/**
 * Returns this Long with bits arithmetically shifted to the right by the given amount.
 * @param {number|!Long} numBits Number of bits
 * @returns {!Long} Shifted Long
 */
LongPrototype.shiftRight = function shiftRight(numBits) {
	if (isLong(numBits))
		numBits = numBits.toInt();
	if ((numBits &= 63) === 0)
		return this;
	else if (numBits < 32)
		return fromBits((this.low >>> numBits) | (this.high << (32 - numBits)), this.high >> numBits, this.unsigned);
	else
		return fromBits(this.high >> (numBits - 32), this.high >= 0 ? 0 : -1, this.unsigned);
};

/**
 * Returns this Long with bits arithmetically shifted to the right by the given amount. This is an alias of {@link Long#shiftRight}.
 * @function
 * @param {number|!Long} numBits Number of bits
 * @returns {!Long} Shifted Long
 */
LongPrototype.shr = LongPrototype.shiftRight;

/**
 * Returns this Long with bits logically shifted to the right by the given amount.
 * @param {number|!Long} numBits Number of bits
 * @returns {!Long} Shifted Long
 */
LongPrototype.shiftRightUnsigned = function shiftRightUnsigned(numBits) {
	if (isLong(numBits))
		numBits = numBits.toInt();
	numBits &= 63;
	if (numBits === 0)
		return this;
	else {
		var high = this.high;
		if (numBits < 32) {
			var low = this.low;
			return fromBits((low >>> numBits) | (high << (32 - numBits)), high >>> numBits, this.unsigned);
		} else if (numBits === 32)
			return fromBits(high, 0, this.unsigned);
		else
			return fromBits(high >>> (numBits - 32), 0, this.unsigned);
	}
};

/**
 * Returns this Long with bits logically shifted to the right by the given amount. This is an alias of {@link Long#shiftRightUnsigned}.
 * @function
 * @param {number|!Long} numBits Number of bits
 * @returns {!Long} Shifted Long
 */
LongPrototype.shru = LongPrototype.shiftRightUnsigned;

/**
 * Returns this Long with bits logically shifted to the right by the given amount. This is an alias of {@link Long#shiftRightUnsigned}.
 * @function
 * @param {number|!Long} numBits Number of bits
 * @returns {!Long} Shifted Long
 */
LongPrototype.shr_u = LongPrototype.shiftRightUnsigned;

/**
 * Converts this Long to signed.
 * @returns {!Long} Signed long
 */
LongPrototype.toSigned = function toSigned() {
	if (!this.unsigned)
		return this;
	return fromBits(this.low, this.high, false);
};

/**
 * Converts this Long to unsigned.
 * @returns {!Long} Unsigned long
 */
LongPrototype.toUnsigned = function toUnsigned() {
	if (this.unsigned)
		return this;
	return fromBits(this.low, this.high, true);
};

/**
 * Converts this Long to its byte representation.
 * @param {boolean=} le Whether little or big endian, defaults to big endian
 * @returns {!Array.<number>} Byte representation
 */
LongPrototype.toBytes = function toBytes(le) {
	return le ? this.toBytesLE() : this.toBytesBE();
};

/**
 * Converts this Long to its little endian byte representation.
 * @returns {!Array.<number>} Little endian byte representation
 */
LongPrototype.toBytesLE = function toBytesLE() {
	var hi = this.high,
		lo = this.low;
	return [
		lo & 0xff,
		lo >>> 8 & 0xff,
		lo >>> 16 & 0xff,
		lo >>> 24,
		hi & 0xff,
		hi >>> 8 & 0xff,
		hi >>> 16 & 0xff,
		hi >>> 24
	];
};

/**
 * Converts this Long to its big endian byte representation.
 * @returns {!Array.<number>} Big endian byte representation
 */
LongPrototype.toBytesBE = function toBytesBE() {
	var hi = this.high,
		lo = this.low;
	return [
		hi >>> 24,
		hi >>> 16 & 0xff,
		hi >>> 8 & 0xff,
		hi & 0xff,
		lo >>> 24,
		lo >>> 16 & 0xff,
		lo >>> 8 & 0xff,
		lo & 0xff
	];
};

/**
 * Creates a Long from its byte representation.
 * @param {!Array.<number>} bytes Byte representation
 * @param {boolean=} unsigned Whether unsigned or not, defaults to signed
 * @param {boolean=} le Whether little or big endian, defaults to big endian
 * @returns {Long} The corresponding Long value
 */
Long$1.fromBytes = function fromBytes(bytes, unsigned, le) {
	return le ? Long$1.fromBytesLE(bytes, unsigned) : Long$1.fromBytesBE(bytes, unsigned);
};

/**
 * Creates a Long from its little endian byte representation.
 * @param {!Array.<number>} bytes Little endian byte representation
 * @param {boolean=} unsigned Whether unsigned or not, defaults to signed
 * @returns {Long} The corresponding Long value
 */
Long$1.fromBytesLE = function fromBytesLE(bytes, unsigned) {
	return new Long$1(
		bytes[0] |
		bytes[1] << 8 |
		bytes[2] << 16 |
		bytes[3] << 24,
		bytes[4] |
		bytes[5] << 8 |
		bytes[6] << 16 |
		bytes[7] << 24,
		unsigned
	);
};

/**
 * Creates a Long from its big endian byte representation.
 * @param {!Array.<number>} bytes Big endian byte representation
 * @param {boolean=} unsigned Whether unsigned or not, defaults to signed
 * @returns {Long} The corresponding Long value
 */
Long$1.fromBytesBE = function fromBytesBE(bytes, unsigned) {
	return new Long$1(
		bytes[4] << 24 |
		bytes[5] << 16 |
		bytes[6] << 8 |
		bytes[7],
		bytes[0] << 24 |
		bytes[1] << 16 |
		bytes[2] << 8 |
		bytes[3],
		unsigned
	);
};

var LongExports = /*#__PURE__*/Object.freeze({
	__proto__: null
});

/**
 * @license
 * Copyright 2021 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
// Workaround for allowing cjs module to be included in bundle created by
// rollup.
// tslint:disable-next-line
const Long =
// tslint:disable-next-line
	LongExports;

function hexToLong(hex) {
	return Long.fromString(hex, true, 16);
}

// Some primes between 2^63 and 2^64 for various uses.
// Hex 0xc3a5c85c97cb3127
const k0 = hexToLong('c3a5c85c97cb3127');
// Hex 0xb492b66fbe98f273
const k1 = hexToLong('b492b66fbe98f273');
// Hex 0x9ae16a3b2f90404f
const k2 = hexToLong('9ae16a3b2f90404f');

function shiftMix(val) {
	return val.xor(val.shru(47));
}

function fetch$2(s, offset, numBytes) {
	const bytes = s.slice(offset, offset + numBytes);
	return Long.fromBytes(Array.from(bytes), true, true);
}

function fetch64(s, offset) {
	return fetch$2(s, offset, 8);
}

function fetch32(s, offset) {
	return fetch$2(s, offset, 4);
}

function rotate64(val, shift) {
	// Avoid shifting by 64: doing so yields an undefined result.
	return shift === 0 ? val : val.shru(shift).or(val.shl(64 - shift));
}

function hashLen16(u, v, mul = hexToLong('9ddfea08eb382d69')) {
	// Murmur-inspired hashing.
	let a = u.xor(v).mul(mul);
	a = a.xor(a.shru(47));
	let b = v.xor(a).mul(mul);
	b = b.xor(b.shru(47));
	b = b.mul(mul);
	return b;
}

// Return a 16-byte hash for 48 bytes.  Quick and dirty.
// Callers do best to use "random-looking" values for a and b.
function weakHashLen32WithSeeds(w, x, y, z, a, b) {
	a = a.add(w);
	b = rotate64(b.add(a).add(z), 21);
	const c = a;
	a = a.add(x);
	a = a.add(y);
	b = b.add(rotate64(a, 44));
	return [a.add(z), b.add(c)];
}

function weakHashLen32WithSeedsStr(s, offset, a, b) {
	return weakHashLen32WithSeeds(fetch64(s, offset), fetch64(s, offset + 8), fetch64(s, offset + 16), fetch64(s, offset + 24), a, b);
}

function hashLen0to16(s, len = s.length) {
	if (len >= 8) {
		const mul = k2.add(len * 2);
		const a = fetch64(s, 0).add(k2);
		const b = fetch64(s, len - 8);
		const c = rotate64(b, 37).mul(mul).add(a);
		const d = rotate64(a, 25).add(b).mul(mul);
		return hashLen16(c, d, mul);
	}
	if (len >= 4) {
		const mul = k2.add(len * 2);
		const a = fetch32(s, 0);
		return hashLen16(a.shl(3).add(len), fetch32(s, len - 4), mul);
	}
	if (len > 0) {
		const a = s[0];
		const b = s[len >> 1];
		const c = s[len - 1];
		const y = a + (b << 8);
		const z = len + (c << 2);
		return shiftMix(k2.mul(y).xor(k0.mul(z))).mul(k2);
	}
	return k2;
}

function hashLen17to32(s, len = s.length) {
	const mul = k2.add(len * 2);
	const a = fetch64(s, 0).mul(k1);
	const b = fetch64(s, 8);
	const c = fetch64(s, len - 8).mul(mul);
	const d = fetch64(s, len - 16).mul(k2);
	return hashLen16(rotate64(a.add(b), 43).add(rotate64(c, 30)).add(d), a.add(rotate64(b.add(k2), 18)).add(c), mul);
}

function hashLen33to64(s, len = s.length) {
	const mul = k2.add(len * 2);
	const a = fetch64(s, 0).mul(k2);
	const b = fetch64(s, 8);
	const c = fetch64(s, len - 8).mul(mul);
	const d = fetch64(s, len - 16).mul(k2);
	const y = rotate64(a.add(b), 43).add(rotate64(c, 30)).add(d);
	const z = hashLen16(y, a.add(rotate64(b.add(k2), 18)).add(c), mul);
	const e = fetch64(s, 16).mul(mul);
	const f = fetch64(s, 24);
	const g = y.add(fetch64(s, len - 32)).mul(mul);
	const h = z.add(fetch64(s, len - 24)).mul(mul);
	return hashLen16(rotate64(e.add(f), 43).add(rotate64(g, 30)).add(h), e.add(rotate64(f.add(a), 18)).add(g), mul);
}

function fingerPrint64(s, len = s.length) {
	const seed = Long.fromNumber(81, true);
	if (len <= 32) {
		if (len <= 16) {
			return hashLen0to16(s, len);
		} else {
			return hashLen17to32(s, len);
		}
	} else if (len <= 64) {
		return hashLen33to64(s, len);
	}
	// For strings over 64 bytes we loop.  Internal state consists of
	// 56 bytes: v, w, x, y, and z.
	let x = seed;
	let y = seed.mul(k1).add(113);
	let z = shiftMix(y.mul(k2).add(113)).mul(k2);
	let v = [Long.UZERO, Long.UZERO];
	let w = [Long.UZERO, Long.UZERO];
	x = x.mul(k2).add(fetch64(s, 0));
	let offset = 0;
	// Set end so that after the loop we have 1 to 64 bytes left to process.
	const end = ((len - 1) >> 6) * 64;
	const last64 = end + ((len - 1) & 63) - 63;
	do {
		x = rotate64(x.add(y).add(v[0]).add(fetch64(s, offset + 8)), 37).mul(k1);
		y = rotate64(y.add(v[1]).add(fetch64(s, offset + 48)), 42).mul(k1);
		x = x.xor(w[1]);
		y = y.add(v[0]).add(fetch64(s, offset + 40));
		z = rotate64(z.add(w[0]), 33).mul(k1);
		v = weakHashLen32WithSeedsStr(s, offset, v[1].mul(k1), x.add(w[0]));
		w = weakHashLen32WithSeedsStr(s, offset + 32, z.add(w[1]), y.add(fetch64(s, offset + 16)));
		[z, x] = [x, z];
		offset += 64;
	} while (offset !== end);
	const mul = k1.add(z.and(0xff).shl(1));
	// Point to the last 64 bytes of input.
	offset = last64;
	w[0] = w[0].add((len - 1) & 63);
	v[0] = v[0].add(w[0]);
	w[0] = w[0].add(v[0]);
	x = rotate64(x.add(y).add(v[0]).add(fetch64(s, offset + 8)), 37).mul(mul);
	y = rotate64(y.add(v[1]).add(fetch64(s, offset + 48)), 42).mul(mul);
	x = x.xor(w[1].mul(9));
	y = y.add(v[0].mul(9).add(fetch64(s, offset + 40)));
	z = rotate64(z.add(w[0]), 33).mul(mul);
	v = weakHashLen32WithSeedsStr(s, offset, v[1].mul(mul), x.add(w[0]));
	w = weakHashLen32WithSeedsStr(s, offset + 32, z.add(w[1]), y.add(fetch64(s, offset + 16)));
	[z, x] = [x, z];
	return hashLen16(hashLen16(v[0], w[0], mul).add(shiftMix(y).mul(k0)).add(z), hashLen16(v[1], w[1], mul).add(x), mul);
}

/**
 * @license
 * Copyright 2017 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Create typed array for scalar value. Used for storing in `DataStorage`.
 */
function createScalarValue(value, dtype) {
	if (dtype === 'string') {
		return encodeString(value);
	}
	return toTypedArray([value], dtype);
}

function noConversionNeeded(a, dtype) {
	return (a instanceof Float32Array && dtype === 'float32') ||
		(a instanceof Int32Array && dtype === 'int32') ||
		(a instanceof Uint8Array && dtype === 'bool');
}

function toTypedArray(a, dtype) {
	if (dtype === 'string') {
		throw new Error('Cannot convert a string[] to a TypedArray');
	}
	if (Array.isArray(a)) {
		a = flatten(a);
	}
	if (env().getBool('DEBUG')) {
		checkConversionForErrors(a, dtype);
	}
	if (noConversionNeeded(a, dtype)) {
		return a;
	}
	if (dtype == null || dtype === 'float32' || dtype === 'complex64') {
		return new Float32Array(a);
	} else if (dtype === 'int32') {
		return new Int32Array(a);
	} else if (dtype === 'bool') {
		const bool = new Uint8Array(a.length);
		for (let i = 0; i < bool.length; ++i) {
			if (Math.round(a[i]) !== 0) {
				bool[i] = 1;
			}
		}
		return bool;
	} else {
		throw new Error(`Unknown data type ${dtype}`);
	}
}

/**
 * Returns the current high-resolution time in milliseconds relative to an
 * arbitrary time in the past. It works across different platforms (node.js,
 * browsers).
 *
 * ```js
 * console.log(tf.util.now());
 * ```
 *
 * @doc {heading: 'Util', namespace: 'util'}
 */
function now() {
	return env().platform.now();
}

/**
 * Returns a platform-specific implementation of
 * [`fetch`](https://developer.mozilla.org/en-US/docs/Web/API/Fetch_API).
 *
 * If `fetch` is defined on the global object (`window`, `process`, etc.),
 * `tf.util.fetch` returns that function.
 *
 * If not, `tf.util.fetch` returns a platform-specific solution.
 *
 * ```js
 * const resource = await tf.util.fetch('https://cdn.jsdelivr.net/npm/@tensorflow/tfjs');
 * // handle response
 * ```
 *
 * @doc {heading: 'Util'}
 */
function fetch$1(path, requestInits) {
	return env().platform.fetch(path, requestInits);
}

/**
 * Encodes the provided string into bytes using the provided encoding scheme.
 *
 * @param s The string to encode.
 * @param encoding The encoding scheme. Defaults to utf-8.
 *
 * @doc {heading: 'Util'}
 */
function encodeString(s, encoding = 'utf-8') {
	encoding = encoding || 'utf-8';
	return env().platform.encode(s, encoding);
}

/**
 * Decodes the provided bytes into a string using the provided encoding scheme.
 * @param bytes The bytes to decode.
 *
 * @param encoding The encoding scheme. Defaults to utf-8.
 *
 * @doc {heading: 'Util'}
 */
function decodeString(bytes, encoding = 'utf-8') {
	encoding = encoding || 'utf-8';
	return env().platform.decode(bytes, encoding);
}

function isTypedArray(a) {
	// TODO(mattsoulanille): Remove this fallback in 5.0.0
	if (env().platform.isTypedArray != null) {
		return env().platform.isTypedArray(a);
	} else {
		return isTypedArrayBrowser(a);
	}
}

// NOTE: We explicitly type out what T extends instead of any so that
// util.flatten on a nested array of number doesn't try to infer T as a
// number[][], causing us to explicitly type util.flatten<number>().
/**
 *  Flattens an arbitrarily nested array.
 *
 * ```js
 * const a = [[1, 2], [3, 4], [5, [6, [7]]]];
 * const flat = tf.util.flatten(a);
 * console.log(flat);
 * ```
 *
 *  @param arr The nested array to flatten.
 *  @param result The destination array which holds the elements.
 *  @param skipTypedArray If true, avoids flattening the typed arrays. Defaults
 *      to false.
 *
 * @doc {heading: 'Util', namespace: 'util'}
 */
function flatten(arr, result = [], skipTypedArray = false) {
	if (result == null) {
		result = [];
	}
	if (typeof arr === 'boolean' || typeof arr === 'number' ||
		typeof arr === 'string' || isPromise(arr) || arr == null ||
		isTypedArray(arr) && skipTypedArray) {
		result.push(arr);
	} else if (Array.isArray(arr) || isTypedArray(arr)) {
		for (let i = 0; i < arr.length; ++i) {
			flatten(arr[i], result, skipTypedArray);
		}
	} else {
		let maxIndex = -1;
		for (const key of Object.keys(arr)) {
			// 0 or positive integer.
			if (/^([1-9]+[0-9]*|0)$/.test(key)) {
				maxIndex = Math.max(maxIndex, Number(key));
			}
		}
		for (let i = 0; i <= maxIndex; i++) {
			// tslint:disable-next-line: no-unnecessary-type-assertion
			flatten(arr[i], result, skipTypedArray);
		}
	}
	return result;
}

var util = /*#__PURE__*/Object.freeze({
	__proto__: null,
	arraysEqual: arraysEqual,
	arraysEqualWithNull: arraysEqualWithNull,
	assert: assert,
	assertNonNegativeIntegerDimensions: assertNonNegativeIntegerDimensions,
	assertNonNull: assertNonNull,
	assertShapesMatch: assertShapesMatch,
	bytesFromStringArray: bytesFromStringArray,
	bytesPerElement: bytesPerElement,
	checkConversionForErrors: checkConversionForErrors,
	clamp: clamp,
	computeStrides: computeStrides,
	convertBackendValuesAndArrayBuffer: convertBackendValuesAndArrayBuffer,
	createScalarValue: createScalarValue,
	createShuffledIndices: createShuffledIndices,
	decodeString: decodeString,
	distSquared: distSquared,
	encodeString: encodeString,
	fetch: fetch$1,
	fingerPrint64: fingerPrint64,
	flatten: flatten,
	getArrayFromDType: getArrayFromDType,
	getTypedArrayFromDType: getTypedArrayFromDType,
	hasEncodingLoss: hasEncodingLoss,
	hexToLong: hexToLong,
	indexToLoc: indexToLoc,
	inferDtype: inferDtype,
	inferFromImplicitShape: inferFromImplicitShape,
	isBoolean: isBoolean,
	isFunction: isFunction,
	isInt: isInt,
	isNumber: isNumber,
	isPromise: isPromise,
	isScalarShape: isScalarShape,
	isString: isString,
	isTypedArray: isTypedArray,
	isValidDtype: isValidDtype,
	locToIndex: locToIndex,
	makeOnesTypedArray: makeOnesTypedArray,
	makeZerosNestedTypedArray: makeZerosNestedTypedArray,
	makeZerosTypedArray: makeZerosTypedArray,
	nearestDivisor: nearestDivisor,
	nearestLargerEven: nearestLargerEven,
	now: now,
	parseAxisParam: parseAxisParam,
	randUniform: randUniform,
	repeatedTry: repeatedTry,
	rightPad: rightPad,
	shuffle: shuffle,
	shuffleCombo: shuffleCombo,
	sizeFromShape: sizeFromShape,
	sizeToSquarishShape: sizeToSquarishShape,
	squeezeShape: squeezeShape,
	sum: sum$2,
	swap: swap,
	tanh: tanh$1,
	toNestedArray: toNestedArray,
	toTypedArray: toTypedArray
});

/**
 * @license
 * Copyright 2018 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
class Profiler {
	constructor(backendTimer, logger) {
		this.backendTimer = backendTimer;
		this.logger = logger;
		if (logger == null) {
			this.logger = new Logger();
		}
	}

	profileKernel(kernelName, inputs, f) {
		let outputs;
		const holdResultWrapperFn = () => {
			outputs = f();
		};
		let timer;
		const start = now();
		if (this.backendTimer.timerAvailable()) {
			timer = this.backendTimer.time(holdResultWrapperFn);
		} else {
			holdResultWrapperFn();
			for (const output of outputs) {
				output.dataSync();
			}
			timer = Promise.resolve({kernelMs: now() - start});
		}
		if (env().getBool('CHECK_COMPUTATION_FOR_ERRORS')) {
			for (let i = 0; i < outputs.length; i++) {
				const output = outputs[i];
				// Dangling promise here because we don't want to propagate up
				// asynchronicity.
				output.data().then(tensorVals => {
					checkComputationForErrors(tensorVals, output.dtype, kernelName);
				});
			}
		}
		const kernelProfile = {
			kernelName,
			outputs,
			inputs,
			timeMs: timer.then(timing => timing.kernelMs),
			extraInfo: timer.then(timing => timing.getExtraProfileInfo != null ?
				timing.getExtraProfileInfo() :
				'')
		};
		return kernelProfile;
	}

	logKernelProfile(kernelProfile) {
		const {kernelName, outputs, timeMs, inputs, extraInfo} = kernelProfile;
		outputs.forEach(result => {
			Promise.all([result.data(), timeMs, extraInfo]).then(valueContainer => {
				this.logger.logKernelProfile(kernelName, result, valueContainer[0], valueContainer[1], inputs, valueContainer[2]);
			});
		});
	}
}

function checkComputationForErrors(vals, dtype, kernelName) {
	if (dtype !== 'float32') {
		// Only floating point computations will generate NaN values
		return false;
	}
	for (let i = 0; i < vals.length; i++) {
		const num = vals[i];
		if (isNaN(num) || !isFinite(num)) {
			// Throwing custom exception so behavior is testable.
			console.warn(`Found ${num} in the result of '${kernelName}'`);
			return true;
		}
	}
	return false;
}

class Logger {
	logKernelProfile(name, result, vals, timeMs, inputs, extraInfo) {
		const time = typeof timeMs === 'number' ? rightPad(`${timeMs}ms`, 9) :
			timeMs['error'];
		const paddedName = rightPad(name, 25);
		const rank = result.rank;
		const size = result.size;
		const shape = rightPad(result.shape.toString(), 14);
		let inputShapesDescription = '';
		for (const name in inputs) {
			const input = inputs[name];
			if (input != null) {
				// The input might be a non-tensor (e.g HTMLImageElement), in which case
				// we claim the output shape as input shape.
				const inputShape = input.shape || result.shape;
				const inputRank = inputShape.length;
				inputShapesDescription +=
					`${name}: ${inputRank}D ${inputRank > 0 ? inputShape : ''} `;
			}
		}
		console.log(`%c${paddedName}\t%c${time}\t%c${rank}D ${shape}\t%c${size}\t%c${inputShapesDescription}\t%c${extraInfo}`, 'font-weight:bold', 'color:red', 'color:blue', 'color: orange', 'color: green', 'color: steelblue');
	}
}

/**
 * @license
 * Copyright 2017 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Computes a list of TapeNodes that connect x to y, filtering everything else
 * out and preserving the order of the original tape elements.
 *
 * @param tape The tape elements to filter.
 * @param xs The input Tensors.
 * @param y The output Tensor.
 */
function getFilteredNodesXToY(tape, xs, y) {
	// Forward pass to compute all the nodes and Tensors that are transitively a
	// function of x.
	const tensorsFromX = {};
	const nodesFromX = {};
	for (let i = 0; i < xs.length; i++) {
		tensorsFromX[xs[i].id] = true;
	}
	for (let i = 0; i < tape.length; i++) {
		const node = tape[i];
		const nodeInputs = node.inputs;
		for (const inputName in nodeInputs) {
			const input = nodeInputs[inputName];
			let anyInputFromX = false;
			for (let j = 0; j < xs.length; j++) {
				if (tensorsFromX[input.id]) {
					node.outputs.forEach(output => tensorsFromX[output.id] = true);
					anyInputFromX = true;
					nodesFromX[node.id] = true;
					break;
				}
			}
			if (anyInputFromX) {
				break;
			}
		}
	}
	// Backward pass to find all of the nodes and Tensors that lead to y.
	const tensorsLeadToY = {};
	tensorsLeadToY[y.id] = true;
	const nodesToY = {};
	for (let i = tape.length - 1; i >= 0; i--) {
		const node = tape[i];
		const nodeInputs = node.inputs;
		// If any of the outputs lead to y, mark all of the inputs as leading to y.
		for (let j = 0; j < node.outputs.length; j++) {
			if (tensorsLeadToY[node.outputs[j].id]) {
				for (const inputName in nodeInputs) {
					tensorsLeadToY[nodeInputs[inputName].id] = true;
					nodesToY[node.id] = true;
				}
				break;
			}
		}
	}
	// Return the paths that come from x and lead to y.
	const filteredTape = [];
	for (let i = 0; i < tape.length; i++) {
		const node = tape[i];
		if (nodesFromX[node.id] && nodesToY[node.id]) {
			// Prune the inputs from the node that aren't a function of x.
			const prunedInputs = {};
			for (const inputName in node.inputs) {
				const nodeInput = node.inputs[inputName];
				if (tensorsFromX[nodeInput.id]) {
					prunedInputs[inputName] = nodeInput;
				}
			}
			// Copy the node and overwrite inputsAndArgs to the pruned version.
			const prunedNode = Object.assign({}, node);
			prunedNode.inputs = prunedInputs;
			prunedNode.outputs = node.outputs;
			filteredTape.push(prunedNode);
		}
	}
	return filteredTape;
}

/**
 * Backpropagate gradients through the filtered TapeNodes.
 *
 * @param tensorAccumulatedGradientMap A map of Tensor to its gradient. This map
 * is mutated by this method.
 * @param filteredTape The filtered TapeNodes to backprop through.
 */
function backpropagateGradients(tensorAccumulatedGradientMap, filteredTape, tidy, add) {
	// Walk the tape backward and keep a map of Tensor to its gradient.
	for (let i = filteredTape.length - 1; i >= 0; i--) {
		const node = filteredTape[i];
		const dys = [];
		node.outputs.forEach(o => {
			const gradTensor = tensorAccumulatedGradientMap[o.id];
			if (gradTensor != null) {
				dys.push(gradTensor);
			} else {
				// This particular output is not in the back-propagation subgraph, so it
				// does not affect the final output, thus we put null for its dy.
				dys.push(null);
			}
		});
		if (node.gradient == null) {
			throw new Error(`Cannot compute gradient: gradient function not found ` +
				`for ${node.kernelName}.`);
		}
		// Backprop dy through this node and accumulate gradients over the inputs.
		const inputGradients = node.gradient(dys);
		for (const inputName in node.inputs) {
			if (!(inputName in inputGradients)) {
				throw new Error(`Cannot backprop through input ${inputName}. ` +
					`Available gradients found: ${Object.keys(inputGradients)}.`);
			}
			// Call the gradient function.
			const dx = tidy(() => inputGradients[inputName]());
			if (dx.dtype !== 'float32') {
				throw new Error(`Error in gradient for op ${node.kernelName}. The gradient of input ` +
					`${inputName} must have 'float32' dtype, but has '${dx.dtype}'`);
			}
			const x = node.inputs[inputName];
			if (!arraysEqual(dx.shape, x.shape)) {
				throw new Error(`Error in gradient for op ${node.kernelName}. The gradient of input ` +
					`'${inputName}' has shape '${dx.shape}', which does not match ` +
					`the shape of the input '${x.shape}'`);
			}
			if (tensorAccumulatedGradientMap[x.id] == null) {
				tensorAccumulatedGradientMap[x.id] = dx;
			} else {
				const curGradient = tensorAccumulatedGradientMap[x.id];
				tensorAccumulatedGradientMap[x.id] = add(curGradient, dx);
				curGradient.dispose();
			}
		}
	}
}

/**
 * @license
 * Copyright 2018 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
// Maximum number of values before we decide to show ellipsis.
const FORMAT_LIMIT_NUM_VALS = 20;
// Number of first and last values to show when displaying a, b,...,y, z.
const FORMAT_NUM_FIRST_LAST_VALS = 3;
// Number of significant digits to show.
const FORMAT_NUM_SIG_DIGITS = 7;

function tensorToString(vals, shape, dtype, verbose) {
	const strides = computeStrides(shape);
	const padPerCol = computeMaxSizePerColumn(vals, shape, dtype, strides);
	const rank = shape.length;
	const valsLines = subTensorToString(vals, shape, dtype, strides, padPerCol);
	const lines = ['Tensor'];
	if (verbose) {
		lines.push(`  dtype: ${dtype}`);
		lines.push(`  rank: ${rank}`);
		lines.push(`  shape: [${shape}]`);
		lines.push(`  values:`);
	}
	lines.push(valsLines.map(l => '    ' + l).join('\n'));
	return lines.join('\n');
}

function computeMaxSizePerColumn(vals, shape, dtype, strides) {
	const n = sizeFromShape(shape);
	const numCols = strides[strides.length - 1];
	const padPerCol = new Array(numCols).fill(0);
	const rank = shape.length;
	const valuesOrTuples = dtype === 'complex64' ? createComplexTuples(vals) : vals;
	if (rank > 1) {
		for (let row = 0; row < n / numCols; row++) {
			const offset = row * numCols;
			for (let j = 0; j < numCols; j++) {
				padPerCol[j] = Math.max(padPerCol[j], valToString(valuesOrTuples[offset + j], 0, dtype).length);
			}
		}
	}
	return padPerCol;
}

function valToString(val, pad, dtype) {
	let valStr;
	if (Array.isArray(val)) {
		valStr = `${parseFloat(val[0].toFixed(FORMAT_NUM_SIG_DIGITS))} + ` +
			`${parseFloat(val[1].toFixed(FORMAT_NUM_SIG_DIGITS))}j`;
	} else if (isString(val)) {
		valStr = `'${val}'`;
	} else if (dtype === 'bool') {
		valStr = boolNumToString(val);
	} else {
		valStr = parseFloat(val.toFixed(FORMAT_NUM_SIG_DIGITS)).toString();
	}
	return rightPad(valStr, pad);
}

function boolNumToString(v) {
	return v === 0 ? 'false' : 'true';
}

function subTensorToString(vals, shape, dtype, strides, padPerCol, isLast = true) {
	const storagePerElement = dtype === 'complex64' ? 2 : 1;
	const size = shape[0];
	const rank = shape.length;
	if (rank === 0) {
		if (dtype === 'complex64') {
			const complexTuple = createComplexTuples(vals);
			return [valToString(complexTuple[0], 0, dtype)];
		}
		if (dtype === 'bool') {
			return [boolNumToString(vals[0])];
		}
		return [vals[0].toString()];
	}
	if (rank === 1) {
		if (size > FORMAT_LIMIT_NUM_VALS) {
			const firstValsSize = FORMAT_NUM_FIRST_LAST_VALS * storagePerElement;
			let firstVals = Array.from(vals.slice(0, firstValsSize));
			let lastVals = Array.from(vals.slice((size - FORMAT_NUM_FIRST_LAST_VALS) * storagePerElement, size * storagePerElement));
			if (dtype === 'complex64') {
				firstVals = createComplexTuples(firstVals);
				lastVals = createComplexTuples(lastVals);
			}
			return [
				'[' +
				firstVals.map((x, i) => valToString(x, padPerCol[i], dtype))
						 .join(', ') +
				', ..., ' +
				lastVals
					.map((x, i) => valToString(x, padPerCol[size - FORMAT_NUM_FIRST_LAST_VALS + i], dtype))
					.join(', ') +
				']'
			];
		}
		const displayVals = dtype === 'complex64' ? createComplexTuples(vals) :
			Array.from(vals);
		return [
			'[' +
			displayVals.map((x, i) => valToString(x, padPerCol[i], dtype))
					   .join(', ') +
			']'
		];
	}
	// The array is rank 2 or more.
	const subshape = shape.slice(1);
	const substrides = strides.slice(1);
	const stride = strides[0] * storagePerElement;
	const lines = [];
	if (size > FORMAT_LIMIT_NUM_VALS) {
		for (let i = 0; i < FORMAT_NUM_FIRST_LAST_VALS; i++) {
			const start = i * stride;
			const end = start + stride;
			lines.push(...subTensorToString(vals.slice(start, end), subshape, dtype, substrides, padPerCol, false /* isLast */));
		}
		lines.push('...');
		for (let i = size - FORMAT_NUM_FIRST_LAST_VALS; i < size; i++) {
			const start = i * stride;
			const end = start + stride;
			lines.push(...subTensorToString(vals.slice(start, end), subshape, dtype, substrides, padPerCol, i === size - 1 /* isLast */));
		}
	} else {
		for (let i = 0; i < size; i++) {
			const start = i * stride;
			const end = start + stride;
			lines.push(...subTensorToString(vals.slice(start, end), subshape, dtype, substrides, padPerCol, i === size - 1 /* isLast */));
		}
	}
	const sep = rank === 2 ? ',' : '';
	lines[0] = '[' + (size > 0 ? lines[0] + sep : '');
	for (let i = 1; i < lines.length - 1; i++) {
		lines[i] = ' ' + lines[i] + sep;
	}
	let newLineSep = ',\n';
	for (let i = 2; i < rank; i++) {
		newLineSep += '\n';
	}
	lines[lines.length - 1] =
		' ' + lines[lines.length - 1] + ']' + (isLast ? '' : newLineSep);
	return lines;
}

function createComplexTuples(vals) {
	const complexTuples = [];
	for (let i = 0; i < vals.length; i += 2) {
		complexTuples.push([vals[i], vals[i + 1]]);
	}
	return complexTuples;
}

/**
 * @license
 * Copyright 2017 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
// Workaround for: https://github.com/bazelbuild/rules_nodejs/issues/1265
/// <reference types="@webgpu/types/dist" />
/**
 * A mutable object, similar to `tf.Tensor`, that allows users to set values
 * at locations before converting to an immutable `tf.Tensor`.
 *
 * See `tf.buffer` for creating a tensor buffer.
 *
 * @doc {heading: 'Tensors', subheading: 'Classes'}
 */
class TensorBuffer {
	constructor(shape, dtype, values) {
		this.dtype = dtype;
		this.shape = shape.slice();
		this.size = sizeFromShape(shape);
		if (values != null) {
			const n = values.length;
			assert(n === this.size, () => `Length of values '${n}' does not match the size ` +
				`inferred by the shape '${this.size}'.`);
		}
		if (dtype === 'complex64') {
			throw new Error(`complex64 dtype TensorBuffers are not supported. Please create ` +
				`a TensorBuffer for the real and imaginary parts separately and ` +
				`call tf.complex(real, imag).`);
		}
		this.values = values || getArrayFromDType(dtype, this.size);
		this.strides = computeStrides(shape);
	}

	/**
	 * Sets a value in the buffer at a given location.
	 *
	 * @param value The value to set.
	 * @param locs  The location indices.
	 *
	 * @doc {heading: 'Tensors', subheading: 'Creation'}
	 */
	set(value, ...locs) {
		if (locs.length === 0) {
			locs = [0];
		}
		assert(locs.length === this.rank, () => `The number of provided coordinates (${locs.length}) must ` +
			`match the rank (${this.rank})`);
		const index = this.locToIndex(locs);
		this.values[index] = value;
	}

	/**
	 * Returns the value in the buffer at the provided location.
	 *
	 * @param locs The location indices.
	 *
	 * @doc {heading: 'Tensors', subheading: 'Creation'}
	 */
	get(...locs) {
		if (locs.length === 0) {
			locs = [0];
		}
		let i = 0;
		for (const loc of locs) {
			if (loc < 0 || loc >= this.shape[i]) {
				const msg = `Requested out of range element at ${locs}. ` +
					`  Buffer shape=${this.shape}`;
				throw new Error(msg);
			}
			i++;
		}
		let index = locs[locs.length - 1];
		for (let i = 0; i < locs.length - 1; ++i) {
			index += this.strides[i] * locs[i];
		}
		return this.values[index];
	}

	locToIndex(locs) {
		if (this.rank === 0) {
			return 0;
		} else if (this.rank === 1) {
			return locs[0];
		}
		let index = locs[locs.length - 1];
		for (let i = 0; i < locs.length - 1; ++i) {
			index += this.strides[i] * locs[i];
		}
		return index;
	}

	indexToLoc(index) {
		if (this.rank === 0) {
			return [];
		} else if (this.rank === 1) {
			return [index];
		}
		const locs = new Array(this.shape.length);
		for (let i = 0; i < locs.length - 1; ++i) {
			locs[i] = Math.floor(index / this.strides[i]);
			index -= locs[i] * this.strides[i];
		}
		locs[locs.length - 1] = index;
		return locs;
	}

	get rank() {
		return this.shape.length;
	}

	/**
	 * Creates an immutable `tf.Tensor` object from the buffer.
	 *
	 * @doc {heading: 'Tensors', subheading: 'Creation'}
	 */
	toTensor() {
		return trackerFn().makeTensor(this.values, this.shape, this.dtype);
	}
}

// For tracking tensor creation and disposal.
let trackerFn = null;
// Used by chaining methods to call into ops.
let opHandler$1 = null;

/**
 * An external consumer can register itself as the tensor tracker. This way
 * the Tensor class can notify the tracker for every tensor created and
 * disposed.
 */
function setTensorTracker(fn) {
	trackerFn = fn;
}

/**
 * An external consumer can register itself as the op handler. This way the
 * Tensor class can have chaining methods that call into ops via the op
 * handler.
 */
function setOpHandler(handler) {
	opHandler$1 = handler;
}

/**
 * A `tf.Tensor` object represents an immutable, multidimensional array of
 * numbers that has a shape and a data type.
 *
 * For performance reasons, functions that create tensors do not necessarily
 * perform a copy of the data passed to them (e.g. if the data is passed as a
 * `Float32Array`), and changes to the data will change the tensor. This is not
 * a feature and is not supported. To avoid this behavior, use the tensor before
 * changing the input data or create a copy with `copy = tf.add(yourTensor, 0)`.
 *
 * See `tf.tensor` for details on how to create a `tf.Tensor`.
 *
 * @doc {heading: 'Tensors', subheading: 'Classes'}
 */
class Tensor {
	constructor(shape, dtype, dataId, id) {
		/** Whether this tensor has been globally kept. */
		this.kept = false;
		this.isDisposedInternal = false;
		this.shape = shape.slice();
		this.dtype = dtype || 'float32';
		this.size = sizeFromShape(shape);
		this.strides = computeStrides(shape);
		this.dataId = dataId;
		this.id = id;
		this.rankType = (this.rank < 5 ? this.rank.toString() : 'higher');
	}

	get rank() {
		return this.shape.length;
	}

	/**
	 * Returns a promise of `tf.TensorBuffer` that holds the underlying data.
	 *
	 * @doc {heading: 'Tensors', subheading: 'Classes'}
	 */
	async buffer() {
		const vals = await this.data();
		return opHandler$1.buffer(this.shape, this.dtype, vals);
	}

	/**
	 * Returns a `tf.TensorBuffer` that holds the underlying data.
	 * @doc {heading: 'Tensors', subheading: 'Classes'}
	 */
	bufferSync() {
		return opHandler$1.buffer(this.shape, this.dtype, this.dataSync());
	}

	/**
	 * Returns the tensor data as a nested array. The transfer of data is done
	 * asynchronously.
	 *
	 * @doc {heading: 'Tensors', subheading: 'Classes'}
	 */
	async array() {
		const vals = await this.data();
		return toNestedArray(this.shape, vals, this.dtype === 'complex64');
	}

	/**
	 * Returns the tensor data as a nested array. The transfer of data is done
	 * synchronously.
	 *
	 * @doc {heading: 'Tensors', subheading: 'Classes'}
	 */
	arraySync() {
		return toNestedArray(this.shape, this.dataSync(), this.dtype === 'complex64');
	}

	/**
	 * Asynchronously downloads the values from the `tf.Tensor`. Returns a
	 * promise of `TypedArray` that resolves when the computation has finished.
	 *
	 * @doc {heading: 'Tensors', subheading: 'Classes'}
	 */
	async data() {
		this.throwIfDisposed();
		const data = trackerFn().read(this.dataId);
		if (this.dtype === 'string') {
			const bytes = await data;
			try {
				return bytes.map(b => decodeString(b));
			} catch (_a) {
				throw new Error('Failed to decode the string bytes into utf-8. ' +
					'To get the original bytes, call tensor.bytes().');
			}
		}
		return data;
	}

	/**
	 * Copy the tensor's data to a new GPU resource. Comparing to the `dataSync()`
	 * and `data()`, this method prevents data from being downloaded to CPU.
	 *
	 * For WebGL backend, the data will be stored on a densely packed texture.
	 * This means that the texture will use the RGBA channels to store value.
	 *
	 * For WebGPU backend, the data will be stored on a buffer. There is no
	 * parameter, so can not use a user-defined size to create the buffer.
	 *
	 * @param options:
	 *     For WebGL,
	 *         - customTexShape: Optional. If set, will use the user defined
	 *     texture shape to create the texture.
	 *
	 * @returns For WebGL backend, a GPUData contains the new texture and
	 *     its information.
	 *     {
	 *        tensorRef: The tensor that is associated with this texture,
	 *        texture: WebGLTexture,
	 *        texShape: [number, number] // [height, width]
	 *     }
	 *
	 *     For WebGPU backend, a GPUData contains the new buffer.
	 *     {
	 *        tensorRef: The tensor that is associated with this buffer,
	 *        buffer: GPUBuffer,
	 *     }
	 *
	 *     Remember to dispose the GPUData after it is used by
	 *     `res.tensorRef.dispose()`.
	 *
	 * @doc {heading: 'Tensors', subheading: 'Classes'}
	 */
	dataToGPU(options) {
		this.throwIfDisposed();
		return trackerFn().readToGPU(this.dataId, options);
	}

	/**
	 * Synchronously downloads the values from the `tf.Tensor`. This blocks the
	 * UI thread until the values are ready, which can cause performance issues.
	 *
	 * @doc {heading: 'Tensors', subheading: 'Classes'}
	 */
	dataSync() {
		this.throwIfDisposed();
		const data = trackerFn().readSync(this.dataId);
		if (this.dtype === 'string') {
			try {
				return data.map(b => decodeString(b));
			} catch (_a) {
				throw new Error('Failed to decode the string bytes into utf-8. ' +
					'To get the original bytes, call tensor.bytes().');
			}
		}
		return data;
	}

	/** Returns the underlying bytes of the tensor's data. */
	async bytes() {
		this.throwIfDisposed();
		const data = await trackerFn().read(this.dataId);
		if (this.dtype === 'string') {
			return data;
		} else {
			return new Uint8Array(data.buffer);
		}
	}

	/**
	 * Disposes `tf.Tensor` from memory.
	 *
	 * @doc {heading: 'Tensors', subheading: 'Classes'}
	 */
	dispose() {
		if (this.isDisposed) {
			return;
		}
		if (this.kerasMask) {
			this.kerasMask.dispose();
		}
		trackerFn().disposeTensor(this);
		this.isDisposedInternal = true;
	}

	get isDisposed() {
		return this.isDisposedInternal;
	}

	throwIfDisposed() {
		if (this.isDisposed) {
			throw new Error(`Tensor is disposed.`);
		}
	}

	/**
	 * Prints the `tf.Tensor`. See `tf.print` for details.
	 *
	 * @param verbose Whether to print verbose information about the tensor,
	 *    including dtype and size.
	 *
	 * @doc {heading: 'Tensors', subheading: 'Classes'}
	 */
	print(verbose = false) {
		return opHandler$1.print(this, verbose);
	}

	/**
	 * Returns a copy of the tensor. See `tf.clone` for details.
	 * @doc {heading: 'Tensors', subheading: 'Classes'}
	 */
	clone() {
		this.throwIfDisposed();
		return opHandler$1.clone(this);
	}

	/**
	 * Returns a human-readable description of the tensor. Useful for logging.
	 *
	 * @doc {heading: 'Tensors', subheading: 'Classes'}
	 */
	toString(verbose = false) {
		const vals = this.dataSync();
		return tensorToString(vals, this.shape, this.dtype, verbose);
	}

	cast(dtype) {
		this.throwIfDisposed();
		return opHandler$1.cast(this, dtype);
	}

	variable(trainable = true, name, dtype) {
		this.throwIfDisposed();
		return trackerFn().makeVariable(this, trainable, name, dtype);
	}
}

Object.defineProperty(Tensor, Symbol.hasInstance, {
	value: (instance) => {
		// Implementation note: we should use properties of the object that will be
		// defined before the constructor body has finished executing (methods).
		// This is because when this code is transpiled by babel, babel will call
		// classCallCheck before the constructor body is run.
		// See https://github.com/tensorflow/tfjs/issues/3384 for backstory.
		return !!instance && instance.data != null && instance.dataSync != null &&
			instance.throwIfDisposed != null;
	}
});

function getGlobalTensorClass() {
	// Use getGlobal so that we can augment the Tensor class across package
	// boundaries because the node resolution alg may result in different modules
	// being returned for this file depending on the path they are loaded from.
	return getGlobal('Tensor', () => {
		return Tensor;
	});
}

// Global side effect. Cache global reference to Tensor class
getGlobalTensorClass();

/**
 * A mutable `tf.Tensor`, useful for persisting state, e.g. for training.
 *
 * @doc {heading: 'Tensors', subheading: 'Classes'}
 */
class Variable extends Tensor {
	constructor(initialValue, trainable, name, tensorId) {
		super(initialValue.shape, initialValue.dtype, initialValue.dataId, tensorId);
		this.trainable = trainable;
		this.name = name;
	}

	/**
	 * Assign a new `tf.Tensor` to this variable. The new `tf.Tensor` must have
	 * the same shape and dtype as the old `tf.Tensor`.
	 *
	 * @param newValue New tensor to be assigned to this variable.
	 *
	 * @doc {heading: 'Tensors', subheading: 'Classes'}
	 */
	assign(newValue) {
		if (newValue.dtype !== this.dtype) {
			throw new Error(`dtype of the new value (${newValue.dtype}) and ` +
				`previous value (${this.dtype}) must match`);
		}
		if (!arraysEqual(newValue.shape, this.shape)) {
			throw new Error(`shape of the new value (${newValue.shape}) and ` +
				`previous value (${this.shape}) must match`);
		}
		trackerFn().disposeTensor(this);
		this.dataId = newValue.dataId;
		trackerFn().incRef(this, null /* backend */);
	}

	dispose() {
		trackerFn().disposeVariable(this);
		this.isDisposedInternal = true;
	}
}

Object.defineProperty(Variable, Symbol.hasInstance, {
	value: (instance) => {
		return instance instanceof Tensor && instance.assign != null &&
			instance.assign instanceof Function;
	}
});

/**
 * @license
 * Copyright 2017 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
var Rank;
(function (Rank) {
	Rank["R0"] = "R0";
	Rank["R1"] = "R1";
	Rank["R2"] = "R2";
	Rank["R3"] = "R3";
	Rank["R4"] = "R4";
	Rank["R5"] = "R5";
	Rank["R6"] = "R6";
})(Rank || (Rank = {}));
// Looks for upcasting types. Used, for example, in operations with mixed dtype
// inputs.
var UpcastInt32AndMap;
(function (UpcastInt32AndMap) {
	UpcastInt32AndMap["float32"] = "float32";
	UpcastInt32AndMap["int32"] = "int32";
	UpcastInt32AndMap["bool"] = "int32";
	UpcastInt32AndMap["complex64"] = "complex64";
})(UpcastInt32AndMap || (UpcastInt32AndMap = {}));
var UpcastBoolAndMap;
(function (UpcastBoolAndMap) {
	UpcastBoolAndMap["float32"] = "float32";
	UpcastBoolAndMap["int32"] = "int32";
	UpcastBoolAndMap["bool"] = "bool";
	UpcastBoolAndMap["complex64"] = "complex64";
})(UpcastBoolAndMap || (UpcastBoolAndMap = {}));
var UpcastFloat32AndMap;
(function (UpcastFloat32AndMap) {
	UpcastFloat32AndMap["float32"] = "float32";
	UpcastFloat32AndMap["int32"] = "float32";
	UpcastFloat32AndMap["bool"] = "float32";
	UpcastFloat32AndMap["complex64"] = "complex64";
})(UpcastFloat32AndMap || (UpcastFloat32AndMap = {}));
var UpcastComplex64AndMap;
(function (UpcastComplex64AndMap) {
	UpcastComplex64AndMap["float32"] = "complex64";
	UpcastComplex64AndMap["int32"] = "complex64";
	UpcastComplex64AndMap["bool"] = "complex64";
	UpcastComplex64AndMap["complex64"] = "complex64";
})(UpcastComplex64AndMap || (UpcastComplex64AndMap = {}));
const upcastTypeMap = {
	'float32': UpcastFloat32AndMap,
	'int32': UpcastInt32AndMap,
	'bool': UpcastBoolAndMap,
	'complex64': UpcastComplex64AndMap
};

function upcastType(typeA, typeB) {
	if (typeA === 'string' || typeB === 'string') {
		if (typeA === 'string' && typeB === 'string') {
			return 'string';
		}
		throw new Error(`Can not upcast ${typeA} with ${typeB}`);
	}
	return upcastTypeMap[typeA][typeB];
}

/** Returns the output type after summation. */
function sumOutType(type) {
	return upcastType(type, 'int32');
}

function isWebGLData(values) {
	return values != null && typeof values === 'object' && 'texture' in values &&
		values.texture instanceof WebGLTexture;
}

function isWebGPUData(values) {
	return typeof GPUBuffer !== 'undefined' && values != null &&
		typeof values === 'object' && 'buffer' in values &&
		values.buffer instanceof GPUBuffer;
}

/**
 * @license
 * Copyright 2018 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
function makeTypesMatch(a, b) {
	if (a.dtype === b.dtype) {
		return [a, b];
	}
	const dtype = upcastType(a.dtype, b.dtype);
	return [a.cast(dtype), b.cast(dtype)];
}

function assertTypesMatch(a, b) {
	assert(a.dtype === b.dtype, () => `The dtypes of the first(${a.dtype}) and` +
		` second(${b.dtype}) input must match`);
}

function isTensorInList(tensor, tensorList) {
	return tensorList.some(x => x.id === tensor.id);
}

/**
 * Extracts any `Tensor`s found within the provided object.
 *
 * @param container an object that may be a `Tensor` or may directly contain
 *   `Tensor`s, such as a `Tensor[]` or `{key: Tensor, ...}`. In general it
 *   is safe to pass any object here, except that `Promise`s are not
 *   supported.
 * @returns An array of `Tensors` found within the passed object. If the
 *   argument is simply a `Tensor', a list containing that `Tensor` is
 *   returned. If the object is not a `Tensor` or does not
 *   contain `Tensors`, an empty list is returned.
 */
function getTensorsInContainer(result) {
	const list = [];
	const seen = new Set();
	walkTensorContainer(result, list, seen);
	return list;
}

function walkTensorContainer(container, list, seen) {
	if (container == null) {
		return;
	}
	if (container instanceof Tensor) {
		list.push(container);
		return;
	}
	if (!isIterable(container)) {
		return;
	}
	// Iteration over keys works also for arrays.
	const iterable = container;
	for (const k in iterable) {
		const val = iterable[k];
		if (!seen.has(val)) {
			seen.add(val);
			walkTensorContainer(val, list, seen);
		}
	}
}

// tslint:disable-next-line:no-any
function isIterable(obj) {
	return Array.isArray(obj) || typeof obj === 'object';
}

var tensor_util = /*#__PURE__*/Object.freeze({
	__proto__: null,
	assertTypesMatch: assertTypesMatch,
	getTensorsInContainer: getTensorsInContainer,
	isTensorInList: isTensorInList,
	makeTypesMatch: makeTypesMatch
});

/**
 * @license
 * Copyright 2018 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
function isRegisteredKernelInvocation(kernelInvocation) {
	return kernelInvocation.kernelName != null;
}

class EngineState {
	constructor() {
		// Public since optimizers will use it.
		this.registeredVariables = {};
		this.nextTapeNodeId = 0;
		this.numBytes = 0;
		this.numTensors = 0;
		this.numStringTensors = 0;
		this.numDataBuffers = 0;
		// Number of nested tf.grad() statements when computing higher-order
		// gradients. E.g. `1` for first-order gradients and `2` for second-order
		// gradients. Used to track if the tape should be removed after a backprop.
		this.gradientDepth = 0;
		// Number of nested kernel calls. When kernel depth is greater than 1, we turn
		// off the tape.
		this.kernelDepth = 0;
		this.scopeStack = [];
		/**
		 * Keeps track of the number of data moves during a kernel execution. We
		 * maintain a stack since kernels can call other kernels, recursively.
		 */
		this.numDataMovesStack = [];
		this.nextScopeId = 0;
		this.tensorInfo = new WeakMap();
		this.profiling = false;
		this.activeProfile = {
			newBytes: 0,
			newTensors: 0,
			peakBytes: 0,
			kernels: [],
			result: null,
			get kernelNames() {
				return Array.from(new Set(this.kernels.map(k => k.name)));
			}
		};
	}

	dispose() {
		for (const variableName in this.registeredVariables) {
			this.registeredVariables[variableName].dispose();
		}
	}
}

class Engine {
	constructor(ENV) {
		this.ENV = ENV;
		this.registry = {};
		this.registryFactory = {};
		this.pendingBackendInitId = 0;
		this.state = new EngineState();
	}

	async ready() {
		if (this.pendingBackendInit != null) {
			return this.pendingBackendInit.then(() => {
			});
		}
		if (this.backendInstance != null) {
			return;
		}
		const sortedBackends = this.getSortedBackends();
		for (let i = 0; i < sortedBackends.length; i++) {
			const backendName = sortedBackends[i];
			const success = await this.initializeBackend(backendName).success;
			if (success) {
				await this.setBackend(backendName);
				return;
			}
		}
		throw new Error(`Could not initialize any backends, all backend initializations ` +
			`failed.`);
	}

	get backend() {
		if (this.pendingBackendInit != null) {
			throw new Error(`Backend '${this.backendName}' has not yet been initialized. Make ` +
				`sure to await tf.ready() or await tf.setBackend() before calling ` +
				`other methods`);
		}
		if (this.backendInstance == null) {
			const {name, asyncInit} = this.initializeBackendsAndReturnBest();
			if (asyncInit) {
				throw new Error(`The highest priority backend '${name}' has not yet been ` +
					`initialized. Make sure to await tf.ready() or ` +
					`await tf.setBackend() before calling other methods`);
			}
			this.setBackend(name);
		}
		return this.backendInstance;
	}

	backendNames() {
		return Object.keys(this.registryFactory);
	}

	findBackend(backendName) {
		if (!(backendName in this.registry)) {
			// If the backend hasn't been initialized but we have a registry entry for
			// it, initialize it and return it.
			if (backendName in this.registryFactory) {
				const {asyncInit} = this.initializeBackend(backendName);
				if (asyncInit) {
					// Backend is not ready yet.
					return null;
				}
			} else {
				return null;
			}
		}
		return this.registry[backendName];
	}

	findBackendFactory(backendName) {
		if (!(backendName in this.registryFactory)) {
			return null;
		}
		return this.registryFactory[backendName].factory;
	}

	registerBackend(backendName, factory, priority = 1) {
		if (backendName in this.registryFactory) {
			warn(`${backendName} backend was already registered. ` +
				`Reusing existing backend factory.`);
			return false;
		}
		this.registryFactory[backendName] = {factory, priority};
		return true;
	}

	async setBackend(backendName) {
		if (this.registryFactory[backendName] == null) {
			throw new Error(`Backend name '${backendName}' not found in registry`);
		}
		this.backendName = backendName;
		if (this.registry[backendName] == null) {
			this.backendInstance = null;
			const {success, asyncInit} = this.initializeBackend(backendName);
			const result = asyncInit ? await success : success;
			if (!result) {
				return false;
			}
		}
		this.backendInstance = this.registry[backendName];
		this.setupRegisteredKernels();
		// Reset the profiler.
		this.profiler = new Profiler(this.backendInstance);
		return true;
	}

	setupRegisteredKernels() {
		const kernels = getKernelsForBackend(this.backendName);
		kernels.forEach(kernel => {
			if (kernel.setupFunc != null) {
				kernel.setupFunc(this.backendInstance);
			}
		});
	}

	disposeRegisteredKernels(backendName) {
		const kernels = getKernelsForBackend(backendName);
		kernels.forEach(kernel => {
			if (kernel.disposeFunc != null) {
				kernel.disposeFunc(this.registry[backendName]);
			}
		});
	}

	/**
	 * Initializes a backend by looking up the backend name in the factory
	 * registry and calling the factory method. Returns a boolean representing
	 * whether the initialization of the backend succeeded. Throws an error if
	 * there is no backend in the factory registry.
	 */
	initializeBackend(backendName) {
		const registryFactoryEntry = this.registryFactory[backendName];
		if (registryFactoryEntry == null) {
			throw new Error(`Cannot initialize backend ${backendName}, no registration found.`);
		}
		try {
			const backend = registryFactoryEntry.factory();
			/* Test if the factory returns a promise.
            Done in a more liberal way than
            previous 'Promise.resolve(backend)===backend'
            as we needed to account for custom Promise
            implementations (e.g. Angular) */
			if (backend && !(backend instanceof KernelBackend) &&
				typeof backend.then === 'function') {
				const promiseId = ++this.pendingBackendInitId;
				const success = backend
					.then(backendInstance => {
						// Outdated promise. Another backend was set in the meantime.
						if (promiseId < this.pendingBackendInitId) {
							return false;
						}
						this.registry[backendName] = backendInstance;
						this.pendingBackendInit = null;
						return true;
					})
					.catch(err => {
						// Outdated promise. Another backend was set in the meantime.
						if (promiseId < this.pendingBackendInitId) {
							return false;
						}
						this.pendingBackendInit = null;
						warn(`Initialization of backend ${backendName} failed`);
						warn(err.stack || err.message);
						return false;
					});
				this.pendingBackendInit = success;
				return {success, asyncInit: true};
			} else {
				this.registry[backendName] = backend;
				return {success: true, asyncInit: false};
			}
		} catch (err) {
			warn(`Initialization of backend ${backendName} failed`);
			warn(err.stack || err.message);
			return {success: false, asyncInit: false};
		}
	}

	removeBackend(backendName) {
		if (!(backendName in this.registryFactory)) {
			throw new Error(`${backendName} backend not found in registry`);
		}
		if (this.backendName === backendName && this.pendingBackendInit != null) {
			// There is a pending promise of the backend we want to remove. Make it
			// obsolete.
			this.pendingBackendInitId++;
		}
		if (backendName in this.registry) {
			this.disposeRegisteredKernels(backendName);
			this.registry[backendName].dispose();
			delete this.registry[backendName];
		}
		delete this.registryFactory[backendName];
		// Unset the backend if it is active.
		if (this.backendName === backendName) {
			this.pendingBackendInit = null;
			this.backendName = null;
			this.backendInstance = null;
		}
	}

	getSortedBackends() {
		if (Object.keys(this.registryFactory).length === 0) {
			throw new Error('No backend found in registry.');
		}
		return Object.keys(this.registryFactory).sort((a, b) => {
			// Highest priority comes first.
			return this.registryFactory[b].priority -
				this.registryFactory[a].priority;
		});
	}

	initializeBackendsAndReturnBest() {
		const sortedBackends = this.getSortedBackends();
		for (let i = 0; i < sortedBackends.length; i++) {
			const backendName = sortedBackends[i];
			const {success, asyncInit} = this.initializeBackend(backendName);
			if (asyncInit || success) {
				return {name: backendName, asyncInit};
			}
		}
		throw new Error(`Could not initialize any backends, all backend initializations ` +
			`failed.`);
	}

	moveData(backend, dataId) {
		const info = this.state.tensorInfo.get(dataId);
		const srcBackend = info.backend;
		const values = this.readSync(dataId);
		const refCount = srcBackend.refCount(dataId);
		// Delete the tensor from the old backend and move it to the new
		// backend.
		srcBackend.disposeData(dataId, true);
		info.backend = backend;
		backend.move(dataId, values, info.shape, info.dtype, refCount);
		if (this.shouldCheckForMemLeaks()) {
			// Track the number of moves during a kernel execution to correctly
			// detect memory leaks.
			this.state.numDataMovesStack[this.state.numDataMovesStack.length - 1]++;
		}
	}

	tidy(nameOrFn, fn) {
		let name = null;
		if (fn == null) {
			// Called with only 1 argument.
			if (typeof nameOrFn !== 'function') {
				throw new Error('Please provide a function to tidy()');
			}
			fn = nameOrFn;
		} else {
			// Called with 2 arguments.
			if (typeof nameOrFn !== 'string' && !(nameOrFn instanceof String)) {
				throw new Error('When calling with two arguments, the first argument ' +
					'to tidy() must be a string');
			}
			if (typeof fn !== 'function') {
				throw new Error('When calling with two arguments, the 2nd argument ' +
					'to tidy() must be a function');
			}
			name = nameOrFn;
			// TODO(nsthorat,smilkov): Do operation logging and performance
			// profiling.
		}
		let result;
		return this.scopedRun(() => this.startScope(name), () => this.endScope(result), () => {
			result = fn();
			if (result instanceof Promise) {
				console.error('Cannot return a Promise inside of tidy.');
			}
			return result;
		});
	}

	scopedRun(start, end, f) {
		start();
		try {
			const res = f();
			end();
			return res;
		} catch (ex) {
			end();
			throw ex;
		}
	}

	nextTensorId() {
		return Engine.nextTensorId++;
	}

	nextVariableId() {
		return Engine.nextVariableId++;
	}

	/**
	 * This method is called instead of the public-facing tensor.clone() when
	 * saving a tensor for backwards pass. It makes sure to add the clone
	 * operation to the tape regardless of being called inside a kernel
	 * execution.
	 */
	clone(x) {
		const y = ENGINE.runKernel(Identity, {x});
		const inputs = {x};
		const grad = (dy) => ({
			x: () => {
				const dtype = 'float32';
				const gradInputs = {x: dy};
				const attrs = {dtype};
				return ENGINE.runKernel(Cast, gradInputs,
					// tslint:disable-next-line: no-unnecessary-type-assertion
					attrs);
			}
		});
		const saved = [];
		this.addTapeNode(this.state.activeScope.name, inputs, [y], grad, saved, {});
		return y;
	}

	/**
	 * Execute a kernel with the given name and return the output tensor.
	 *
	 * @param kernelName The name of the kernel to execute.
	 * @param inputs A map of input names to tensors.
	 * @param attrs A map of attribute names to their values. An attribute is a
	 *     primitive (non-tensor) input to the kernel.
	 * @param inputsToSave A list of tensors, inputs to save for the backprop
	 *     computation.
	 * @param outputsToSave A list of booleans, specifying which output to save
	 *     for the backprop computation. These are booleans since the output
	 * tensors are not visible to the user.
	 */
	runKernel(kernelName, inputs, attrs) {
		if (this.backendName == null) {
			// backend has not been initialized yet (backend initialization is lazy
			// can be deferred until an op/ kernel is run).
			// The below getter has side effects that will try to initialize the
			// backend and set properties like this.backendName
			// tslint:disable-next-line: no-unused-expression
			this.backend;
		}
		const hasKernel = getKernel(kernelName, this.backendName) != null;
		if (!hasKernel) {
			throw new Error(`Kernel '${kernelName}' not registered for backend '${this.backendName}'`);
		}
		return this.runKernelFunc({kernelName, inputs, attrs});
	}

	shouldCheckForMemLeaks() {
		return this.ENV.getBool('IS_TEST');
	}

	checkKernelForMemLeak(kernelName, numDataIdsBefore, outInfos) {
		const numDataIdsAfter = this.backend.numDataIds();
		// Count the number of data ids associated with the result of the kernel.
		let numOutputDataIds = 0;
		outInfos.forEach(info => {
			// Complex numbers allocate 3 data ids, one for 'real', one for
			// 'imaginary', and one for the container that holds the former two.
			numOutputDataIds += (info.dtype === 'complex64' ? 3 : 1);
		});
		// Account for the number of moves during kernel execution. A "data move"
		// can happen in the middle of a kernel execution, placing a new (key,value)
		// pair in the data storage. Since data moves have net zero effect (we
		// always remove the data from the old backend), we have to cancel them out
		// when detecting memory leaks.
		const numMoves = this.state.numDataMovesStack[this.state.numDataMovesStack.length - 1];
		const dataIdsLeaked = numDataIdsAfter - numDataIdsBefore - numOutputDataIds - numMoves;
		if (dataIdsLeaked > 0) {
			throw new Error(`Backend '${this.backendName}' has an internal memory leak ` +
				`(${dataIdsLeaked} data ids) after running '${kernelName}'`);
		}
	}

	/**
	 * Internal helper method to execute a kernel Func
	 *
	 * Use `runKernel` to execute kernels from outside of engine.
	 */
	runKernelFunc(kernelParams) {
		let outputs;
		let saved = [];
		const isTapeOn = this.isTapeOn();
		const startingBytecount = this.state.numBytes;
		const startingNumTensors = this.state.numTensors;
		if (this.shouldCheckForMemLeaks()) {
			this.state.numDataMovesStack.push(0);
		}
		let kernelFunc;
		if (this.backendName == null) {
			// backend has not been initialized yet (backend initialization is lazy
			// can be deferred until an op/ kernel is run).
			// The below getter has side effects that will try to initialize the
			// backend and set properties like this.backendName
			// tslint:disable-next-line: no-unused-expression
			this.backend;
		}
		let out;
		const kernelOrScopeName = isRegisteredKernelInvocation(kernelParams) ?
			kernelParams.kernelName :
			this.state.activeScope != null ? this.state.activeScope.name : '';
		// Create the kernelFunc from either a registered kernel OR passed in
		// forward/backward functions (used by custom grad). In this context a
		// kernelFunc wraps a kernel implementation with some bookkeeping.
		if (isRegisteredKernelInvocation(kernelParams)) {
			const {kernelName, inputs, attrs} = kernelParams;
			if (this.backendName == null) {
				// backend has not been initialized yet (backend initialization is lazy
				// can be deferred until an op/ kernel is run).
				// The below getter has side effects that will try to initialize the
				// backend and set properties like this.backendName
				// tslint:disable-next-line: no-unused-expression
				this.backend;
			}
			const kernel = getKernel(kernelName, this.backendName);
			assert(kernel != null, () => `Cannot find registered kernel '${kernelName}' for backend '${this.backendName}'`);
			kernelFunc = () => {
				const numDataIdsBefore = this.backend.numDataIds();
				out = kernel.kernelFunc({inputs, attrs, backend: this.backend});
				const outInfos = Array.isArray(out) ? out : [out];
				if (this.shouldCheckForMemLeaks()) {
					this.checkKernelForMemLeak(kernelName, numDataIdsBefore, outInfos);
				}
				const outTensors = outInfos.map((outInfo) => {
					// todo (yassogba) remove this option (Tensor) when node backend
					// methods have been modularized and they all return tensorInfo.
					// TensorInfos do not have a rank attribute.
					if (outInfo.rank != null) {
						return outInfo;
					}
					return this.makeTensorFromTensorInfo(outInfo);
				});
				// Save any required inputs and outputs.
				// Do not save unless we are recording to the tape. Otherwise it would
				// cause a mem leak since there would be no backprop for these tensors
				// (which would otherwise dispose them).
				if (isTapeOn) {
					const tensorsToSave = this.getTensorsForGradient(kernelName, inputs, outTensors);
					saved = this.saveTensorsForBackwardMode(tensorsToSave);
				}
				return outTensors;
			};
		} else {
			const {forwardFunc} = kernelParams;
			// Running a customGrad op.
			const saveFunc = (tensors) => {
				// Do not save unless we are recording to the tape. Otherwise it would
				// cause a mem leak since we would never run backprop, which disposes
				// the kept tensors.
				if (!isTapeOn) {
					return;
				}
				saved = tensors.map(tensor => this.keep(this.clone(tensor)));
			};
			kernelFunc = () => {
				const numDataIdsBefore = this.backend.numDataIds();
				out = this.tidy(() => forwardFunc(this.backend, saveFunc));
				const outs = (Array.isArray(out) ? out : [out]);
				if (this.shouldCheckForMemLeaks()) {
					// Scope name is used to print a more helpful error message if needed.
					this.checkKernelForMemLeak(kernelOrScopeName, numDataIdsBefore, outs);
				}
				return outs;
			};
		}
		//
		// Run the kernelFunc. Optionally profiling it.
		//
		const {inputs, attrs} = kernelParams;
		const backwardsFunc = isRegisteredKernelInvocation(kernelParams) ?
			null :
			kernelParams.backwardsFunc;
		let kernelProfile;
		this.scopedRun(
			// Stop recording to a tape when running a kernel.
			() => this.state.kernelDepth++, () => this.state.kernelDepth--, () => {
				if (!this.ENV.getBool('DEBUG') && !this.state.profiling) {
					outputs = kernelFunc();
				} else {
					kernelProfile = this.profiler.profileKernel(kernelOrScopeName, inputs, () => kernelFunc());
					if (this.ENV.getBool('DEBUG')) {
						this.profiler.logKernelProfile(kernelProfile);
					}
					outputs = kernelProfile.outputs;
				}
			});
		if (isTapeOn) {
			this.addTapeNode(kernelOrScopeName, inputs, outputs, backwardsFunc, saved, attrs);
		}
		if (this.state.profiling) {
			this.state.activeProfile.kernels.push({
				name: kernelOrScopeName,
				bytesAdded: this.state.numBytes - startingBytecount,
				totalBytesSnapshot: this.state.numBytes,
				tensorsAdded: this.state.numTensors - startingNumTensors,
				totalTensorsSnapshot: this.state.numTensors,
				inputShapes: Object.keys(inputs).map(key => inputs[key] != null ? inputs[key].shape : null),
				outputShapes: outputs.map(item => item.shape),
				kernelTimeMs: kernelProfile.timeMs,
				extraInfo: kernelProfile.extraInfo
			});
		}
		return (Array.isArray(out) ? outputs : outputs[0]);
	}

	/**
	 * Saves tensors used in forward mode for use in backward mode.
	 *
	 * @param tensors the list of tensors to save.
	 */
	saveTensorsForBackwardMode(tensors) {
		const saved = tensors.map(tensor => this.keep(this.clone(tensor)));
		return saved;
	}

	/**
	 * Returns a list of tensors to save for a given gradient calculation.
	 *
	 * @param kernelName name of kernel to look up gradient for.
	 * @param inputs a map of input tensors.
	 * @param outputs an array of output tensors from forward mode of kernel.
	 */
	getTensorsForGradient(kernelName, inputs, outputs) {
		const gradConfig = getGradient(kernelName);
		if (gradConfig != null) {
			const inputsToSave = gradConfig.inputsToSave || [];
			const outputsToSave = gradConfig.outputsToSave || [];
			// If saveAllInputs is true, all inputs will be saved. Otherwise, inputs
			// specified in inputsToSave will be saved.
			let inputTensorsToSave;
			if (gradConfig.saveAllInputs) {
				assert(Array.isArray(inputs), () => 'saveAllInputs is true, expected inputs to be an array.');
				inputTensorsToSave = Object.keys(inputs).map((key) => inputs[key]);
			} else {
				inputTensorsToSave = inputsToSave.map((inputName) => inputs[inputName]);
			}
			const outputTensorsToSave = outputs.filter((_, i) => outputsToSave[i]);
			return inputTensorsToSave.concat(outputTensorsToSave);
		}
		// We return an empty list rather than throw an error because the kernel we
		// are looking up may not actually be relevant to backproping through the
		// overall function
		//
		// See 'does not error if irrelevant (pruned) ops are missing grads' test
		// in gradients_test.ts for an example.
		return [];
	}

	/**
	 * Internal method used by public APIs for tensor creation. Makes a new
	 * tensor with the provided shape, dtype and values. It always
	 * creates a new data id and writes the values to the underlying backend.
	 */
	makeTensor(values, shape, dtype, backend) {
		if (values == null) {
			throw new Error('Values passed to engine.makeTensor() are null');
		}
		dtype = dtype || 'float32';
		backend = backend || this.backend;
		let backendVals = values;
		if (dtype === 'string' && isString(values[0])) {
			backendVals = values.map(d => encodeString(d));
		}
		const dataId = backend.write(backendVals, shape, dtype);
		const t = new Tensor(shape, dtype, dataId, this.nextTensorId());
		this.trackTensor(t, backend);
		// Count bytes for string tensors.
		if (dtype === 'string') {
			const info = this.state.tensorInfo.get(dataId);
			const newBytes = bytesFromStringArray(backendVals);
			this.state.numBytes += newBytes - info.bytes;
			info.bytes = newBytes;
		}
		return t;
	}

	/**
	 * Internal method used by backends. Makes a new tensor
	 * that is a wrapper around an existing data id. It doesn't create
	 * a new data id, only increments the ref count used in memory tracking.
	 * @deprecated
	 */
	makeTensorFromDataId(dataId, shape, dtype, backend) {
		dtype = dtype || 'float32';
		const tensorInfo = {dataId, shape, dtype};
		return this.makeTensorFromTensorInfo(tensorInfo, backend);
	}

	/**
	 * Internal method used by backends. Makes a new tensor that is a wrapper
	 * around an existing data id in TensorInfo. It doesn't create a new data id,
	 * only increments the ref count used in memory tracking.
	 */
	makeTensorFromTensorInfo(tensorInfo, backend) {
		const {dataId, shape, dtype} = tensorInfo;
		const t = new Tensor(shape, dtype, dataId, this.nextTensorId());
		this.trackTensor(t, backend);
		return t;
	}

	makeVariable(initialValue, trainable = true, name, dtype) {
		name = name || this.nextVariableId().toString();
		if (dtype != null && dtype !== initialValue.dtype) {
			initialValue = initialValue.cast(dtype);
		}
		const v = new Variable(initialValue, trainable, name, this.nextTensorId());
		if (this.state.registeredVariables[v.name] != null) {
			throw new Error(`Variable with name ${v.name} was already registered`);
		}
		this.state.registeredVariables[v.name] = v;
		this.incRef(v, this.backend);
		return v;
	}

	trackTensor(a, backend) {
		this.state.numTensors++;
		if (a.dtype === 'string') {
			this.state.numStringTensors++;
		}
		// Bytes for complex numbers are counted by their components. Bytes for
		// string tensors are counted when writing values.
		let bytes = 0;
		if (a.dtype !== 'complex64' && a.dtype !== 'string') {
			bytes = a.size * bytesPerElement(a.dtype);
		}
		this.state.numBytes += bytes;
		if (!this.state.tensorInfo.has(a.dataId)) {
			this.state.numDataBuffers++;
			this.state.tensorInfo.set(a.dataId, {
				backend: backend || this.backend,
				dtype: a.dtype,
				shape: a.shape,
				bytes
			});
		}
		if (!(a instanceof Variable)) {
			this.track(a);
		}
	}

	// Track the tensor by dataId and increase the refCount for the dataId in the
	// backend.
	// TODO(pyu10055): This is currently used by makeVariable method, to increase
	// refCount on the backend for the dataId. It can potentially be replaced with
	// Identity op indead of calling backend directly.
	incRef(a, backend) {
		this.trackTensor(a, backend);
		this.backend.incRef(a.dataId);
	}

	removeDataId(dataId, backend) {
		if (this.state.tensorInfo.has(dataId) &&
			this.state.tensorInfo.get(dataId).backend === backend) {
			this.state.tensorInfo.delete(dataId);
			this.state.numDataBuffers--;
		}
	}

	disposeTensor(a) {
		if (!this.state.tensorInfo.has(a.dataId)) {
			return;
		}
		const info = this.state.tensorInfo.get(a.dataId);
		this.state.numTensors--;
		if (a.dtype === 'string') {
			this.state.numStringTensors--;
			this.state.numBytes -= info.bytes;
		}
		// Don't count bytes for complex numbers as they are counted by their
		// components.
		if (a.dtype !== 'complex64' && a.dtype !== 'string') {
			const bytes = a.size * bytesPerElement(a.dtype);
			this.state.numBytes -= bytes;
		}
		// Remove the reference to dataId if backend dispose the data successfully
		if (info.backend.disposeData(a.dataId)) {
			this.removeDataId(a.dataId, info.backend);
		}
		// TODO(nsthorat): Construct an error and save the stack trace for
		// debugging when in debug mode. Creating a stack trace is too expensive
		// to do unconditionally.
	}

	disposeVariables() {
		for (const varName in this.state.registeredVariables) {
			const v = this.state.registeredVariables[varName];
			this.disposeVariable(v);
		}
	}

	disposeVariable(v) {
		this.disposeTensor(v);
		if (this.state.registeredVariables[v.name] != null) {
			delete this.state.registeredVariables[v.name];
		}
	}

	memory() {
		const info = this.backend.memory();
		info.numTensors = this.state.numTensors;
		info.numDataBuffers = this.state.numDataBuffers;
		info.numBytes = this.state.numBytes;
		if (this.state.numStringTensors > 0) {
			info.unreliable = true;
			if (info.reasons == null) {
				info.reasons = [];
			}
			info.reasons.push('Memory usage by string tensors is approximate ' +
				'(2 bytes per character)');
		}
		return info;
	}

	async profile(query) {
		this.state.profiling = true;
		const startBytes = this.state.numBytes;
		const startNumTensors = this.state.numTensors;
		this.state.activeProfile.kernels = [];
		this.state.activeProfile.result = await query();
		this.state.profiling = false;
		this.state.activeProfile.peakBytes = Math.max(...this.state.activeProfile.kernels.map(d => d.totalBytesSnapshot));
		this.state.activeProfile.newBytes = this.state.numBytes - startBytes;
		this.state.activeProfile.newTensors =
			this.state.numTensors - startNumTensors;
		for (const kernel of this.state.activeProfile.kernels) {
			kernel.kernelTimeMs = await kernel.kernelTimeMs;
			kernel.extraInfo = await kernel.extraInfo;
		}
		return this.state.activeProfile;
	}

	isTapeOn() {
		return this.state.gradientDepth > 0 && this.state.kernelDepth === 0;
	}

	addTapeNode(kernelName, inputs, outputs, gradientsFunc, saved, attrs) {
		const tapeNode = {id: this.state.nextTapeNodeId++, kernelName, inputs, outputs, saved};
		const gradConfig = getGradient(kernelName);
		if (gradConfig != null) {
			gradientsFunc = gradConfig.gradFunc;
		}
		if (gradientsFunc != null) {
			tapeNode.gradient = (dys) => {
				// TODO(smilkov): To optimize back-prop, pass dys that are not used in
				// the backprop graph to the user as null instead of zeros
				dys = dys.map((dy, i) => {
					if (dy == null) {
						const output = outputs[i];
						const vals = makeZerosTypedArray(output.size, output.dtype);
						return this.makeTensor(vals, output.shape, output.dtype);
					}
					return dy;
				});
				// Grad functions of ops with single outputs expect a dy, while ops
				// with multiple outputs expect dys (array of dy).
				return gradientsFunc(dys.length > 1 ? dys : dys[0], saved, attrs);
			};
		}
		this.state.activeTape.push(tapeNode);
	}

	keep(result) {
		result.kept = true;
		return result;
	}

	startTape() {
		if (this.state.gradientDepth === 0) {
			this.state.activeTape = [];
		}
		this.state.gradientDepth++;
	}

	endTape() {
		this.state.gradientDepth--;
	}

	/**
	 * Start a scope. Use this with endScope() to achieve the same functionality
	 * as scope() without the need for a function closure.
	 */
	startScope(name) {
		const scopeInfo = {
			track: [],
			name: 'unnamed scope',
			id: this.state.nextScopeId++
		};
		if (name) {
			scopeInfo.name = name;
		}
		this.state.scopeStack.push(scopeInfo);
		this.state.activeScope = scopeInfo;
	}

	/**
	 * End a scope. Use this with startScope() to achieve the same functionality
	 * as scope() without the need for a function closure.
	 */
	endScope(result) {
		const tensorsToTrackInParent = getTensorsInContainer(result);
		const tensorsToTrackInParentSet = new Set(tensorsToTrackInParent.map(t => t.id));
		// Dispose the arrays tracked in this scope.
		for (let i = 0; i < this.state.activeScope.track.length; i++) {
			const tensor = this.state.activeScope.track[i];
			if (!tensor.kept && !tensorsToTrackInParentSet.has(tensor.id)) {
				tensor.dispose();
			}
		}
		const oldScope = this.state.scopeStack.pop();
		this.state.activeScope = this.state.scopeStack.length === 0 ?
			null :
			this.state.scopeStack[this.state.scopeStack.length - 1];
		// Track the current result in the parent scope.
		tensorsToTrackInParent.forEach(tensor => {
			// Only track the tensor if was allocated in the inner scope and is not
			// globally kept.
			if (!tensor.kept && tensor.scopeId === oldScope.id) {
				this.track(tensor);
			}
		});
	}

	/**
	 * Returns gradients of `f` with respect to each of the `xs`. The gradients
	 * returned are of the same length as `xs`, but some might be null if `f`
	 * was not a function of that `x`. It also takes optional dy to multiply the
	 * gradient, which defaults to `1`.
	 */
	gradients(f, xs, dy, allowNoGradients = false) {
		assert(xs.length > 0, () => 'gradients() received an empty list of xs.');
		if (dy != null && dy.dtype !== 'float32') {
			throw new Error(`dy must have 'float32' dtype, but has '${dy.dtype}'`);
		}
		const y = this.scopedRun(() => this.startTape(), () => this.endTape(), () => this.tidy('forward', f));
		assert(y instanceof Tensor, () => 'The result y returned by f() must be a tensor.');
		// Filter out the nodes that don't connect x => y.
		const filteredTape = getFilteredNodesXToY(this.state.activeTape, xs, y);
		if (!allowNoGradients && filteredTape.length === 0 && xs.length > 0) {
			throw new Error('Cannot compute gradient of y=f(x) with respect to x. Make sure ' +
				'that the f you passed encloses all operations that lead from x ' +
				'to y.');
		}
		return this.tidy('backward', () => {
			const accumulatedGradientMap = {};
			accumulatedGradientMap[y.id] = (dy == null) ? ones$1(y.shape) : dy;
			// Backprop gradients through the filtered nodes.
			backpropagateGradients(accumulatedGradientMap, filteredTape,
				// Pass the tidy function to avoid circular dep with `tape.ts`.
				f => this.tidy(f),
				// Pass an add function to avoide a circular dep with `tape.ts`.
				add$2);
			const grads = xs.map(x => accumulatedGradientMap[x.id]);
			if (this.state.gradientDepth === 0) {
				// This means that we are not computing higher-order gradients
				// and can clean up the tape.
				this.state.activeTape.forEach(node => {
					for (const tensor of node.saved) {
						tensor.dispose();
					}
				});
				this.state.activeTape = null;
			}
			return {value: y, grads};
		});
	}

	customGrad(f) {
		assert(isFunction(f), () => 'The f passed in customGrad(f) must be a function.');
		return (...inputs) => {
			assert(inputs.every(t => t instanceof Tensor), () => 'The args passed in customGrad(f)(x1, x2,...) must all be ' +
				'tensors');
			let res;
			const inputMap = {};
			inputs.forEach((input, i) => {
				inputMap[i] = input;
			});
			const forwardFunc = (_, save) => {
				res = f(...[...inputs, save]);
				assert(res.value instanceof Tensor, () => 'The function f passed in customGrad(f) must return an ' +
					'object where `obj.value` is a tensor');
				assert(isFunction(res.gradFunc), () => 'The function f passed in customGrad(f) must return an ' +
					'object where `obj.gradFunc` is a function.');
				return res.value;
			};
			const backwardsFunc = (dy, saved) => {
				const gradRes = res.gradFunc(dy, saved);
				const grads = Array.isArray(gradRes) ? gradRes : [gradRes];
				assert(grads.length === inputs.length, () => 'The function f passed in customGrad(f) must return an ' +
					'object where `obj.gradFunc` is a function that returns ' +
					'the same number of tensors as inputs passed to f(...).');
				assert(grads.every(t => t instanceof Tensor), () => 'The function f passed in customGrad(f) must return an ' +
					'object where `obj.gradFunc` is a function that returns ' +
					'a list of only tensors.');
				const gradMap = {};
				grads.forEach((grad, i) => {
					gradMap[i] = () => grad;
				});
				return gradMap;
			};
			return this.runKernelFunc({
				forwardFunc,
				backwardsFunc,
				inputs: inputMap,
			});
		};
	}

	readSync(dataId) {
		// Route the read to the correct backend.
		const info = this.state.tensorInfo.get(dataId);
		return info.backend.readSync(dataId);
	}

	read(dataId) {
		// Route the read to the correct backend.
		const info = this.state.tensorInfo.get(dataId);
		return info.backend.read(dataId);
	}

	readToGPU(dataId, options) {
		// Route the read to the correct backend.
		const info = this.state.tensorInfo.get(dataId);
		return info.backend.readToGPU(dataId, options);
	}

	async time(query) {
		const start = now();
		const timingInfo = await this.backend.time(query);
		timingInfo.wallMs = now() - start;
		return timingInfo;
	}

	/**
	 * Tracks a Tensor in the current scope to be automatically cleaned up
	 * when the current scope ends, and returns the value.
	 *
	 * @param result The Tensor to track in the current scope.
	 */
	track(result) {
		if (this.state.activeScope != null) {
			result.scopeId = this.state.activeScope.id;
			this.state.activeScope.track.push(result);
		}
		return result;
	}

	get registeredVariables() {
		return this.state.registeredVariables;
	}

	/**
	 * Resets the engine state. Removes all backends but does not remove
	 * registered backend factories.
	 */
	reset() {
		// Make any pending promise obsolete.
		this.pendingBackendInitId++;
		this.state.dispose();
		this.ENV.reset();
		this.state = new EngineState();
		for (const backendName in this.registry) {
			this.disposeRegisteredKernels(backendName);
			this.registry[backendName].dispose();
			delete this.registry[backendName];
		}
		this.backendName = null;
		this.backendInstance = null;
		this.pendingBackendInit = null;
	}
}

Engine.nextTensorId = 0;
Engine.nextVariableId = 0;

function ones$1(shape) {
	const values = makeOnesTypedArray(sizeFromShape(shape), 'float32');
	return ENGINE.makeTensor(values, shape, 'float32');
}

function getOrMakeEngine() {
	const ns = getGlobalNamespace();
	if (ns._tfengine == null) {
		const environment = new Environment(ns);
		ns._tfengine = new Engine(environment);
	}
	setEnvironmentGlobal(ns._tfengine.ENV);
	// Tell the current tensor interface that the global engine is responsible
	// for tracking.
	setTensorTracker(() => ns._tfengine);
	return ns._tfengine;
}

const ENGINE = getOrMakeEngine();

/**
 * A implementation of the add op for use within engine and tape.
 *
 * This allows us to avoid a circular dependency between add.ts and engine.
 * It is exported to be available in tape tests.
 */
function add$2(a, b) {
	// We duplicate Add here to avoid a circular dependency with add.ts.
	const inputs = {a, b};
	return ENGINE.runKernel(Add, inputs);
}

/**
 * @license
 * Copyright 2017 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
// tslint:disable-next-line:no-any
function _isNavigatorDefined() {
	return typeof navigator !== 'undefined' && navigator != null;
}

let isMobileMockValue;

function mockIsMobile(value) {
	isMobileMockValue = value;
}

function isMobile(nav) {
	if (isMobileMockValue !== undefined) {
		return isMobileMockValue;
	}
	if (nav || _isNavigatorDefined()) {
		if (!nav) {
			nav = navigator;
		}
		if (nav.product === 'ReactNative') {
			return true;
		}
		const a = nav.userAgent || nav.vendor ||
			// tslint:disable-next-line:no-any
			(typeof window !== 'undefined' ? window.opera : '');
		// Use `navigator.userAgentData.mobile` as fallback.
		if (!a) {
			// tslint:disable-next-line:no-any
			const navAny = nav;
			return navAny.userAgentData && navAny.userAgentData.mobile;
		}
		// tslint:disable-next-line:max-line-length
		return /(android|bb\d+|meego).+mobile|avantgo|bada\/|blackberry|blazer|compal|elaine|fennec|hiptop|iemobile|ip(hone|od)|iris|kindle|lge |maemo|midp|mmp|mobile.+firefox|netfront|opera m(ob|in)i|palm( os)?|phone|p(ixi|re)\/|plucker|pocket|psp|series(4|6)0|symbian|treo|up\.(browser|link)|vodafone|wap|windows ce|xda|xiino/i
				.test(a) ||
			// tslint:disable-next-line:max-line-length
			/1207|6310|6590|3gso|4thp|50[1-6]i|770s|802s|a wa|abac|ac(er|oo|s\-)|ai(ko|rn)|al(av|ca|co)|amoi|an(ex|ny|yw)|aptu|ar(ch|go)|as(te|us)|attw|au(di|\-m|r |s )|avan|be(ck|ll|nq)|bi(lb|rd)|bl(ac|az)|br(e|v)w|bumb|bw\-(n|u)|c55\/|capi|ccwa|cdm\-|cell|chtm|cldc|cmd\-|co(mp|nd)|craw|da(it|ll|ng)|dbte|dc\-s|devi|dica|dmob|do(c|p)o|ds(12|\-d)|el(49|ai)|em(l2|ul)|er(ic|k0)|esl8|ez([4-7]0|os|wa|ze)|fetc|fly(\-|_)|g1 u|g560|gene|gf\-5|g\-mo|go(\.w|od)|gr(ad|un)|haie|hcit|hd\-(m|p|t)|hei\-|hi(pt|ta)|hp( i|ip)|hs\-c|ht(c(\-| |_|a|g|p|s|t)|tp)|hu(aw|tc)|i\-(20|go|ma)|i230|iac( |\-|\/)|ibro|idea|ig01|ikom|im1k|inno|ipaq|iris|ja(t|v)a|jbro|jemu|jigs|kddi|keji|kgt( |\/)|klon|kpt |kwc\-|kyo(c|k)|le(no|xi)|lg( g|\/(k|l|u)|50|54|\-[a-w])|libw|lynx|m1\-w|m3ga|m50\/|ma(te|ui|xo)|mc(01|21|ca)|m\-cr|me(rc|ri)|mi(o8|oa|ts)|mmef|mo(01|02|bi|de|do|t(\-| |o|v)|zz)|mt(50|p1|v )|mwbp|mywa|n10[0-2]|n20[2-3]|n30(0|2)|n50(0|2|5)|n7(0(0|1)|10)|ne((c|m)\-|on|tf|wf|wg|wt)|nok(6|i)|nzph|o2im|op(ti|wv)|oran|owg1|p800|pan(a|d|t)|pdxg|pg(13|\-([1-8]|c))|phil|pire|pl(ay|uc)|pn\-2|po(ck|rt|se)|prox|psio|pt\-g|qa\-a|qc(07|12|21|32|60|\-[2-7]|i\-)|qtek|r380|r600|raks|rim9|ro(ve|zo)|s55\/|sa(ge|ma|mm|ms|ny|va)|sc(01|h\-|oo|p\-)|sdk\/|se(c(\-|0|1)|47|mc|nd|ri)|sgh\-|shar|sie(\-|m)|sk\-0|sl(45|id)|sm(al|ar|b3|it|t5)|so(ft|ny)|sp(01|h\-|v\-|v )|sy(01|mb)|t2(18|50)|t6(00|10|18)|ta(gt|lk)|tcl\-|tdg\-|tel(i|m)|tim\-|t\-mo|to(pl|sh)|ts(70|m\-|m3|m5)|tx\-9|up(\.b|g1|si)|utst|v400|v750|veri|vi(rg|te)|vk(40|5[0-3]|\-v)|vm40|voda|vulc|vx(52|53|60|61|70|80|81|83|85|98)|w3c(\-| )|webc|whit|wi(g |nc|nw)|wmlb|wonu|x700|yas\-|your|zeto|zte\-/i
				.test(a.substr(0, 4));
	}
	return false;
}

function isBrowser() {
	return (typeof window !== 'undefined' && window.document != null) ||
		//@ts-ignore
		(typeof WorkerGlobalScope !== 'undefined');
}

var device_util = /*#__PURE__*/Object.freeze({
	__proto__: null,
	isBrowser: isBrowser,
	isMobile: isMobile,
	mockIsMobile: mockIsMobile
});

/**
 * @license
 * Copyright 2019 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
const ENV = env();
/**
 * This file contains environment-related flag registrations.
 */
/** Whether to enable debug mode. */
ENV.registerFlag('DEBUG', () => false, debugValue => {
	if (debugValue) {
		console.warn('Debugging mode is ON. The output of every math call will ' +
			'be downloaded to CPU and checked for NaNs. ' +
			'This significantly impacts performance.');
	}
});
/** Whether we are in a browser (as versus, say, node.js) environment. */
ENV.registerFlag('IS_BROWSER', () => isBrowser());
/** Whether we are in a browser (as versus, say, node.js) environment. */
ENV.registerFlag('IS_NODE', () => (typeof process !== 'undefined') &&
	(typeof process.versions !== 'undefined') &&
	(typeof process.versions.node !== 'undefined'));
/** Whether this browser is Chrome. */
ENV.registerFlag('IS_CHROME', () => typeof navigator !== 'undefined' && navigator != null &&
	navigator.userAgent != null && /Chrome/.test(navigator.userAgent) &&
	/Google Inc/.test(navigator.vendor));
/** Whether this browser is Safari. */
ENV.registerFlag('IS_SAFARI', () => typeof navigator !== 'undefined' && navigator != null &&
	navigator.userAgent != null && /Safari/.test(navigator.userAgent) &&
	/Apple/.test(navigator.vendor));
/**
 * True when the environment is "production" where we disable safety checks
 * to gain performance.
 */
ENV.registerFlag('PROD', () => false);
/**
 * Whether to do sanity checks when inferring a shape from user-provided
 * values, used when creating a new tensor.
 */
ENV.registerFlag('TENSORLIKE_CHECK_SHAPE_CONSISTENCY', () => ENV.getBool('DEBUG'));
/** Whether deprecation warnings are enabled. */
ENV.registerFlag('DEPRECATION_WARNINGS_ENABLED', () => true);
/** True if running unit tests. */
ENV.registerFlag('IS_TEST', () => false);
/** Whether to check computation result for errors. */
ENV.registerFlag('CHECK_COMPUTATION_FOR_ERRORS', () => ENV.getBool('DEBUG'));
/** Whether the backend needs to wrap input to imageBitmap. */
ENV.registerFlag('WRAP_TO_IMAGEBITMAP', () => false);
/** Whether to enable canvas2d willReadFrequently for GPU backends */
ENV.registerFlag('CANVAS2D_WILL_READ_FREQUENTLY_FOR_GPU', () => false);
/** Whether to use setTimeoutCustom */
ENV.registerFlag('USE_SETTIMEOUTCUSTOM', () => false);

/**
 * @license
 * Copyright 2018 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
function inferShape(val, dtype) {
	let firstElem = val;
	if (isTypedArray(val)) {
		return dtype === 'string' ? [] : [val.length];
	}
	if (isWebGLData(val)) {
		const usedChannels = val.channels || 'RGBA';
		return [val.height, val.width * usedChannels.length];
	} else if (isWebGPUData(val)) {
		return [val.buffer.size / (dtype == null ? 4 : bytesPerElement(dtype))];
	}
	if (!Array.isArray(val)) {
		return []; // Scalar.
	}
	const shape = [];
	while (Array.isArray(firstElem) ||
	isTypedArray(firstElem) && dtype !== 'string') {
		shape.push(firstElem.length);
		firstElem = firstElem[0];
	}
	if (Array.isArray(val) &&
		env().getBool('TENSORLIKE_CHECK_SHAPE_CONSISTENCY')) {
		deepAssertShapeConsistency(val, shape, []);
	}
	return shape;
}

function deepAssertShapeConsistency(val, shape, indices) {
	indices = indices || [];
	if (!(Array.isArray(val)) && !isTypedArray(val)) {
		assert(shape.length === 0, () => `Element arr[${indices.join('][')}] is a primitive, ` +
			`but should be an array/TypedArray of ${shape[0]} elements`);
		return;
	}
	assert(shape.length > 0, () => `Element arr[${indices.join('][')}] should be a primitive, ` +
		`but is an array of ${val.length} elements`);
	assert(val.length === shape[0], () => `Element arr[${indices.join('][')}] should have ${shape[0]} ` +
		`elements, but has ${val.length} elements`);
	const subShape = shape.slice(1);
	for (let i = 0; i < val.length; ++i) {
		deepAssertShapeConsistency(val[i], subShape, indices.concat(i));
	}
}

function assertDtype(expectedDtype, actualDType, argName, functionName) {
	if (expectedDtype === 'string_or_numeric') {
		return;
	}
	if (expectedDtype == null) {
		throw new Error(`Expected dtype cannot be null.`);
	}
	if (expectedDtype !== 'numeric' && expectedDtype !== actualDType ||
		expectedDtype === 'numeric' && actualDType === 'string') {
		throw new Error(`Argument '${argName}' passed to '${functionName}' must ` +
			`be ${expectedDtype} tensor, but got ${actualDType} tensor`);
	}
}

function convertToTensor(x, argName, functionName, parseAsDtype = 'numeric') {
	if (x instanceof getGlobalTensorClass()) {
		assertDtype(parseAsDtype, x.dtype, argName, functionName);
		return x;
	}
	let inferredDtype = inferDtype(x);
	// If the user expects a bool/int/float, use that info to update the
	// inferredDtype when it is not a string.
	if (inferredDtype !== 'string' &&
		['bool', 'int32', 'float32'].indexOf(parseAsDtype) >= 0) {
		inferredDtype = parseAsDtype;
	}
	assertDtype(parseAsDtype, inferredDtype, argName, functionName);
	if ((x == null) ||
		(!isTypedArray(x) && !Array.isArray(x) && typeof x !== 'number' &&
			typeof x !== 'boolean' && typeof x !== 'string')) {
		const type = x == null ? 'null' : x.constructor.name;
		throw new Error(`Argument '${argName}' passed to '${functionName}' must be a ` +
			`Tensor or TensorLike, but got '${type}'`);
	}
	const inferredShape = inferShape(x, inferredDtype);
	if (!isTypedArray(x) && !Array.isArray(x)) {
		x = [x];
	}
	const skipTypedArray = true;
	const values = inferredDtype !== 'string' ?
		toTypedArray(x, inferredDtype) :
		flatten(x, [], skipTypedArray);
	return ENGINE.makeTensor(values, inferredShape, inferredDtype);
}

function convertToTensorArray(arg, argName, functionName, parseAsDtype = 'numeric') {
	if (!Array.isArray(arg)) {
		throw new Error(`Argument ${argName} passed to ${functionName} must be a ` +
			'`Tensor[]` or `TensorLike[]`');
	}
	const tensors = arg;
	return tensors.map((t, i) => convertToTensor(t, `${argName}[${i}]`, functionName, parseAsDtype));
}

/**
 * @license
 * Copyright 2018 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
const OP_SCOPE_SUFFIX = '__op';

/**
 * Used for wrapping functions that perform math operations on
 * Tensors. The function will be wrapped in a named scope that cleans all
 * memory usage after the function is done.
 */
function op(f) {
	const keys = Object.keys(f);
	if (keys.length !== 1) {
		throw new Error(`Please provide an object with a single key ` +
			`(operation name) mapping to a function. Got an object with ` +
			`${keys.length} keys.`);
	}
	let opName = keys[0];
	const fn = f[opName];
	// Strip the underscore from the end of the function name.
	if (opName.endsWith('_')) {
		opName = opName.substring(0, opName.length - 1);
	}
	// add an __op suffix to distinguish ops from kernels in tf.profile
	opName = opName + OP_SCOPE_SUFFIX;
	// tslint:disable-next-line:no-any
	const f2 = (...args) => {
		ENGINE.startScope(opName);
		try {
			const result = fn(...args);
			if (isPromise(result)) {
				console.error('Cannot return a Promise inside of tidy.');
			}
			ENGINE.endScope(result);
			return result;
		} catch (ex) {
			ENGINE.endScope(null);
			throw ex;
		}
	};
	Object.defineProperty(f2, 'name', {value: opName, configurable: true});
	// tslint:disable-next-line:no-any
	return f2;
}

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Converts two real numbers to a complex number.
 *
 * Given a tensor `real` representing the real part of a complex number, and a
 * tensor `imag` representing the imaginary part of a complex number, this
 * operation returns complex numbers elementwise of the form [r0, i0, r1, i1],
 * where r represents the real part and i represents the imag part.
 *
 * The input tensors real and imag must have the same shape.
 *
 * ```js
 * const real = tf.tensor1d([2.25, 3.25]);
 * const imag = tf.tensor1d([4.75, 5.75]);
 * const complex = tf.complex(real, imag);
 *
 * complex.print();
 * ```
 *
 * @doc {heading: 'Tensors', subheading: 'Creation'}
 */
function complex_(real, imag) {
	const $real = convertToTensor(real, 'real', 'complex');
	const $imag = convertToTensor(imag, 'imag', 'complex');
	assertShapesMatch($real.shape, $imag.shape, `real and imag shapes, ${$real.shape} and ${$imag.shape}, ` +
		`must match in call to tf.complex().`);
	const inputs = {real: $real, imag: $imag};
	return ENGINE.runKernel(Complex, inputs);
}

const complex$1 = /* @__PURE__ */ op({complex_});

/**
 * @license
 * Copyright 2018 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/** This is shared code across all tensor creation methods. */
function makeTensor(values, shape, inferredShape, dtype) {
	if (dtype == null) {
		dtype = inferDtype(values);
	} else if (dtype === 'complex64') {
		throw new Error(`Cannot construct a complex64 tensor directly. ` +
			`Please use tf.complex(real, imag).`);
	}
	if (isWebGPUData(values) || isWebGLData(values)) {
		if (dtype !== 'float32' && dtype !== 'int32') {
			throw new Error(`Creating tensor from GPU data only supports ` +
				`'float32'|'int32' dtype, while the dtype is ${dtype}.`);
		}
		return ENGINE.backend.createTensorFromGPUData(values, shape || inferredShape, dtype);
	}
	if (!isTypedArray(values) && !Array.isArray(values) &&
		typeof values !== 'number' && typeof values !== 'boolean' &&
		typeof values !== 'string') {
		throw new Error('values passed to tensor(values) must be a number/boolean/string or ' +
			'an array of numbers/booleans/strings, or a TypedArray');
	}
	// Verify that the shape matches the inferred shape.
	if (shape != null) {
		assertNonNegativeIntegerDimensions(shape);
		const providedSize = sizeFromShape(shape);
		const inferredSize = sizeFromShape(inferredShape);
		assert(providedSize === inferredSize, () => `Based on the provided shape, [${shape}], the tensor should have ` +
			`${providedSize} values but has ${inferredSize}`);
		for (let i = 0; i < inferredShape.length; ++i) {
			const inferred = inferredShape[i];
			const flatDimsDontMatch = i === inferredShape.length - 1 ?
				inferred !== sizeFromShape(shape.slice(i)) :
				true;
			assert(inferredShape[i] === shape[i] || !flatDimsDontMatch, () => `Error creating a new Tensor. Inferred shape ` +
				`(${inferredShape}) does not match the provided ` +
				`shape (${shape}). `);
		}
	}
	if (!isTypedArray(values) && !Array.isArray(values)) {
		values = [values];
	}
	shape = shape || inferredShape;
	values = dtype !== 'string' ?
		toTypedArray(values, dtype) :
		flatten(values, [], true);
	return ENGINE.makeTensor(values, shape, dtype);
}

/**
 * @license
 * Copyright 2018 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Creates a `tf.Tensor` with the provided values, shape and dtype.
 *
 * ```js
 * // Pass an array of values to create a vector.
 * tf.tensor([1, 2, 3, 4]).print();
 * ```
 *
 * ```js
 * // Pass a nested array of values to make a matrix or a higher
 * // dimensional tensor.
 * tf.tensor([[1, 2], [3, 4]]).print();
 * ```
 *
 * ```js
 * // Pass a flat array and specify a shape yourself.
 * tf.tensor([1, 2, 3, 4], [2, 2]).print();
 * ```
 *
 * ```js
 * // Pass a `WebGLData` object and specify a shape yourself.
 *
 * // This makes it possible for TF.js applications to avoid GPU / CPU sync.
 * // For example, if your application includes a preprocessing step on the GPU,
 * // you could upload the GPU output directly to TF.js, rather than first
 * // downloading the values.
 *
 * // Example for WebGL2:
 * if (tf.findBackend('custom-webgl') == null) {
 *   const customCanvas = document.createElement('canvas');
 *   const customBackend = new tf.MathBackendWebGL(customCanvas);
 *   tf.registerBackend('custom-webgl', () => customBackend);
 * }
 * const savedBackend = tf.getBackend();
 * await tf.setBackend('custom-webgl');
 * const gl = tf.backend().gpgpu.gl;
 * const texture = gl.createTexture();
 * const tex2d = gl.TEXTURE_2D;
 * const width = 2;
 * const height = 2;
 *
 * gl.bindTexture(tex2d, texture);
 * gl.texParameteri(tex2d, gl.TEXTURE_WRAP_S, gl.CLAMP_TO_EDGE);
 * gl.texParameteri(tex2d, gl.TEXTURE_WRAP_T, gl.CLAMP_TO_EDGE);
 * gl.texParameteri(tex2d, gl.TEXTURE_MIN_FILTER, gl.NEAREST);
 * gl.texParameteri(tex2d, gl.TEXTURE_MAG_FILTER, gl.NEAREST);
 * gl.texImage2D(
 *   tex2d, 0, gl.RGBA32F, // internalFormat
 *   width, height, 0,
 *   gl.RGBA, // textureFormat
 *   gl.FLOAT, // textureType
 *   new Float32Array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15])
 * );
 *
 * // Currently, the `texture` has 4 pixels:
 * // Pixel0 is {R:0, G:1, B:2, A:3}
 * // Pixel1 is {R:4, G:5, B:6, A:7}
 * // Pixel2 is {R:8, G:9, B:10, A:11}
 * // Pixel3 is {R:12, G:13, B:14, A:15}
 *
 * const logicalShape = [height * width * 2];
 * const a = tf.tensor({texture, height, width, channels: 'BR'}, logicalShape);
 * a.print();
 * // Tensor value will be [2, 0, 6, 4, 10, 8, 14, 12], since [2, 0] is the
 * // values of 'B' and 'R' channels of Pixel0, [6, 4] is the values of 'B' and
 * 'R'
 * // channels of Pixel1...
 *
 * // For postprocessing on the GPU, it's possible to retrieve the texture
 * // backing any tensor by calling the tensor's `dataToGPU` method like
 * // so:
 *
 * const tex = a.dataToGPU();
 * await tf.setBackend(savedBackend);
 * ```
 *
 * ```js
 * // Pass a `WebGPUData` object and specify a shape yourself.
 *
 * // This makes it possible for TF.js applications to avoid GPU / CPU sync.
 * // For example, if your application includes a preprocessing step on the GPU,
 * // you could upload the GPU output directly to TF.js, rather than first
 * // downloading the values. Unlike WebGL, this optionally supports zero copy
 * // by WebGPUData.zeroCopy. When zeroCopy is false or undefined(default), this
 * // passing GPUBuffer can be destroyed after tensor is created. When zeroCopy
 * // is true, this GPUBuffer is bound directly by the tensor, so do not destroy
 * // this GPUBuffer until all access is done.
 *
 * // Example for WebGPU:
 * function createGPUBufferFromData(device, data, dtype) {
 *   const bytesPerElement = 4;
 *   const sizeInBytes = data.length * bytesPerElement;
 *
 *   const gpuWriteBuffer = device.createBuffer({
 *     mappedAtCreation: true,
 *     size: sizeInBytes,
 *     usage: GPUBufferUsage.MAP_WRITE | GPUBufferUsage.COPY_SRC
 *   });
 *   const arrayBuffer = gpuWriteBuffer.getMappedRange();
 *   if (dtype === 'float32') {
 *     new Float32Array(arrayBuffer).set(data);
 *   } else if (dtype === 'int32') {
 *     new Int32Array(arrayBuffer).set(data);
 *   } else {
 *     throw new Error(
 *         `Creating tensor from GPUBuffer only supports` +
 *         `'float32'|'int32' dtype, while the dtype is ${dtype}.`);
 *   }
 *   gpuWriteBuffer.unmap();
 *
 *   const gpuReadBuffer = device.createBuffer({
 *     mappedAtCreation: false,
 *     size: sizeInBytes,
 *     usage: GPUBufferUsage.COPY_DST | GPUBufferUsage.STORAGE |
 *         GPUBufferUsage.COPY_SRC
 *   });
 *
 *   const copyEncoder = device.createCommandEncoder();
 *   copyEncoder.copyBufferToBuffer(
 *       gpuWriteBuffer, 0, gpuReadBuffer, 0, sizeInBytes);
 *   const copyCommands = copyEncoder.finish();
 *   device.queue.submit([copyCommands]);
 *   gpuWriteBuffer.destroy();
 *   return gpuReadBuffer;
 * }
 *
 * const savedBackend = tf.getBackend();
 * await tf.setBackend('webgpu').catch(
 *     () => {throw new Error(
 *         'Failed to use WebGPU backend. Please use Chrome Canary to run.')});
 * const dtype = 'float32';
 * const device = tf.backend().device;
 * const aData = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16];
 * const bData = [1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4];
 * const expected = [2, 4, 6, 8, 6, 8, 10, 12, 10, 12, 14, 16, 14, 16, 18, 20];
 * const aBuffer = createGPUBufferFromData(device, aData, dtype);
 * const shape = [aData.length];
 * // To use zeroCopy, use {buffer: aBuffer, zeroCopy: true} instead and destroy
 * // aBuffer untill all access is done.
 * const a = tf.tensor({buffer: aBuffer}, shape, dtype);
 * const b = tf.tensor(bData, shape, dtype);
 * const result = tf.add(a, b);
 * result.print();
 * a.dispose();
 * b.dispose();
 * result.dispose();
 * aBuffer.destroy();
 * await tf.setBackend(savedBackend);
 * ```
 * @param values The values of the tensor. Can be nested array of numbers,
 * or a flat array, or a `TypedArray`(At the moment it supports Uint8Array,
 * Uint8ClampedArray, Int32Array, Float32Array) data types, or a `WebGLData`
 * object, or a `WebGPUData` object. If the values are strings, they will be
 * encoded as utf-8 and kept as `Uint8Array[]`. If the values is a `WebGLData`
 * object, the dtype could only be 'float32' or 'int32' and the object has to
 * have: 1. texture, a `WebGLTexture`, the texture must share the same
 * `WebGLRenderingContext` with TFJS's WebGL backend (you could create a custom
 * WebGL backend from your texture's canvas) and the internal texture format
 * for the input texture must be floating point or normalized integer; 2.
 * height, the height of the texture; 3. width, the width of the texture; 4.
 * channels, a non-empty subset of 'RGBA', indicating the values of which
 * channels will be passed to the tensor, such as 'R' or 'BR' (The order of the
 * channels affect the order of tensor values. ). (If the values passed from
 * texture is less than the tensor size, zeros will be padded at the rear.). If
 * the values is a `WebGPUData` object, the dtype could only be 'float32' or
 * 'int32 and the object has to have: buffer, a `GPUBuffer`. The buffer must:
 * 1. share the same `GPUDevice` with TFJS's WebGPU backend; 2. buffer.usage
 * should at least support GPUBufferUsage.STORAGE | GPUBufferUsage.COPY_SRC; 3.
 * buffer.size should not be smaller than the byte size of tensor shape.
 * WebGPUData optionally supports zero copy by flag zeroCopy. When zeroCopy is
 * false or undefined(default),this passing GPUBuffer can be destroyed after
 * tensor is created. When zeroCopy is true, this GPUBuffer is bound directly
 * by the tensor, so do not destroy this GPUBuffer until all access is done.
 * @param shape The shape of the tensor. Optional. If not provided,
 *   it is inferred from `values`.
 * @param dtype The data type.
 *
 * @doc {heading: 'Tensors', subheading: 'Creation'}
 */
function tensor(values, shape, dtype) {
	const inferredShape = inferShape(values, dtype);
	return makeTensor(values, shape, inferredShape, dtype);
}

/**
 * @license
 * Copyright 2018 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/* Type definitions for exporting and importing of models. */
/**
 * A map from Tensor dtype to number of bytes per element of the Tensor.
 */
const DTYPE_VALUE_SIZE_MAP = {
	'float32': 4,
	'float16': 2,
	'int32': 4,
	'uint16': 2,
	'uint8': 1,
	'bool': 1,
	'complex64': 8
};

/**
 * Wraps a list of ArrayBuffers into a `slice()`-able object without allocating
 * a large ArrayBuffer.
 *
 * Allocating large ArrayBuffers (~2GB) can be unstable on Chrome. TFJS loads
 * its weights as a list of (usually) 4MB ArrayBuffers and then slices the
 * weight tensors out of them. For small models, it's safe to concatenate all
 * the weight buffers into a single ArrayBuffer and then slice the weight
 * tensors out of it, but for large models, a different approach is needed.
 */
class CompositeArrayBuffer {
	/**
	 * Concatenate a number of ArrayBuffers into one.
	 *
	 * @param buffers An array of ArrayBuffers to concatenate, or a single
	 *     ArrayBuffer.
	 * @returns Result of concatenating `buffers` in order.
	 */
	static join(buffers) {
		return new CompositeArrayBuffer(buffers).slice();
	}

	constructor(buffers) {
		this.shards = [];
		this.previousShardIndex = 0;
		if (buffers == null) {
			return;
		}
		// Normalize the `buffers` input to be `ArrayBuffer[]`.
		if (!(buffers instanceof Array)) {
			buffers = [buffers];
		}
		buffers = buffers.map((bufferOrTypedArray) => {
			if (isTypedArray(bufferOrTypedArray)) {
				return bufferOrTypedArray.buffer;
			}
			return bufferOrTypedArray;
		});
		// Skip setting up shards if there are no buffers.
		if (buffers.length === 0) {
			return;
		}
		this.bufferUniformSize = buffers[0].byteLength;
		let start = 0;
		for (let i = 0; i < buffers.length; i++) {
			const buffer = buffers[i];
			// Check that all buffers except the last one have the same length.
			if (i !== buffers.length - 1 &&
				buffer.byteLength !== this.bufferUniformSize) {
				// Unset the buffer uniform size, since the buffer sizes are not
				// uniform.
				this.bufferUniformSize = undefined;
			}
			// Create the shards, including their start and end points.
			const end = start + buffer.byteLength;
			this.shards.push({buffer, start, end});
			start = end;
		}
		// Set the byteLength
		if (this.shards.length === 0) {
			this.byteLength = 0;
		}
		this.byteLength = this.shards[this.shards.length - 1].end;
	}

	slice(start = 0, end = this.byteLength) {
		// If there are no shards, then the CompositeArrayBuffer was initialized
		// with no data.
		if (this.shards.length === 0) {
			return new ArrayBuffer(0);
		}
		// NaN is treated as zero for slicing. This matches ArrayBuffer's behavior.
		start = isNaN(Number(start)) ? 0 : start;
		end = isNaN(Number(end)) ? 0 : end;
		// Fix the bounds to within the array.
		start = Math.max(0, start);
		end = Math.min(this.byteLength, end);
		if (end <= start) {
			return new ArrayBuffer(0);
		}
		const startShardIndex = this.findShardForByte(start);
		if (startShardIndex === -1) {
			// This should not happen since the start and end indices are always
			// within 0 and the composite array's length.
			throw new Error(`Could not find start shard for byte ${start}`);
		}
		const size = end - start;
		const outputBuffer = new ArrayBuffer(size);
		const outputArray = new Uint8Array(outputBuffer);
		let sliced = 0;
		for (let i = startShardIndex; i < this.shards.length; i++) {
			const shard = this.shards[i];
			const globalStart = start + sliced;
			const localStart = globalStart - shard.start;
			const outputStart = sliced;
			const globalEnd = Math.min(end, shard.end);
			const localEnd = globalEnd - shard.start;
			const outputSlice = new Uint8Array(shard.buffer, localStart, localEnd - localStart);
			outputArray.set(outputSlice, outputStart);
			sliced += outputSlice.length;
			if (end < shard.end) {
				break;
			}
		}
		return outputBuffer;
	}

	/**
	 * Get the index of the shard that contains the byte at `byteIndex`.
	 */
	findShardForByte(byteIndex) {
		if (this.shards.length === 0 || byteIndex < 0 ||
			byteIndex >= this.byteLength) {
			return -1;
		}
		// If the buffers have a uniform size, compute the shard directly.
		if (this.bufferUniformSize != null) {
			this.previousShardIndex = Math.floor(byteIndex / this.bufferUniformSize);
			return this.previousShardIndex;
		}
		// If the buffers don't have a uniform size, we need to search for the
		// shard. That means we need a function to check where the byteIndex lies
		// relative to a given shard.
		function check(shard) {
			if (byteIndex < shard.start) {
				return -1;
			}
			if (byteIndex >= shard.end) {
				return 1;
			}
			return 0;
		}

		// For efficiency, try the previous shard first.
		if (check(this.shards[this.previousShardIndex]) === 0) {
			return this.previousShardIndex;
		}
		// Otherwise, use a generic search function.
		// This should almost never end up being used in practice since the weight
		// entries should always be in order.
		const index = search(this.shards, check);
		if (index === -1) {
			return -1;
		}
		this.previousShardIndex = index;
		return this.previousShardIndex;
	}
}

/**
 * Search for an element of a sorted array.
 *
 * @param sortedArray The sorted array to search
 * @param compare A function to compare the current value against the searched
 *     value. Return 0 on a match, negative if the searched value is less than
 *     the value passed to the function, and positive if the searched value is
 *     greater than the value passed to the function.
 * @returns The index of the element, or -1 if it's not in the array.
 */
function search(sortedArray, compare) {
	// Binary search
	let min = 0;
	let max = sortedArray.length;
	while (min <= max) {
		const middle = Math.floor((max - min) / 2) + min;
		const side = compare(sortedArray[middle]);
		if (side === 0) {
			return middle;
		} else if (side < 0) {
			max = middle;
		} else {
			min = middle + 1;
		}
	}
	return -1;
}

/**
 * @license
 * Copyright 2018 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Enables production mode which disables correctness checks in favor of
 * performance.
 *
 * @doc {heading: 'Environment'}
 */
function enableProdMode() {
	env().set('PROD', true);
}

/**
 * Enables debug mode which will log information about all executed kernels:
 * the elapsed time of the kernel execution, as well as the rank, shape, and
 * size of the output tensor.
 *
 * Debug mode will significantly slow down your application as it will
 * download the result of every operation to the CPU. This should not be used in
 * production. Debug mode does not affect the timing information of the kernel
 * execution as we do not measure download time in the kernel execution time.
 *
 * See also: `tf.profile`, `tf.memory`.
 *
 * @doc {heading: 'Environment'}
 */
function enableDebugMode() {
	env().set('DEBUG', true);
}

/** Globally disables deprecation warnings */
function disableDeprecationWarnings() {
	env().set('DEPRECATION_WARNINGS_ENABLED', false);
	console.warn(`TensorFlow.js deprecation warnings have been disabled.`);
}

/** Warn users about deprecated functionality. */
function deprecationWarn(msg) {
	if (env().getBool('DEPRECATION_WARNINGS_ENABLED')) {
		console.warn(msg + ' You can disable deprecation warnings with ' +
			'tf.disableDeprecationWarnings().');
	}
}

/**
 * Dispose all variables kept in backend engine.
 *
 * @doc {heading: 'Environment'}
 */
function disposeVariables() {
	ENGINE.disposeVariables();
}

/**
 * It returns the global engine that keeps track of all tensors and backends.
 *
 * @doc {heading: 'Environment'}
 */
function engine() {
	return ENGINE;
}

/**
 * Returns memory info at the current time in the program. The result is an
 * object with the following properties:
 *
 * - `numBytes`: Number of bytes allocated (undisposed) at this time.
 * - `numTensors`: Number of unique tensors allocated.
 * - `numDataBuffers`: Number of unique data buffers allocated
 *   (undisposed) at this time, which is  the number of tensors
 *   (e.g. `a.reshape(newShape)` makes a new Tensor that shares the same
 *   data buffer with `a`).
 * - `unreliable`: True if the memory usage is unreliable. See `reasons` when
 *    `unreliable` is true.
 * - `reasons`: `string[]`, reasons why the memory is unreliable, present if
 *    `unreliable` is true.
 *
 * WebGL Properties:
 * - `numBytesInGPU`: Number of bytes allocated (undisposed) in the GPU only at
 *     this time.
 *
 * @doc {heading: 'Performance', subheading: 'Memory'}
 */
function memory() {
	return ENGINE.memory();
}

/**
 * Executes the provided function `f()` and returns a promise that resolves
 * with information about the function's memory use:
 * - `newBytes`: the number of new bytes allocated
 * - `newTensors`: the number of new tensors created
 * - `peakBytes`: the peak number of bytes allocated
 * - `kernels`: an array of objects for each kernel involved that reports
 * their input and output shapes, number of bytes used, and number of new
 * tensors created.
 * - `kernelNames`: an array of unique strings with just the names of the
 * kernels in the `kernels` array.
 *
 * ```js
 * const profile = await tf.profile(() => {
 *   const x = tf.tensor1d([1, 2, 3]);
 *   let x2 = x.square();
 *   x2.dispose();
 *   x2 = x.square();
 *   x2.dispose();
 *   return x;
 * });
 *
 * console.log(`newBytes: ${profile.newBytes}`);
 * console.log(`newTensors: ${profile.newTensors}`);
 * console.log(`byte usage over all kernels: ${profile.kernels.map(k =>
 * k.totalBytesSnapshot)}`);
 * ```
 *
 *
 * @doc {heading: 'Performance', subheading: 'Profile'}
 */
function profile(f) {
	return ENGINE.profile(f);
}

/**
 * Executes the provided function `fn` and after it is executed, cleans up all
 * intermediate tensors allocated by `fn` except those returned by `fn`.
 * `fn` must not return a Promise (async functions not allowed). The returned
 * result can be a complex object.
 *
 * Using this method helps avoid memory leaks. In general, wrap calls to
 * operations in `tf.tidy` for automatic memory cleanup.
 *
 * NOTE: Variables do *not* get cleaned up when inside a tidy(). If you want to
 * dispose variables, please use `tf.disposeVariables` or call dispose()
 * directly on variables.
 *
 * ```js
 * // y = 2 ^ 2 + 1
 * const y = tf.tidy(() => {
 *   // a, b, and one will be cleaned up when the tidy ends.
 *   const one = tf.scalar(1);
 *   const a = tf.scalar(2);
 *   const b = a.square();
 *
 *   console.log('numTensors (in tidy): ' + tf.memory().numTensors);
 *
 *   // The value returned inside the tidy function will return
 *   // through the tidy, in this case to the variable y.
 *   return b.add(one);
 * });
 *
 * console.log('numTensors (outside tidy): ' + tf.memory().numTensors);
 * y.print();
 * ```
 *
 * @param nameOrFn The name of the closure, or the function to execute.
 *     If a name is provided, the 2nd argument should be the function.
 *     If debug mode is on, the timing and the memory usage of the function
 *     will be tracked and displayed on the console using the provided name.
 * @param fn The function to execute.
 *
 * @doc {heading: 'Performance', subheading: 'Memory'}
 */
function tidy(nameOrFn, fn) {
	return ENGINE.tidy(nameOrFn, fn);
}

/**
 * Disposes any `tf.Tensor`s found within the provided object.
 *
 * @param container an object that may be a `tf.Tensor` or may directly
 *     contain `tf.Tensor`s, such as a `Tensor[]` or `{key: Tensor, ...}`. If
 *     the object is not a `tf.Tensor` or does not contain `Tensors`, nothing
 *     happens. In general it is safe to pass any object here, except that
 *     `Promise`s are not supported.
 *
 * @doc {heading: 'Performance', subheading: 'Memory'}
 */
function dispose(container) {
	const tensors = getTensorsInContainer(container);
	tensors.forEach(tensor => tensor.dispose());
}

/**
 * Keeps a `tf.Tensor` generated inside a `tf.tidy` from being disposed
 * automatically.
 *
 * ```js
 * let b;
 * const y = tf.tidy(() => {
 *   const one = tf.scalar(1);
 *   const a = tf.scalar(2);
 *
 *   // b will not be cleaned up by the tidy. a and one will be cleaned up
 *   // when the tidy ends.
 *   b = tf.keep(a.square());
 *
 *   console.log('numTensors (in tidy): ' + tf.memory().numTensors);
 *
 *   // The value returned inside the tidy function will return
 *   // through the tidy, in this case to the variable y.
 *   return b.add(one);
 * });
 *
 * console.log('numTensors (outside tidy): ' + tf.memory().numTensors);
 * console.log('y:');
 * y.print();
 * console.log('b:');
 * b.print();
 * ```
 *
 * @param result The tensor to keep from being disposed.
 *
 * @doc {heading: 'Performance', subheading: 'Memory'}
 */
function keep(result) {
	return ENGINE.keep(result);
}

/**
 * Executes `f()` and returns a promise that resolves with timing
 * information.
 *
 * The result is an object with the following properties:
 *
 * - `wallMs`: Wall execution time.
 * - `kernelMs`: Kernel execution time, ignoring data transfer. If using the
 * WebGL backend and the query timer extension is not available, this will
 * return an error object.
 * - On `WebGL` The following additional properties exist:
 *   - `uploadWaitMs`: CPU blocking time on texture uploads.
 *   - `downloadWaitMs`: CPU blocking time on texture downloads (readPixels).
 *
 * ```js
 * const x = tf.randomNormal([20, 20]);
 * const time = await tf.time(() => x.matMul(x));
 *
 * console.log(`kernelMs: ${time.kernelMs}, wallTimeMs: ${time.wallMs}`);
 * ```
 *
 * @param f The function to execute and time.
 *
 * @doc {heading: 'Performance', subheading: 'Timing'}
 */
function time(f) {
	return ENGINE.time(f);
}

/**
 * Sets the backend (cpu, webgl, wasm, etc) responsible for creating tensors and
 * executing operations on those tensors. Returns a promise that resolves
 * to a boolean if the backend initialization was successful.
 *
 * Note this disposes the current backend, if any, as well as any tensors
 * associated with it. A new backend is initialized, even if it is of the
 * same type as the previous one.
 *
 * @param backendName The name of the backend. Currently supports
 *     `'webgl'|'cpu'` in the browser, `'tensorflow'` under node.js
 *     (requires tfjs-node), and `'wasm'` (requires tfjs-backend-wasm).
 *
 * @doc {heading: 'Backends'}
 */
function setBackend(backendName) {
	return ENGINE.setBackend(backendName);
}

/**
 * Returns a promise that resolves when the currently selected backend (or the
 * highest priority one) has initialized. Await this promise when you are using
 * a backend that has async initialization.
 *
 * @doc {heading: 'Backends'}
 */
function ready() {
	return ENGINE.ready();
}

/**
 * Returns the current backend name (cpu, webgl, etc). The backend is
 * responsible for creating tensors and executing operations on those tensors.
 *
 * @doc {heading: 'Backends'}
 */
function getBackend() {
	return ENGINE.backendName;
}

/**
 * Removes a backend and the registered factory.
 *
 * @doc {heading: 'Backends'}
 */
function removeBackend(name) {
	ENGINE.removeBackend(name);
}

/**
 * Finds the backend registered under the provided name. Returns null if the
 * name is not in the registry, or the registration hasn't finished yet.
 */
function findBackend(name) {
	return ENGINE.findBackend(name);
}

/**
 * Finds the backend factory registered under the provided name. Returns a
 * function that produces a new backend when called. Returns null if the name
 * is not in the registry.
 */
function findBackendFactory(name) {
	return ENGINE.findBackendFactory(name);
}

/**
 * Registers a global backend. The registration should happen when importing
 * a module file (e.g. when importing `backend_webgl.ts`), and is used for
 * modular builds (e.g. custom tfjs bundle with only webgl support).
 *
 * @param factory The backend factory function. When called, it should
 * return a backend instance, or a promise of an instance.
 * @param priority The priority of the backend (higher = more important).
 *     In case multiple backends are registered, the priority is used to find
 *     the best backend. Defaults to 1.
 * @return False if there is already a registered backend under this name, true
 *     if not.
 *
 * @doc {heading: 'Backends'}
 */
function registerBackend(name, factory, priority = 1) {
	return ENGINE.registerBackend(name, factory, priority);
}

/**
 * Gets the current backend. If no backends have been initialized, this will
 * attempt to initialize the best backend. Will throw an error if the highest
 * priority backend has async initialization, in which case you should call
 * 'await tf.ready()' before running other code.
 *
 * @doc {heading: 'Backends'}
 */
function backend() {
	return ENGINE.backend;
}

/**
 * Sets the global platform.
 *
 * @param platformName The name of this platform.
 * @param platform A platform implementation.
 */
function setPlatform(platformName, platform) {
	env().setPlatform(platformName, platform);
}

/**
 * @license
 * Copyright 2018 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/** Number of bytes reserved for the length of the string. (32bit integer). */
const NUM_BYTES_STRING_LENGTH = 4;

/**
 * Encode a map from names to weight values as an ArrayBuffer, along with an
 * `Array` of `WeightsManifestEntry` as specification of the encoded weights.
 *
 * This function does not perform sharding.
 *
 * This function is the reverse of `decodeWeights`.
 *
 * @param tensors A map ("dict") from names to tensors.
 * @param group Group to which the weights belong (optional).
 * @returns A `Promise` of
 *   - A flat `ArrayBuffer` with all the binary values of the `Tensor`s
 *     concatenated.
 *   - An `Array` of `WeightManifestEntry`s, carrying information including
 *     tensor names, `dtype`s and shapes.
 * @throws Error: on unsupported tensor `dtype`.
 */
async function encodeWeights(tensors, group) {
	// TODO(adarob, cais): Support quantization.
	const specs = [];
	const dataPromises = [];
	const names = Array.isArray(tensors) ?
		tensors.map(tensor => tensor.name) :
		Object.keys(tensors);
	for (let i = 0; i < names.length; ++i) {
		const name = names[i];
		const t = Array.isArray(tensors) ? tensors[i].tensor : tensors[name];
		if (t.dtype !== 'float32' && t.dtype !== 'int32' && t.dtype !== 'bool' &&
			t.dtype !== 'string' && t.dtype !== 'complex64') {
			throw new Error(`Unsupported dtype in weight '${name}': ${t.dtype}`);
		}
		const spec = {name, shape: t.shape, dtype: t.dtype};
		if (t.dtype === 'string') {
			const utf8bytes = new Promise(async (resolve) => {
				const vals = await t.bytes();
				const totalNumBytes = vals.reduce((p, c) => p + c.length, 0) +
					NUM_BYTES_STRING_LENGTH * vals.length;
				const bytes = new Uint8Array(totalNumBytes);
				let offset = 0;
				for (let i = 0; i < vals.length; i++) {
					const val = vals[i];
					const bytesOfLength = new Uint8Array(new Uint32Array([val.length]).buffer);
					bytes.set(bytesOfLength, offset);
					offset += NUM_BYTES_STRING_LENGTH;
					bytes.set(val, offset);
					offset += val.length;
				}
				resolve(bytes);
			});
			dataPromises.push(utf8bytes);
		} else {
			dataPromises.push(t.data());
		}
		if (group != null) {
			spec.group = group;
		}
		specs.push(spec);
	}
	const tensorValues = await Promise.all(dataPromises);
	return {data: concatenateTypedArrays(tensorValues), specs};
}

/**
 * Decode flat ArrayBuffer as weights.
 *
 * This function does not handle sharding.
 *
 * This function is the reverse of `encodeWeights`.
 *
 * @param weightData A flat ArrayBuffer or an array of ArrayBuffers carrying the
 *   binary values of the tensors concatenated in the order specified in
 *   `specs`.
 * @param specs Specifications of the names, dtypes and shapes of the tensors
 *   whose value are encoded by `buffer`.
 * @return A map from tensor name to tensor value, with the names corresponding
 *   to names in `specs`.
 * @throws Error, if any of the tensors has unsupported dtype.
 */
function decodeWeights(weightData, specs) {
	// TODO(adarob, cais): Support quantization.
	const compositeBuffer = new CompositeArrayBuffer(weightData);
	const out = {};
	let offset = 0;
	for (const spec of specs) {
		const byteLength = getWeightBytelength(spec, (start, end) => {
			return compositeBuffer.slice(offset + start, offset + end);
		});
		out[spec.name] = decodeWeight(spec, compositeBuffer
			.slice(offset, offset + byteLength));
		offset += byteLength;
	}
	return out;
}

function getWeightBytelength(spec, slice) {
	const size = sizeFromShape(spec.shape);
	let bytesPerValue;
	if ('quantization' in spec) {
		const quantization = spec.quantization;
		bytesPerValue = DTYPE_VALUE_SIZE_MAP[quantization.dtype];
	} else if (spec.dtype === 'string') {
		// Can not statically determine string length.
		let byteLength = 0;
		for (let i = 0; i < size; i++) {
			byteLength += NUM_BYTES_STRING_LENGTH + new Uint32Array(slice(byteLength, byteLength + NUM_BYTES_STRING_LENGTH))[0];
		}
		return byteLength;
	} else {
		bytesPerValue = DTYPE_VALUE_SIZE_MAP[spec.dtype];
	}
	return size * bytesPerValue;
}

async function getWeightBytelengthAsync(spec, slice) {
	const size = sizeFromShape(spec.shape);
	let bytesPerValue;
	if ('quantization' in spec) {
		const quantization = spec.quantization;
		bytesPerValue = DTYPE_VALUE_SIZE_MAP[quantization.dtype];
	} else if (spec.dtype === 'string') {
		// Can not statically determine string length.
		let byteLength = 0;
		for (let i = 0; i < size; i++) {
			byteLength += NUM_BYTES_STRING_LENGTH + new Uint32Array(await slice(byteLength, byteLength + NUM_BYTES_STRING_LENGTH))[0];
		}
		return byteLength;
	} else {
		bytesPerValue = DTYPE_VALUE_SIZE_MAP[spec.dtype];
	}
	return size * bytesPerValue;
}

function decodeWeight(spec, byteBuffer) {
	const name = spec.name;
	const dtype = spec.dtype;
	const shape = spec.shape;
	const size = sizeFromShape(shape);
	let values;
	let offset = 0;
	if ('quantization' in spec) {
		const quantization = spec.quantization;
		if (quantization.dtype === 'uint8' || quantization.dtype === 'uint16') {
			if (!('min' in quantization && 'scale' in quantization)) {
				throw new Error(`Weight ${spec.name} with quantization ${quantization.dtype} ` +
					`doesn't have corresponding metadata min and scale.`);
			}
		} else if (quantization.dtype === 'float16') {
			if (dtype !== 'float32') {
				throw new Error(`Weight ${spec.name} is quantized with ${quantization.dtype} ` +
					`which only supports weights of type float32 not ${dtype}.`);
			}
		} else {
			throw new Error(`Weight ${spec.name} has unknown ` +
				`quantization dtype ${quantization.dtype}. ` +
				`Supported quantization dtypes are: ` +
				`'uint8', 'uint16', and 'float16'.`);
		}
		const quantizationSizeFactor = DTYPE_VALUE_SIZE_MAP[quantization.dtype];
		const quantizedArray = (quantization.dtype === 'uint8') ?
			new Uint8Array(byteBuffer) :
			new Uint16Array(byteBuffer);
		if (dtype === 'float32') {
			if (quantization.dtype === 'uint8' || quantization.dtype === 'uint16') {
				values = new Float32Array(quantizedArray.length);
				for (let i = 0; i < quantizedArray.length; i++) {
					const v = quantizedArray[i];
					values[i] = v * quantization.scale + quantization.min;
				}
			} else if (quantization.dtype === 'float16') {
				// TODO: This is inefficient. Make getFloat16Decoder efficient.
				const float16Decode = getFloat16Decoder();
				values = float16Decode(quantizedArray);
			} else {
				throw new Error(`Unsupported quantization type ${quantization.dtype} ` +
					`for weight type float32.`);
			}
		} else if (dtype === 'int32') {
			if (quantization.dtype !== 'uint8' && quantization.dtype !== 'uint16') {
				throw new Error(`Unsupported quantization type ${quantization.dtype} ` +
					`for weight type int32.`);
			}
			values = new Int32Array(quantizedArray.length);
			for (let i = 0; i < quantizedArray.length; i++) {
				const v = quantizedArray[i];
				values[i] = Math.round(v * quantization.scale + quantization.min);
			}
		} else {
			throw new Error(`Unsupported dtype in weight '${name}': ${dtype}`);
		}
		offset += size * quantizationSizeFactor;
	} else if (dtype === 'string') {
		const size = sizeFromShape(spec.shape);
		values = [];
		for (let i = 0; i < size; i++) {
			const byteLength = new Uint32Array(byteBuffer.slice(offset, offset + NUM_BYTES_STRING_LENGTH))[0];
			offset += NUM_BYTES_STRING_LENGTH;
			const bytes = new Uint8Array(byteBuffer.slice(offset, offset + byteLength));
			values.push(bytes);
			offset += byteLength;
		}
	} else {
		const dtypeFactor = DTYPE_VALUE_SIZE_MAP[dtype];
		if (dtype === 'float32') {
			values = new Float32Array(byteBuffer);
		} else if (dtype === 'int32') {
			values = new Int32Array(byteBuffer);
		} else if (dtype === 'bool') {
			values = new Uint8Array(byteBuffer);
		} else if (dtype === 'complex64') {
			values = new Float32Array(byteBuffer);
			const real = new Float32Array(values.length / 2);
			const image = new Float32Array(values.length / 2);
			for (let i = 0; i < real.length; i++) {
				real[i] = values[i * 2];
				image[i] = values[i * 2 + 1];
			}
			const realTensor = tensor(real, shape, 'float32');
			const imageTensor = tensor(image, shape, 'float32');
			const complexTensor = complex$1(realTensor, imageTensor);
			realTensor.dispose();
			imageTensor.dispose();
			return complexTensor;
		} else {
			throw new Error(`Unsupported dtype in weight '${name}': ${dtype}`);
		}
		offset += size * dtypeFactor;
	}
	return tensor(values, shape, dtype);
}

async function readToLength(reader, initialData, length) {
	let data = new Uint8Array(initialData);
	while (data.byteLength < length) {
		const {done, value} = await reader.read();
		if (done && value == null) {
			const missing = length - data.byteLength;
			throw new Error(`Reader is done but ${missing} bytes are still expected`);
		}
		// TODO: Don't create a new array every loop.
		const newData = new Uint8Array(data.length + value.byteLength);
		newData.set(data, 0);
		newData.set(new Uint8Array(value), data.length);
		data = newData;
	}
	return data.buffer;
}

async function decodeWeightsStream(weightStream, specs) {
	const tensors = {};
	const reader = weightStream.getReader();
	let data = new ArrayBuffer(0);
	for (const spec of specs) {
		const byteLength = await getWeightBytelengthAsync(spec, async (start, end) => {
			data = await readToLength(reader, data, end);
			return data.slice(start, end);
		});
		data = await readToLength(reader, data, byteLength);
		// Slice the tensor out
		const tensorData = data.slice(0, byteLength);
		data = data.slice(byteLength);
		const weightTensor = decodeWeight(spec, tensorData);
		tensors[spec.name] = weightTensor;
		// TODO(mattsoulanille): Better way to call uploadToGPU.
		// TODO(mattsoulanille): Make this work for webgl too.
		if (getBackend() === 'webgpu') {
			const b = backend();
			if ('uploadToGPU' in b &&
				sizeFromShape(weightTensor.shape) >= env()
					.get('WEBGPU_CPU_HANDOFF_SIZE_THRESHOLD')) {
				b.uploadToGPU(weightTensor.dataId);
			}
		}
	}
	return tensors;
}

/**
 * Concatenate TypedArrays into an ArrayBuffer.
 */
function concatenateTypedArrays(xs) {
	// TODO(adarob, cais): Support quantization.
	if (xs === null) {
		throw new Error(`Invalid input value: ${JSON.stringify(xs)}`);
	}
	let totalByteLength = 0;
	// `normalizedXs` is here for this reason: a `TypedArray`'s `buffer'
	// can have a different byte length from that of the `TypedArray` itself,
	// for example, when the `TypedArray` is created from an offset in an
	// `ArrayBuffer`. `normliazedXs` holds `TypedArray`s whose `buffer`s match
	// the `TypedArray` in byte length. If an element of `xs` does not show
	// this property, a new `TypedArray` that satisfy this property will be
	// constructed and pushed into `normalizedXs`.
	const normalizedXs = [];
	xs.forEach((x) => {
		totalByteLength += x.byteLength;
		// tslint:disable:no-any
		normalizedXs.push(x.byteLength === x.buffer.byteLength ? x :
			new x.constructor(x));
		if (!(x instanceof Float32Array || x instanceof Int32Array ||
			x instanceof Uint8Array)) {
			throw new Error(`Unsupported TypedArray subtype: ${x.constructor.name}`);
		}
		// tslint:enable:no-any
	});
	const y = new Uint8Array(totalByteLength);
	let offset = 0;
	normalizedXs.forEach((x) => {
		y.set(new Uint8Array(x.buffer), offset);
		offset += x.byteLength;
	});
	return y.buffer;
}

// Use Buffer on Node.js instead of Blob/atob/btoa
const useNodeBuffer = typeof Buffer !== 'undefined' &&
	(typeof Blob === 'undefined' || typeof atob === 'undefined' ||
		typeof btoa === 'undefined');

/**
 * Calculate the byte length of a JavaScript string.
 *
 * Note that a JavaScript string can contain wide characters, therefore the
 * length of the string is not necessarily equal to the byte length.
 *
 * @param str Input string.
 * @returns Byte length.
 */
function stringByteLength(str) {
	if (useNodeBuffer) {
		return Buffer.byteLength(str, 'utf8');
	}
	return new Blob([str]).size;
}

/**
 * Encode an ArrayBuffer as a base64 encoded string.
 *
 * @param buffer `ArrayBuffer` to be converted.
 * @returns A string that base64-encodes `buffer`.
 */
function arrayBufferToBase64String(buffer) {
	if (useNodeBuffer) {
		return Buffer.from(buffer).toString('base64');
	}
	const buf = new Uint8Array(buffer);
	let s = '';
	for (let i = 0, l = buf.length; i < l; i++) {
		s += String.fromCharCode(buf[i]);
	}
	return btoa(s);
}

/**
 * Decode a base64 string as an ArrayBuffer.
 *
 * @param str Base64 string.
 * @returns Decoded `ArrayBuffer`.
 */
function base64StringToArrayBuffer(str) {
	if (useNodeBuffer) {
		const buf = Buffer.from(str, 'base64');
		return buf.buffer.slice(buf.byteOffset, buf.byteOffset + buf.byteLength);
	}
	const s = atob(str);
	const buffer = new Uint8Array(s.length);
	for (let i = 0; i < s.length; ++i) {
		buffer.set([s.charCodeAt(i)], i);
	}
	return buffer.buffer;
}

/**
 * Concatenate a number of ArrayBuffers into one.
 *
 * @param buffers An array of ArrayBuffers to concatenate, or a single
 *     ArrayBuffer.
 * @returns Result of concatenating `buffers` in order.
 *
 * @deprecated Use tf.io.CompositeArrayBuffer.join() instead.
 */
function concatenateArrayBuffers(buffers) {
	return CompositeArrayBuffer.join(buffers);
}

/**
 * Get the basename of a path.
 *
 * Behaves in a way analogous to Linux's basename command.
 *
 * @param path
 */
function basename(path) {
	const SEPARATOR = '/';
	path = path.trim();
	while (path.endsWith(SEPARATOR)) {
		path = path.slice(0, path.length - 1);
	}
	const items = path.split(SEPARATOR);
	return items[items.length - 1];
}

/**
 * Create `ModelJSON` from `ModelArtifacts`.
 *
 * @param artifacts Model artifacts, describing the model and its weights.
 * @param manifest Weight manifest, describing where the weights of the
 *     `ModelArtifacts` are stored, and some metadata about them.
 * @returns Object representing the `model.json` file describing the model
 *     artifacts and weights
 */
function getModelJSONForModelArtifacts(artifacts, manifest) {
	const result = {
		modelTopology: artifacts.modelTopology,
		format: artifacts.format,
		generatedBy: artifacts.generatedBy,
		convertedBy: artifacts.convertedBy,
		weightsManifest: manifest
	};
	if (artifacts.signature != null) {
		result.signature = artifacts.signature;
	}
	if (artifacts.userDefinedMetadata != null) {
		result.userDefinedMetadata = artifacts.userDefinedMetadata;
	}
	if (artifacts.modelInitializer != null) {
		result.modelInitializer = artifacts.modelInitializer;
	}
	if (artifacts.initializerSignature != null) {
		result.initializerSignature = artifacts.initializerSignature;
	}
	if (artifacts.trainingConfig != null) {
		result.trainingConfig = artifacts.trainingConfig;
	}
	return result;
}

/**
 * Create `ModelArtifacts` from a JSON file and weights.
 *
 * @param modelJSON Object containing the parsed JSON of `model.json`
 * @param weightSpecs The list of WeightsManifestEntry for the model. Must be
 *     passed if the modelJSON has a weightsManifest.
 * @param weightData An ArrayBuffer or array of ArrayBuffers of weight data for
 *     the model corresponding to the weights in weightSpecs. Must be passed if
 *     the modelJSON has a weightsManifest.
 * @returns A Promise of the `ModelArtifacts`, as described by the JSON file.
 */
function getModelArtifactsForJSONSync(modelJSON, weightSpecs, weightData) {
	const modelArtifacts = {
		modelTopology: modelJSON.modelTopology,
		format: modelJSON.format,
		generatedBy: modelJSON.generatedBy,
		convertedBy: modelJSON.convertedBy
	};
	if (modelJSON.trainingConfig != null) {
		modelArtifacts.trainingConfig = modelJSON.trainingConfig;
	}
	if (modelJSON.weightsManifest != null) {
		if (!weightSpecs) {
			throw new Error('modelJSON has weightsManifest but weightSpecs is null');
		}
		if (!weightData) {
			throw new Error('modelJSON has weightsManifest but weightData is null');
		}
		modelArtifacts.weightSpecs = weightSpecs;
		modelArtifacts.weightData = weightData;
	}
	if (modelJSON.signature != null) {
		modelArtifacts.signature = modelJSON.signature;
	}
	if (modelJSON.userDefinedMetadata != null) {
		modelArtifacts.userDefinedMetadata = modelJSON.userDefinedMetadata;
	}
	if (modelJSON.modelInitializer != null) {
		modelArtifacts.modelInitializer = modelJSON.modelInitializer;
	}
	if (modelJSON.initializerSignature != null) {
		modelArtifacts.initializerSignature = modelJSON.initializerSignature;
	}
	return modelArtifacts;
}

/**
 * Create `ModelArtifacts` from a JSON file.
 *
 * @param modelJSON Object containing the parsed JSON of `model.json`
 * @param loadWeights Function that takes the JSON file's weights manifest,
 *     reads weights from the listed path(s), and returns a Promise of the
 *     weight manifest entries along with the weights data.
 * @returns A Promise of the `ModelArtifacts`, as described by the JSON file.
 */
async function getModelArtifactsForJSON(modelJSON, loadWeights) {
	let weightSpecs;
	let weightData;
	if (modelJSON.weightsManifest != null) {
		[weightSpecs, weightData] = await loadWeights(modelJSON.weightsManifest);
	}
	return getModelArtifactsForJSONSync(modelJSON, weightSpecs, weightData);
}

/**
 * Populate ModelArtifactsInfo fields for a model with JSON topology.
 * @param modelArtifacts
 * @returns A ModelArtifactsInfo object.
 */
function getModelArtifactsInfoForJSON(modelArtifacts) {
	if (modelArtifacts.modelTopology instanceof ArrayBuffer) {
		throw new Error('Expected JSON model topology, received ArrayBuffer.');
	}
	return {
		dateSaved: new Date(),
		modelTopologyType: 'JSON',
		modelTopologyBytes: modelArtifacts.modelTopology == null ?
			0 :
			stringByteLength(JSON.stringify(modelArtifacts.modelTopology)),
		weightSpecsBytes: modelArtifacts.weightSpecs == null ?
			0 :
			stringByteLength(JSON.stringify(modelArtifacts.weightSpecs)),
		weightDataBytes: modelArtifacts.weightData == null ?
			0 :
			new CompositeArrayBuffer(modelArtifacts.weightData).byteLength,
	};
}

/**
 * Concatenate the weights stored in a WeightsManifestConfig into a list of
 * WeightsManifestEntry
 *
 * @param weightsManifest The WeightsManifestConfig to extract weights from.
 * @returns A list of WeightsManifestEntry of the weights in the weightsManifest
 */
function getWeightSpecs(weightsManifest) {
	const weightSpecs = [];
	for (const entry of weightsManifest) {
		weightSpecs.push(...entry.weights);
	}
	return weightSpecs;
}

/**
 * Computes mantisa table for casting Float16 to Float32
 * See http://www.fox-toolkit.org/ftp/fasthalffloatconversion.pdf
 *
 * @returns Uint32Array, 2048 mantissa lookup values.
 */
function computeFloat16MantisaTable() {
	const convertMantissa = (i) => {
		let m = i << 13;
		let e = 0;
		while ((m & 0x00800000) === 0) {
			e -= 0x00800000;
			m <<= 1;
		}
		m &= -8388609;
		e += 0x38800000;
		return m | e;
	};
	const mantisaTable = new Uint32Array(2048);
	mantisaTable[0] = 0;
	for (let i = 1; i < 1024; i++) {
		mantisaTable[i] = convertMantissa(i);
	}
	for (let i = 1024; i < 2048; i++) {
		mantisaTable[i] = 0x38000000 + ((i - 1024) << 13);
	}
	return mantisaTable;
}

/**
 * Computes exponent table for casting Float16 to Float32
 * See http://www.fox-toolkit.org/ftp/fasthalffloatconversion.pdf
 *
 * @returns Uint32Array, 64 exponent lookup values.
 */
function computeFloat16ExponentTable() {
	const exponentTable = new Uint32Array(64);
	exponentTable[0] = 0;
	exponentTable[31] = 0x47800000;
	exponentTable[32] = 0x80000000;
	exponentTable[63] = 0xc7800000;
	for (let i = 1; i < 31; i++) {
		exponentTable[i] = i << 23;
	}
	for (let i = 33; i < 63; i++) {
		exponentTable[i] = 0x80000000 + ((i - 32) << 23);
	}
	return exponentTable;
}

/**
 * Computes offset table for casting Float16 to Float32
 * See http://www.fox-toolkit.org/ftp/fasthalffloatconversion.pdf
 *
 * @returns Uint32Array, 6d offset values.
 */
function computeFloat16OffsetTable() {
	const offsetTable = new Uint32Array(64);
	for (let i = 0; i < 64; i++) {
		offsetTable[i] = 1024;
	}
	offsetTable[0] = offsetTable[32] = 0;
	return offsetTable;
}

/**
 * Retrieve a Float16 decoder which will decode a ByteArray of Float16 values
 * to a Float32Array.
 *
 * @returns Function (buffer: Uint16Array) => Float32Array which decodes
 *          the Uint16Array of Float16 bytes to a Float32Array.
 */
function getFloat16Decoder() {
	// Algorithm is based off of
	// http://www.fox-toolkit.org/ftp/fasthalffloatconversion.pdf
	// Cache lookup tables
	const mantisaTable = computeFloat16MantisaTable();
	const exponentTable = computeFloat16ExponentTable();
	const offsetTable = computeFloat16OffsetTable();
	return (quantizedArray) => {
		const buffer = new ArrayBuffer(4 * quantizedArray.length);
		const bufferUint32View = new Uint32Array(buffer);
		for (let index = 0; index < quantizedArray.length; index++) {
			const float16Bits = quantizedArray[index];
			const float32Bits = mantisaTable[offsetTable[float16Bits >> 10] + (float16Bits & 0x3ff)] +
				exponentTable[float16Bits >> 10];
			bufferUint32View[index] = float32Bits;
		}
		return new Float32Array(buffer);
	};
}

/**
 * @license
 * Copyright 2018 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
class IORouterRegistry {
	constructor() {
		this.saveRouters = [];
		this.loadRouters = [];
	}

	static getInstance() {
		if (IORouterRegistry.instance == null) {
			IORouterRegistry.instance = new IORouterRegistry();
		}
		return IORouterRegistry.instance;
	}

	/**
	 * Register a save-handler router.
	 *
	 * @param saveRouter A function that maps a URL-like string onto an instance
	 * of `IOHandler` with the `save` method defined or `null`.
	 */
	static registerSaveRouter(saveRouter) {
		IORouterRegistry.getInstance().saveRouters.push(saveRouter);
	}

	/**
	 * Register a load-handler router.
	 *
	 * @param loadRouter A function that maps a URL-like string onto an instance
	 * of `IOHandler` with the `load` method defined or `null`.
	 */
	static registerLoadRouter(loadRouter) {
		IORouterRegistry.getInstance().loadRouters.push(loadRouter);
	}

	/**
	 * Look up IOHandler for saving, given a URL-like string.
	 *
	 * @param url
	 * @returns If only one match is found, an instance of IOHandler with the
	 * `save` method defined. If no match is found, `null`.
	 * @throws Error, if more than one match is found.
	 */
	static getSaveHandlers(url) {
		return IORouterRegistry.getHandlers(url, 'save');
	}

	/**
	 * Look up IOHandler for loading, given a URL-like string.
	 *
	 * @param url
	 * @param loadOptions Optional, custom load options.
	 * @returns All valid handlers for `url`, given the currently registered
	 *   handler routers.
	 */
	static getLoadHandlers(url, loadOptions) {
		return IORouterRegistry.getHandlers(url, 'load', loadOptions);
	}

	static getHandlers(url, handlerType, loadOptions) {
		const validHandlers = [];
		const routers = handlerType === 'load' ?
			IORouterRegistry.getInstance().loadRouters :
			IORouterRegistry.getInstance().saveRouters;
		routers.forEach(router => {
			const handler = router(url, loadOptions);
			if (handler !== null) {
				validHandlers.push(handler);
			}
		});
		return validHandlers;
	}
}

const registerSaveRouter = (loudRouter) => IORouterRegistry.registerSaveRouter(loudRouter);
const registerLoadRouter = (loudRouter) => IORouterRegistry.registerLoadRouter(loudRouter);
const getSaveHandlers = (url) => IORouterRegistry.getSaveHandlers(url);
const getLoadHandlers = (url, loadOptions) => IORouterRegistry.getLoadHandlers(url, loadOptions);

/**
 * @license
 * Copyright 2018 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
const DATABASE_NAME = 'tensorflowjs';
const DATABASE_VERSION = 1;
// Model data and ModelArtifactsInfo (metadata) are stored in two separate
// stores for efficient access of the list of stored models and their metadata.
// 1. The object store for model data: topology, weights and weight manifests.
const MODEL_STORE_NAME = 'models_store';
// 2. The object store for ModelArtifactsInfo, including meta-information such
//    as the type of topology (JSON vs binary), byte size of the topology, byte
//    size of the weights, etc.
const INFO_STORE_NAME = 'model_info_store';

function getIndexedDBFactory() {
	if (!env().getBool('IS_BROWSER')) {
		// TODO(cais): Add more info about what IOHandler subtypes are available.
		//   Maybe point to a doc page on the web and/or automatically determine
		//   the available IOHandlers and print them in the error message.
		throw new Error('Failed to obtain IndexedDB factory because the current environment' +
			'is not a web browser.');
	}
	// tslint:disable-next-line:no-any
	const theWindow = typeof window === 'undefined' ? self : window;
	const factory = theWindow.indexedDB || theWindow.mozIndexedDB ||
		theWindow.webkitIndexedDB || theWindow.msIndexedDB ||
		theWindow.shimIndexedDB;
	if (factory == null) {
		throw new Error('The current browser does not appear to support IndexedDB.');
	}
	return factory;
}

function setUpDatabase(openRequest) {
	const db = openRequest.result;
	db.createObjectStore(MODEL_STORE_NAME, {keyPath: 'modelPath'});
	db.createObjectStore(INFO_STORE_NAME, {keyPath: 'modelPath'});
}

/**
 * IOHandler subclass: Browser IndexedDB.
 *
 * See the doc string of `browserIndexedDB` for more details.
 */
class BrowserIndexedDB {
	constructor(modelPath) {
		this.indexedDB = getIndexedDBFactory();
		if (modelPath == null || !modelPath) {
			throw new Error('For IndexedDB, modelPath must not be null, undefined or empty.');
		}
		this.modelPath = modelPath;
	}

	async save(modelArtifacts) {
		// TODO(cais): Support saving GraphDef models.
		if (modelArtifacts.modelTopology instanceof ArrayBuffer) {
			throw new Error('BrowserLocalStorage.save() does not support saving model topology ' +
				'in binary formats yet.');
		}
		return this.databaseAction(this.modelPath, modelArtifacts);
	}

	async load() {
		return this.databaseAction(this.modelPath);
	}

	/**
	 * Perform database action to put model artifacts into or read model artifacts
	 * from IndexedDB object store.
	 *
	 * Whether the action is put or get depends on whether `modelArtifacts` is
	 * specified. If it is specified, the action will be put; otherwise the action
	 * will be get.
	 *
	 * @param modelPath A unique string path for the model.
	 * @param modelArtifacts If specified, it will be the model artifacts to be
	 *   stored in IndexedDB.
	 * @returns A `Promise` of `SaveResult`, if the action is put, or a `Promise`
	 *   of `ModelArtifacts`, if the action is get.
	 */
	databaseAction(modelPath, modelArtifacts) {
		return new Promise((resolve, reject) => {
			const openRequest = this.indexedDB.open(DATABASE_NAME, DATABASE_VERSION);
			openRequest.onupgradeneeded = () => setUpDatabase(openRequest);
			openRequest.onsuccess = () => {
				const db = openRequest.result;
				if (modelArtifacts == null) {
					// Read model out from object store.
					const modelTx = db.transaction(MODEL_STORE_NAME, 'readonly');
					const modelStore = modelTx.objectStore(MODEL_STORE_NAME);
					const getRequest = modelStore.get(this.modelPath);
					getRequest.onsuccess = () => {
						if (getRequest.result == null) {
							db.close();
							return reject(new Error(`Cannot find model with path '${this.modelPath}' ` +
								`in IndexedDB.`));
						} else {
							resolve(getRequest.result.modelArtifacts);
						}
					};
					getRequest.onerror = error => {
						db.close();
						return reject(getRequest.error);
					};
					modelTx.oncomplete = () => db.close();
				} else {
					// Put model into object store.
					// Concatenate all the model weights into a single ArrayBuffer. Large
					// models (~1GB) have problems saving if they are not concatenated.
					// TODO(mattSoulanille): Save large models to multiple indexeddb
					// records.
					modelArtifacts.weightData = CompositeArrayBuffer.join(modelArtifacts.weightData);
					const modelArtifactsInfo = getModelArtifactsInfoForJSON(modelArtifacts);
					// First, put ModelArtifactsInfo into info store.
					const infoTx = db.transaction(INFO_STORE_NAME, 'readwrite');
					let infoStore = infoTx.objectStore(INFO_STORE_NAME);
					let putInfoRequest;
					try {
						putInfoRequest =
							infoStore.put({modelPath: this.modelPath, modelArtifactsInfo});
					} catch (error) {
						return reject(error);
					}
					let modelTx;
					putInfoRequest.onsuccess = () => {
						// Second, put model data into model store.
						modelTx = db.transaction(MODEL_STORE_NAME, 'readwrite');
						const modelStore = modelTx.objectStore(MODEL_STORE_NAME);
						let putModelRequest;
						try {
							putModelRequest = modelStore.put({
								modelPath: this.modelPath,
								modelArtifacts,
								modelArtifactsInfo
							});
						} catch (error) {
							// Sometimes, the serialized value is too large to store.
							return reject(error);
						}
						putModelRequest.onsuccess = () => resolve({modelArtifactsInfo});
						putModelRequest.onerror = error => {
							// If the put-model request fails, roll back the info entry as
							// well.
							infoStore = infoTx.objectStore(INFO_STORE_NAME);
							const deleteInfoRequest = infoStore.delete(this.modelPath);
							deleteInfoRequest.onsuccess = () => {
								db.close();
								return reject(putModelRequest.error);
							};
							deleteInfoRequest.onerror = error => {
								db.close();
								return reject(putModelRequest.error);
							};
						};
					};
					putInfoRequest.onerror = error => {
						db.close();
						return reject(putInfoRequest.error);
					};
					infoTx.oncomplete = () => {
						if (modelTx == null) {
							db.close();
						} else {
							modelTx.oncomplete = () => db.close();
						}
					};
				}
			};
			openRequest.onerror = error => reject(openRequest.error);
		});
	}
}

BrowserIndexedDB.URL_SCHEME = 'indexeddb://';
const indexedDBRouter = (url) => {
	if (!env().getBool('IS_BROWSER')) {
		return null;
	} else {
		if (!Array.isArray(url) && url.startsWith(BrowserIndexedDB.URL_SCHEME)) {
			return browserIndexedDB(url.slice(BrowserIndexedDB.URL_SCHEME.length));
		} else {
			return null;
		}
	}
};
IORouterRegistry.registerSaveRouter(indexedDBRouter);
IORouterRegistry.registerLoadRouter(indexedDBRouter);

/**
 * Creates a browser IndexedDB IOHandler for saving and loading models.
 *
 * ```js
 * const model = tf.sequential();
 * model.add(
 *     tf.layers.dense({units: 1, inputShape: [100], activation: 'sigmoid'}));
 *
 * const saveResult = await model.save('indexeddb://MyModel'));
 * console.log(saveResult);
 * ```
 *
 * @param modelPath A unique identifier for the model to be saved. Must be a
 *   non-empty string.
 * @returns An instance of `BrowserIndexedDB` (subclass of `IOHandler`),
 *   which can be used with, e.g., `tf.Model.save`.
 */
function browserIndexedDB(modelPath) {
	return new BrowserIndexedDB(modelPath);
}

function maybeStripScheme$1(key) {
	return key.startsWith(BrowserIndexedDB.URL_SCHEME) ?
		key.slice(BrowserIndexedDB.URL_SCHEME.length) :
		key;
}

class BrowserIndexedDBManager {
	constructor() {
		this.indexedDB = getIndexedDBFactory();
	}

	async listModels() {
		return new Promise((resolve, reject) => {
			const openRequest = this.indexedDB.open(DATABASE_NAME, DATABASE_VERSION);
			openRequest.onupgradeneeded = () => setUpDatabase(openRequest);
			openRequest.onsuccess = () => {
				const db = openRequest.result;
				const tx = db.transaction(INFO_STORE_NAME, 'readonly');
				const store = tx.objectStore(INFO_STORE_NAME);
				// tslint:disable:max-line-length
				// Need to cast `store` as `any` here because TypeScript's DOM
				// library does not have the `getAll()` method even though the
				// method is supported in the latest version of most mainstream
				// browsers:
				// https://developer.mozilla.org/en-US/docs/Web/API/IDBObjectStore/getAll
				// tslint:enable:max-line-length
				// tslint:disable-next-line:no-any
				const getAllInfoRequest = store.getAll();
				getAllInfoRequest.onsuccess = () => {
					const out = {};
					for (const item of getAllInfoRequest.result) {
						out[item.modelPath] = item.modelArtifactsInfo;
					}
					resolve(out);
				};
				getAllInfoRequest.onerror = error => {
					db.close();
					return reject(getAllInfoRequest.error);
				};
				tx.oncomplete = () => db.close();
			};
			openRequest.onerror = error => reject(openRequest.error);
		});
	}

	async removeModel(path) {
		path = maybeStripScheme$1(path);
		return new Promise((resolve, reject) => {
			const openRequest = this.indexedDB.open(DATABASE_NAME, DATABASE_VERSION);
			openRequest.onupgradeneeded = () => setUpDatabase(openRequest);
			openRequest.onsuccess = () => {
				const db = openRequest.result;
				const infoTx = db.transaction(INFO_STORE_NAME, 'readwrite');
				const infoStore = infoTx.objectStore(INFO_STORE_NAME);
				const getInfoRequest = infoStore.get(path);
				let modelTx;
				getInfoRequest.onsuccess = () => {
					if (getInfoRequest.result == null) {
						db.close();
						return reject(new Error(`Cannot find model with path '${path}' ` +
							`in IndexedDB.`));
					} else {
						// First, delete the entry in the info store.
						const deleteInfoRequest = infoStore.delete(path);
						const deleteModelData = () => {
							// Second, delete the entry in the model store.
							modelTx = db.transaction(MODEL_STORE_NAME, 'readwrite');
							const modelStore = modelTx.objectStore(MODEL_STORE_NAME);
							const deleteModelRequest = modelStore.delete(path);
							deleteModelRequest.onsuccess = () => resolve(getInfoRequest.result.modelArtifactsInfo);
							deleteModelRequest.onerror = error => reject(getInfoRequest.error);
						};
						// Proceed with deleting model data regardless of whether deletion
						// of info data succeeds or not.
						deleteInfoRequest.onsuccess = deleteModelData;
						deleteInfoRequest.onerror = error => {
							deleteModelData();
							db.close();
							return reject(getInfoRequest.error);
						};
					}
				};
				getInfoRequest.onerror = error => {
					db.close();
					return reject(getInfoRequest.error);
				};
				infoTx.oncomplete = () => {
					if (modelTx == null) {
						db.close();
					} else {
						modelTx.oncomplete = () => db.close();
					}
				};
			};
			openRequest.onerror = error => reject(openRequest.error);
		});
	}
}

/**
 * @license
 * Copyright 2018 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
const PATH_SEPARATOR = '/';
const PATH_PREFIX = 'tensorflowjs_models';
const INFO_SUFFIX = 'info';
const MODEL_TOPOLOGY_SUFFIX = 'model_topology';
const WEIGHT_SPECS_SUFFIX = 'weight_specs';
const WEIGHT_DATA_SUFFIX = 'weight_data';
const MODEL_METADATA_SUFFIX = 'model_metadata';

function getModelKeys(path) {
	return {
		info: [PATH_PREFIX, path, INFO_SUFFIX].join(PATH_SEPARATOR),
		topology: [PATH_PREFIX, path, MODEL_TOPOLOGY_SUFFIX].join(PATH_SEPARATOR),
		weightSpecs: [PATH_PREFIX, path, WEIGHT_SPECS_SUFFIX].join(PATH_SEPARATOR),
		weightData: [PATH_PREFIX, path, WEIGHT_DATA_SUFFIX].join(PATH_SEPARATOR),
		modelMetadata: [PATH_PREFIX, path, MODEL_METADATA_SUFFIX].join(PATH_SEPARATOR)
	};
}

function removeItems(keys) {
	for (const key of Object.values(keys)) {
		window.localStorage.removeItem(key);
	}
}

/**
 * Get model path from a local-storage key.
 *
 * E.g., 'tensorflowjs_models/my/model/1/info' --> 'my/model/1'
 *
 * @param key
 */
function getModelPathFromKey(key) {
	const items = key.split(PATH_SEPARATOR);
	if (items.length < 3) {
		throw new Error(`Invalid key format: ${key}`);
	}
	return items.slice(1, items.length - 1).join(PATH_SEPARATOR);
}

function maybeStripScheme(key) {
	return key.startsWith(BrowserLocalStorage.URL_SCHEME) ?
		key.slice(BrowserLocalStorage.URL_SCHEME.length) :
		key;
}

/**
 * IOHandler subclass: Browser Local Storage.
 *
 * See the doc string to `browserLocalStorage` for more details.
 */
class BrowserLocalStorage {
	constructor(modelPath) {
		if (!env().getBool('IS_BROWSER') || typeof window === 'undefined' ||
			typeof window.localStorage === 'undefined') {
			// TODO(cais): Add more info about what IOHandler subtypes are
			// available.
			//   Maybe point to a doc page on the web and/or automatically determine
			//   the available IOHandlers and print them in the error message.
			throw new Error('The current environment does not support local storage.');
		}
		this.LS = window.localStorage;
		if (modelPath == null || !modelPath) {
			throw new Error('For local storage, modelPath must not be null, undefined or empty.');
		}
		this.modelPath = modelPath;
		this.keys = getModelKeys(this.modelPath);
	}

	/**
	 * Save model artifacts to browser local storage.
	 *
	 * See the documentation to `browserLocalStorage` for details on the saved
	 * artifacts.
	 *
	 * @param modelArtifacts The model artifacts to be stored.
	 * @returns An instance of SaveResult.
	 */
	async save(modelArtifacts) {
		if (modelArtifacts.modelTopology instanceof ArrayBuffer) {
			throw new Error('BrowserLocalStorage.save() does not support saving model topology ' +
				'in binary formats yet.');
		} else {
			const topology = JSON.stringify(modelArtifacts.modelTopology);
			const weightSpecs = JSON.stringify(modelArtifacts.weightSpecs);
			const modelArtifactsInfo = getModelArtifactsInfoForJSON(modelArtifacts);
			// TODO(mattsoulanille): Support saving models over 2GB that exceed
			// Chrome's ArrayBuffer size limit.
			const weightBuffer = CompositeArrayBuffer.join(modelArtifacts.weightData);
			try {
				this.LS.setItem(this.keys.info, JSON.stringify(modelArtifactsInfo));
				this.LS.setItem(this.keys.topology, topology);
				this.LS.setItem(this.keys.weightSpecs, weightSpecs);
				this.LS.setItem(this.keys.weightData, arrayBufferToBase64String(weightBuffer));
				// Note that JSON.stringify doesn't write out keys that have undefined
				// values, so for some keys, we set undefined instead of a null-ish
				// value.
				const metadata = {
					format: modelArtifacts.format,
					generatedBy: modelArtifacts.generatedBy,
					convertedBy: modelArtifacts.convertedBy,
					signature: modelArtifacts.signature != null ?
						modelArtifacts.signature :
						undefined,
					userDefinedMetadata: modelArtifacts.userDefinedMetadata != null ?
						modelArtifacts.userDefinedMetadata :
						undefined,
					modelInitializer: modelArtifacts.modelInitializer != null ?
						modelArtifacts.modelInitializer :
						undefined,
					initializerSignature: modelArtifacts.initializerSignature != null ?
						modelArtifacts.initializerSignature :
						undefined,
					trainingConfig: modelArtifacts.trainingConfig != null ?
						modelArtifacts.trainingConfig :
						undefined
				};
				this.LS.setItem(this.keys.modelMetadata, JSON.stringify(metadata));
				return {modelArtifactsInfo};
			} catch (err) {
				// If saving failed, clean up all items saved so far.
				removeItems(this.keys);
				throw new Error(`Failed to save model '${this.modelPath}' to local storage: ` +
					`size quota being exceeded is a possible cause of this failure: ` +
					`modelTopologyBytes=${modelArtifactsInfo.modelTopologyBytes}, ` +
					`weightSpecsBytes=${modelArtifactsInfo.weightSpecsBytes}, ` +
					`weightDataBytes=${modelArtifactsInfo.weightDataBytes}.`);
			}
		}
	}

	/**
	 * Load a model from local storage.
	 *
	 * See the documentation to `browserLocalStorage` for details on the saved
	 * artifacts.
	 *
	 * @returns The loaded model (if loading succeeds).
	 */
	async load() {
		const info = JSON.parse(this.LS.getItem(this.keys.info));
		if (info == null) {
			throw new Error(`In local storage, there is no model with name '${this.modelPath}'`);
		}
		if (info.modelTopologyType !== 'JSON') {
			throw new Error('BrowserLocalStorage does not support loading non-JSON model ' +
				'topology yet.');
		}
		const out = {};
		// Load topology.
		const topology = JSON.parse(this.LS.getItem(this.keys.topology));
		if (topology == null) {
			throw new Error(`In local storage, the topology of model '${this.modelPath}' ` +
				`is missing.`);
		}
		out.modelTopology = topology;
		// Load weight specs.
		const weightSpecs = JSON.parse(this.LS.getItem(this.keys.weightSpecs));
		if (weightSpecs == null) {
			throw new Error(`In local storage, the weight specs of model '${this.modelPath}' ` +
				`are missing.`);
		}
		out.weightSpecs = weightSpecs;
		// Load meta-data fields.
		const metadataString = this.LS.getItem(this.keys.modelMetadata);
		if (metadataString != null) {
			const metadata = JSON.parse(metadataString);
			out.format = metadata.format;
			out.generatedBy = metadata.generatedBy;
			out.convertedBy = metadata.convertedBy;
			if (metadata.signature != null) {
				out.signature = metadata.signature;
			}
			if (metadata.userDefinedMetadata != null) {
				out.userDefinedMetadata = metadata.userDefinedMetadata;
			}
			if (metadata.modelInitializer != null) {
				out.modelInitializer = metadata.modelInitializer;
			}
			if (metadata.initializerSignature != null) {
				out.initializerSignature = metadata.initializerSignature;
			}
			if (metadata.trainingConfig != null) {
				out.trainingConfig = metadata.trainingConfig;
			}
		}
		// Load weight data.
		const weightDataBase64 = this.LS.getItem(this.keys.weightData);
		if (weightDataBase64 == null) {
			throw new Error(`In local storage, the binary weight values of model ` +
				`'${this.modelPath}' are missing.`);
		}
		out.weightData = base64StringToArrayBuffer(weightDataBase64);
		return out;
	}
}

BrowserLocalStorage.URL_SCHEME = 'localstorage://';
const localStorageRouter = (url) => {
	if (!env().getBool('IS_BROWSER')) {
		return null;
	} else {
		if (!Array.isArray(url) && url.startsWith(BrowserLocalStorage.URL_SCHEME)) {
			return browserLocalStorage(url.slice(BrowserLocalStorage.URL_SCHEME.length));
		} else {
			return null;
		}
	}
};
IORouterRegistry.registerSaveRouter(localStorageRouter);
IORouterRegistry.registerLoadRouter(localStorageRouter);

/**
 * Factory function for local storage IOHandler.
 *
 * This `IOHandler` supports both `save` and `load`.
 *
 * For each model's saved artifacts, four items are saved to local storage.
 *   - `${PATH_SEPARATOR}/${modelPath}/info`: Contains meta-info about the
 *     model, such as date saved, type of the topology, size in bytes, etc.
 *   - `${PATH_SEPARATOR}/${modelPath}/topology`: Model topology. For Keras-
 *     style models, this is a stringized JSON.
 *   - `${PATH_SEPARATOR}/${modelPath}/weight_specs`: Weight specs of the
 *     model, can be used to decode the saved binary weight values (see
 *     item below).
 *   - `${PATH_SEPARATOR}/${modelPath}/weight_data`: Concatenated binary
 *     weight values, stored as a base64-encoded string.
 *
 * Saving may throw an `Error` if the total size of the artifacts exceed the
 * browser-specific quota.
 *
 * @param modelPath A unique identifier for the model to be saved. Must be a
 *   non-empty string.
 * @returns An instance of `IOHandler`, which can be used with, e.g.,
 *   `tf.Model.save`.
 */
function browserLocalStorage(modelPath) {
	return new BrowserLocalStorage(modelPath);
}

class BrowserLocalStorageManager {
	constructor() {
		assert(env().getBool('IS_BROWSER'), () => 'Current environment is not a web browser');
		assert(typeof window === 'undefined' ||
			typeof window.localStorage !== 'undefined', () => 'Current browser does not appear to support localStorage');
		this.LS = window.localStorage;
	}

	async listModels() {
		const out = {};
		const prefix = PATH_PREFIX + PATH_SEPARATOR;
		const suffix = PATH_SEPARATOR + INFO_SUFFIX;
		for (let i = 0; i < this.LS.length; ++i) {
			const key = this.LS.key(i);
			if (key.startsWith(prefix) && key.endsWith(suffix)) {
				const modelPath = getModelPathFromKey(key);
				out[modelPath] = JSON.parse(this.LS.getItem(key));
			}
		}
		return out;
	}

	async removeModel(path) {
		path = maybeStripScheme(path);
		const keys = getModelKeys(path);
		if (this.LS.getItem(keys.info) == null) {
			throw new Error(`Cannot find model at path '${path}'`);
		}
		const info = JSON.parse(this.LS.getItem(keys.info));
		removeItems(keys);
		return info;
	}
}

/**
 * @license
 * Copyright 2018 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * IOHandlers related to files, such as browser-triggered file downloads,
 * user-selected files in browser.
 */
const DEFAULT_FILE_NAME_PREFIX = 'model';
const DEFAULT_JSON_EXTENSION_NAME = '.json';
const DEFAULT_WEIGHT_DATA_EXTENSION_NAME = '.weights.bin';

function defer(f) {
	return new Promise(resolve => setTimeout(resolve)).then(f);
}

class BrowserDownloads {
	constructor(fileNamePrefix) {
		if (!env().getBool('IS_BROWSER')) {
			// TODO(cais): Provide info on what IOHandlers are available under the
			//   current environment.
			throw new Error('browserDownloads() cannot proceed because the current environment ' +
				'is not a browser.');
		}
		if (fileNamePrefix.startsWith(BrowserDownloads.URL_SCHEME)) {
			fileNamePrefix = fileNamePrefix.slice(BrowserDownloads.URL_SCHEME.length);
		}
		if (fileNamePrefix == null || fileNamePrefix.length === 0) {
			fileNamePrefix = DEFAULT_FILE_NAME_PREFIX;
		}
		this.modelJsonFileName = fileNamePrefix + DEFAULT_JSON_EXTENSION_NAME;
		this.weightDataFileName =
			fileNamePrefix + DEFAULT_WEIGHT_DATA_EXTENSION_NAME;
	}

	async save(modelArtifacts) {
		if (typeof (document) === 'undefined') {
			throw new Error('Browser downloads are not supported in ' +
				'this environment since `document` is not present');
		}
		// TODO(mattsoulanille): Support saving models over 2GB that exceed
		// Chrome's ArrayBuffer size limit.
		const weightBuffer = CompositeArrayBuffer.join(modelArtifacts.weightData);
		const weightsURL = window.URL.createObjectURL(new Blob([weightBuffer], {type: 'application/octet-stream'}));
		if (modelArtifacts.modelTopology instanceof ArrayBuffer) {
			throw new Error('BrowserDownloads.save() does not support saving model topology ' +
				'in binary formats yet.');
		} else {
			const weightsManifest = [{
				paths: ['./' + this.weightDataFileName],
				weights: modelArtifacts.weightSpecs
			}];
			const modelJSON = getModelJSONForModelArtifacts(modelArtifacts, weightsManifest);
			const modelJsonURL = window.URL.createObjectURL(new Blob([JSON.stringify(modelJSON)], {type: 'application/json'}));
			// If anchor elements are not provided, create them without attaching them
			// to parents, so that the downloaded file names can be controlled.
			const jsonAnchor = this.modelJsonAnchor == null ?
				document.createElement('a') :
				this.modelJsonAnchor;
			jsonAnchor.download = this.modelJsonFileName;
			jsonAnchor.href = modelJsonURL;
			// Trigger downloads by evoking a click event on the download anchors.
			// When multiple downloads are started synchronously, Firefox will only
			// save the last one.
			await defer(() => jsonAnchor.dispatchEvent(new MouseEvent('click')));
			if (modelArtifacts.weightData != null) {
				const weightDataAnchor = this.weightDataAnchor == null ?
					document.createElement('a') :
					this.weightDataAnchor;
				weightDataAnchor.download = this.weightDataFileName;
				weightDataAnchor.href = weightsURL;
				await defer(() => weightDataAnchor.dispatchEvent(new MouseEvent('click')));
			}
			return {modelArtifactsInfo: getModelArtifactsInfoForJSON(modelArtifacts)};
		}
	}
}

BrowserDownloads.URL_SCHEME = 'downloads://';

class BrowserFiles {
	constructor(files) {
		if (files == null || files.length < 1) {
			throw new Error(`When calling browserFiles, at least 1 file is required, ` +
				`but received ${files}`);
		}
		this.jsonFile = files[0];
		this.weightsFiles = files.slice(1);
	}

	async load() {
		return new Promise((resolve, reject) => {
			const jsonReader = new FileReader();
			jsonReader.onload = (event) => {
				// tslint:disable-next-line:no-any
				const modelJSON = JSON.parse(event.target.result);
				const modelTopology = modelJSON.modelTopology;
				if (modelTopology == null) {
					reject(new Error(`modelTopology field is missing from file ${this.jsonFile.name}`));
					return;
				}
				const weightsManifest = modelJSON.weightsManifest;
				if (weightsManifest == null) {
					reject(new Error(`weightManifest field is missing from file ${this.jsonFile.name}`));
					return;
				}
				if (this.weightsFiles.length === 0) {
					resolve({modelTopology});
					return;
				}
				const modelArtifactsPromise = getModelArtifactsForJSON(modelJSON, (weightsManifest) => this.loadWeights(weightsManifest));
				resolve(modelArtifactsPromise);
			};
			jsonReader.onerror = error => reject(`Failed to read model topology and weights manifest JSON ` +
				`from file '${this.jsonFile.name}'. BrowserFiles supports loading ` +
				`Keras-style tf.Model artifacts only.`);
			jsonReader.readAsText(this.jsonFile);
		});
	}

	loadWeights(weightsManifest) {
		const weightSpecs = [];
		const paths = [];
		for (const entry of weightsManifest) {
			weightSpecs.push(...entry.weights);
			paths.push(...entry.paths);
		}
		const pathToFile = this.checkManifestAndWeightFiles(weightsManifest);
		const promises = paths.map(path => this.loadWeightsFile(path, pathToFile[path]));
		return Promise.all(promises).then(buffers => [weightSpecs, buffers]);
	}

	loadWeightsFile(path, file) {
		return new Promise((resolve, reject) => {
			const weightFileReader = new FileReader();
			weightFileReader.onload = (event) => {
				// tslint:disable-next-line:no-any
				const weightData = event.target.result;
				resolve(weightData);
			};
			weightFileReader.onerror = error => reject(`Failed to weights data from file of path '${path}'.`);
			weightFileReader.readAsArrayBuffer(file);
		});
	}

	/**
	 * Check the compatibility between weights manifest and weight files.
	 */
	checkManifestAndWeightFiles(manifest) {
		const basenames = [];
		const fileNames = this.weightsFiles.map(file => basename(file.name));
		const pathToFile = {};
		for (const group of manifest) {
			group.paths.forEach(path => {
				const pathBasename = basename(path);
				if (basenames.indexOf(pathBasename) !== -1) {
					throw new Error(`Duplicate file basename found in weights manifest: ` +
						`'${pathBasename}'`);
				}
				basenames.push(pathBasename);
				if (fileNames.indexOf(pathBasename) === -1) {
					throw new Error(`Weight file with basename '${pathBasename}' is not provided.`);
				} else {
					pathToFile[path] = this.weightsFiles[fileNames.indexOf(pathBasename)];
				}
			});
		}
		if (basenames.length !== this.weightsFiles.length) {
			throw new Error(`Mismatch in the number of files in weights manifest ` +
				`(${basenames.length}) and the number of weight files provided ` +
				`(${this.weightsFiles.length}).`);
		}
		return pathToFile;
	}
}

const browserDownloadsRouter = (url) => {
	if (!env().getBool('IS_BROWSER')) {
		return null;
	} else {
		if (!Array.isArray(url) && url.startsWith(BrowserDownloads.URL_SCHEME)) {
			return browserDownloads(url.slice(BrowserDownloads.URL_SCHEME.length));
		} else {
			return null;
		}
	}
};
IORouterRegistry.registerSaveRouter(browserDownloadsRouter);

/**
 * Creates an IOHandler that triggers file downloads from the browser.
 *
 * The returned `IOHandler` instance can be used as model exporting methods such
 * as `tf.Model.save` and supports only saving.
 *
 * ```js
 * const model = tf.sequential();
 * model.add(tf.layers.dense(
 *     {units: 1, inputShape: [10], activation: 'sigmoid'}));
 * const saveResult = await model.save('downloads://mymodel');
 * // This will trigger downloading of two files:
 * //   'mymodel.json' and 'mymodel.weights.bin'.
 * console.log(saveResult);
 * ```
 *
 * @param fileNamePrefix Prefix name of the files to be downloaded. For use with
 *   `tf.Model`, `fileNamePrefix` should follow either of the following two
 *   formats:
 *   1. `null` or `undefined`, in which case the default file
 *      names will be used:
 *      - 'model.json' for the JSON file containing the model topology and
 *        weights manifest.
 *      - 'model.weights.bin' for the binary file containing the binary weight
 *        values.
 *   2. A single string or an Array of a single string, as the file name prefix.
 *      For example, if `'foo'` is provided, the downloaded JSON
 *      file and binary weights file will be named 'foo.json' and
 *      'foo.weights.bin', respectively.
 * @param config Additional configuration for triggering downloads.
 * @returns An instance of `BrowserDownloads` `IOHandler`.
 *
 * @doc {
 *   heading: 'Models',
 *   subheading: 'Loading',
 *   namespace: 'io',
 *   ignoreCI: true
 * }
 */
function browserDownloads(fileNamePrefix = 'model') {
	return new BrowserDownloads(fileNamePrefix);
}

/**
 * Creates an IOHandler that loads model artifacts from user-selected files.
 *
 * This method can be used for loading from files such as user-selected files
 * in the browser.
 * When used in conjunction with `tf.loadLayersModel`, an instance of
 * `tf.LayersModel` (Keras-style) can be constructed from the loaded artifacts.
 *
 * ```js
 * // Note: This code snippet won't run properly without the actual file input
 * //   elements in the HTML DOM.
 *
 * // Suppose there are two HTML file input (`<input type="file" ...>`)
 * // elements.
 * const uploadJSONInput = document.getElementById('upload-json');
 * const uploadWeightsInput = document.getElementById('upload-weights');
 * const model = await tf.loadLayersModel(tf.io.browserFiles(
 *     [uploadJSONInput.files[0], uploadWeightsInput.files[0]]));
 * ```
 *
 * @param files `File`s to load from. Currently, this function supports only
 *   loading from files that contain Keras-style models (i.e., `tf.Model`s), for
 *   which an `Array` of `File`s is expected (in that order):
 *   - A JSON file containing the model topology and weight manifest.
 *   - Optionally, one or more binary files containing the binary weights.
 *     These files must have names that match the paths in the `weightsManifest`
 *     contained by the aforementioned JSON file, or errors will be thrown
 *     during loading. These weights files have the same format as the ones
 *     generated by `tensorflowjs_converter` that comes with the `tensorflowjs`
 *     Python PIP package. If no weights files are provided, only the model
 *     topology will be loaded from the JSON file above.
 * @returns An instance of `Files` `IOHandler`.
 *
 * @doc {
 *   heading: 'Models',
 *   subheading: 'Loading',
 *   namespace: 'io',
 *   ignoreCI: true
 * }
 */
function browserFiles(files) {
	return new BrowserFiles(files);
}

/**
 * @license
 * Copyright 2019 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Monitor Promise.all progress, fire onProgress callback function.
 *
 * @param promises Promise list going to be monitored
 * @param onProgress Callback function. Fired when a promise resolved.
 * @param startFraction Optional fraction start. Default to 0.
 * @param endFraction Optional fraction end. Default to 1.
 */
function monitorPromisesProgress(promises, onProgress, startFraction, endFraction) {
	checkPromises(promises);
	startFraction = startFraction == null ? 0 : startFraction;
	endFraction = endFraction == null ? 1 : endFraction;
	checkFraction(startFraction, endFraction);
	let resolvedPromise = 0;
	const registerMonitor = (promise) => {
		promise.then(value => {
			const fraction = startFraction +
				++resolvedPromise / promises.length * (endFraction - startFraction);
			// pass fraction as parameter to callback function.
			onProgress(fraction);
			return value;
		});
		return promise;
	};

	function checkPromises(promises) {
		assert(promises != null && Array.isArray(promises) && promises.length > 0, () => 'promises must be a none empty array');
	}

	function checkFraction(startFraction, endFraction) {
		assert(startFraction >= 0 && startFraction <= 1, () => `Progress fraction must be in range [0, 1], but ` +
			`got startFraction ${startFraction}`);
		assert(endFraction >= 0 && endFraction <= 1, () => `Progress fraction must be in range [0, 1], but ` +
			`got endFraction ${endFraction}`);
		assert(endFraction >= startFraction, () => `startFraction must be no more than endFraction, but ` +
			`got startFraction ${startFraction} and endFraction ` +
			`${endFraction}`);
	}

	return Promise.all(promises.map(registerMonitor));
}

/**
 * @license
 * Copyright 2018 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Reads binary weights data from a number of URLs.
 *
 * @param fetchURLs URLs to send the HTTP requests at, using `fetch` calls.
 * @param requestOptions RequestInit (options) for the HTTP requests.
 * @param fetchFunc Optional overriding value for the `window.fetch` function.
 * @param onProgress Optional, progress callback function, fired periodically
 *   before the load is completed.
 * @returns A `Promise` of an Array of `ArrayBuffer`. The Array has the same
 *   length as `fetchURLs`.
 */
async function loadWeightsAsArrayBuffer(fetchURLs, loadOptions) {
	if (loadOptions == null) {
		loadOptions = {};
	}
	const fetchFunc = loadOptions.fetchFunc == null ? env().platform.fetch :
		loadOptions.fetchFunc;
	// Create the requests for all of the weights in parallel.
	const requests = fetchURLs.map(fetchURL => fetchFunc(fetchURL, loadOptions.requestInit, {isBinary: true}));
	const fetchStartFraction = 0;
	const fetchEndFraction = 0.5;
	const responses = loadOptions.onProgress == null ?
		await Promise.all(requests) :
		await monitorPromisesProgress(requests, loadOptions.onProgress, fetchStartFraction, fetchEndFraction);
	const bufferPromises = responses.map(response => response.arrayBuffer());
	const bufferStartFraction = 0.5;
	const bufferEndFraction = 1;
	const buffers = loadOptions.onProgress == null ?
		await Promise.all(bufferPromises) :
		await monitorPromisesProgress(bufferPromises, loadOptions.onProgress, bufferStartFraction, bufferEndFraction);
	return buffers;
}

function streamWeights(fetchURLs, loadOptions) {
	var _a;
	const fetchFunc = loadOptions.fetchFunc == null ? env().platform.fetch :
		loadOptions.fetchFunc;
	let fetchIndex = 0;
	let chunkReader;
	(_a = loadOptions.onProgress) === null || _a === void 0 ? void 0 : _a.call(loadOptions, 0);
	return new ReadableStream({
		pull: async (controller) => {
			var _a;
			while (fetchIndex < fetchURLs.length) {
				if (!chunkReader) {
					const body = (await fetchFunc(fetchURLs[fetchIndex], loadOptions.requestInit, {isBinary: true})).body;
					chunkReader = body.getReader();
				}
				const {done, value} = await chunkReader.read();
				if (done) {
					fetchIndex++;
					chunkReader = undefined;
					(_a = loadOptions.onProgress) === null || _a === void 0 ? void 0 : _a.call(loadOptions, fetchIndex / fetchURLs.length);
					continue;
				}
				controller.enqueue(value);
				return;
			}
			controller.close();
		},
	});
}

/**
 * Reads a weights manifest JSON configuration, fetches the weights and
 * returns them as `Tensor`s.
 *
 * @param manifest The weights manifest JSON.
 * @param filePathPrefix The path prefix for filenames given in the manifest.
 *     Defaults to the empty string.
 * @param weightNames The names of the weights to be fetched.
 */
async function loadWeights(manifest, filePathPrefix = '', weightNames, requestInit) {
	// TODO(nsthorat): Groups are currently fetched atomically. If you need a
	// single weight from a group, the whole group will be fetched. At a future
	// date, we should support fetching only the individual shards within a
	// group that are needed to reconstruct the requested weight.
	// TODO(cais): Use `decodeWeights` for implementation.
	const fetchWeights = (fetchUrls) => loadWeightsAsArrayBuffer(fetchUrls, {requestInit});
	const loadWeights = weightsLoaderFactory(fetchWeights);
	return loadWeights(manifest, filePathPrefix, weightNames);
}

/**
 * Creates a function, which reads a weights manifest JSON configuration,
 * fetches the weight files using the specified function and returns them as
 * `Tensor`s.
 *
 * ```js
 * // example for creating a nodejs weight loader, which reads the weight files
 * // from disk using fs.readFileSync
 *
 * import * as fs from 'fs'
 *
 * const fetchWeightsFromDisk = (filePaths: string[]) =>
 *   filePaths.map(filePath => fs.readFileSync(filePath).buffer)
 *
 * const loadWeights = tf.io.weightsLoaderFactory(fetchWeightsFromDisk)
 *
 * const manifest = JSON.parse(
 *   fs.readFileSync('./my_model-weights_manifest').toString()
 * )
 * const weightMap = await loadWeights(manifest, './')
 * ```
 * @param fetchWeightsFunction The function used for fetching the weight files.
 * @returns Weight loading function.
 */
function weightsLoaderFactory(fetchWeightsFunction) {
	return async (manifest, filePathPrefix = '', weightNames) => {
		// Collect all the groups, weights, and their relative offsets to be
		// fetched.
		const groupIndicesToFetchMap = manifest.map(() => false);
		const groupWeightsToFetch = {};
		const weightsFound = weightNames != null ? weightNames.map(() => false) : [];
		const allManifestWeightNames = [];
		manifest.forEach((manifestGroupConfig, groupIndex) => {
			let groupOffset = 0;
			manifestGroupConfig.weights.forEach(weightsEntry => {
				const rawDtype = ('quantization' in weightsEntry) ?
					weightsEntry.quantization.dtype :
					weightsEntry.dtype;
				const weightsBytes = DTYPE_VALUE_SIZE_MAP[rawDtype] *
					sizeFromShape(weightsEntry.shape);
				const enqueueWeightsForFetchingFn = () => {
					groupIndicesToFetchMap[groupIndex] = true;
					if (groupWeightsToFetch[groupIndex] == null) {
						groupWeightsToFetch[groupIndex] = [];
					}
					groupWeightsToFetch[groupIndex].push({
						manifestEntry: weightsEntry,
						groupOffset,
						sizeBytes: weightsBytes
					});
				};
				if (weightNames != null) {
					weightNames.forEach((weightName, weightIndex) => {
						if (weightName === weightsEntry.name) {
							enqueueWeightsForFetchingFn();
							weightsFound[weightIndex] = true;
						}
					});
				} else {
					enqueueWeightsForFetchingFn();
				}
				allManifestWeightNames.push(weightsEntry.name);
				groupOffset += weightsBytes;
			});
		});
		if (!weightsFound.every(found => found)) {
			const weightsNotFound = weightNames.filter((_, i) => !weightsFound[i]);
			throw new Error(`Could not find weights in manifest with names: ` +
				`${weightsNotFound.join(', ')}. \n` +
				`Manifest JSON has weights with names: ` +
				`${allManifestWeightNames.join(', ')}.`);
		}
		// Convert the one-hot boolean groupId => shouldFetch map to a list of group
		// IDs.
		const groupIndicesToFetch = groupIndicesToFetchMap.reduce((accumulator, shouldFetch, i) => {
			if (shouldFetch) {
				accumulator.push(i);
			}
			return accumulator;
		}, []);
		const fetchUrls = [];
		groupIndicesToFetch.forEach(i => {
			manifest[i].paths.forEach(filepath => {
				const fetchUrl = filePathPrefix +
					(!filePathPrefix.endsWith('/') ? '/' : '') + filepath;
				fetchUrls.push(fetchUrl);
			});
		});
		const buffers = await fetchWeightsFunction(fetchUrls);
		const weightsTensorMap = {};
		let bufferIndexOffset = 0;
		groupIndicesToFetch.forEach(i => {
			const numBuffers = manifest[i].paths.length;
			const weightsBuffer = new CompositeArrayBuffer(buffers.slice(bufferIndexOffset, bufferIndexOffset + numBuffers));
			const weightsEntries = groupWeightsToFetch[i];
			weightsEntries.forEach(weightsEntry => {
				const byteBuffer = weightsBuffer.slice(weightsEntry.groupOffset, weightsEntry.groupOffset + weightsEntry.sizeBytes);
				const nameToTensorMap = decodeWeights(byteBuffer, [weightsEntry.manifestEntry]);
				for (const name in nameToTensorMap) {
					weightsTensorMap[name] = nameToTensorMap[name];
				}
			});
			bufferIndexOffset += numBuffers;
		});
		return weightsTensorMap;
	};
}

/**
 * @license
 * Copyright 2018 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * IOHandler implementations based on HTTP requests in the web browser.
 *
 * Uses [`fetch`](https://developer.mozilla.org/en-US/docs/Web/API/Fetch_API).
 */
const OCTET_STREAM_MIME_TYPE = 'application/octet-stream';
const JSON_TYPE = 'application/json';

class HTTPRequest {
	constructor(path, loadOptions) {
		this.DEFAULT_METHOD = 'POST';
		if (loadOptions == null) {
			loadOptions = {};
		}
		this.weightPathPrefix = loadOptions.weightPathPrefix;
		this.weightUrlConverter = loadOptions.weightUrlConverter;
		if (loadOptions.fetchFunc != null) {
			assert(typeof loadOptions.fetchFunc === 'function', () => 'Must pass a function that matches the signature of ' +
				'`fetch` (see ' +
				'https://developer.mozilla.org/en-US/docs/Web/API/Fetch_API)');
			this.fetch = loadOptions.fetchFunc;
		} else {
			this.fetch = env().platform.fetch;
		}
		assert(path != null && path.length > 0, () => 'URL path for http must not be null, undefined or ' +
			'empty.');
		if (Array.isArray(path)) {
			assert(path.length === 2, () => 'URL paths for http must have a length of 2, ' +
				`(actual length is ${path.length}).`);
		}
		this.path = path;
		if (loadOptions.requestInit != null &&
			loadOptions.requestInit.body != null) {
			throw new Error('requestInit is expected to have no pre-existing body, but has one.');
		}
		this.requestInit = loadOptions.requestInit || {};
		this.loadOptions = loadOptions;
	}

	async save(modelArtifacts) {
		if (modelArtifacts.modelTopology instanceof ArrayBuffer) {
			throw new Error('BrowserHTTPRequest.save() does not support saving model topology ' +
				'in binary formats yet.');
		}
		const init = Object.assign({method: this.DEFAULT_METHOD}, this.requestInit);
		init.body = new FormData();
		const weightsManifest = [{
			paths: ['./model.weights.bin'],
			weights: modelArtifacts.weightSpecs,
		}];
		const modelTopologyAndWeightManifest = getModelJSONForModelArtifacts(modelArtifacts, weightsManifest);
		init.body.append('model.json', new Blob([JSON.stringify(modelTopologyAndWeightManifest)], {type: JSON_TYPE}), 'model.json');
		if (modelArtifacts.weightData != null) {
			// TODO(mattsoulanille): Support saving models over 2GB that exceed
			// Chrome's ArrayBuffer size limit.
			const weightBuffer = CompositeArrayBuffer.join(modelArtifacts.weightData);
			init.body.append('model.weights.bin', new Blob([weightBuffer], {type: OCTET_STREAM_MIME_TYPE}), 'model.weights.bin');
		}
		const response = await this.fetch(this.path, init);
		if (response.ok) {
			return {
				modelArtifactsInfo: getModelArtifactsInfoForJSON(modelArtifacts),
				responses: [response],
			};
		} else {
			throw new Error(`BrowserHTTPRequest.save() failed due to HTTP response status ` +
				`${response.status}.`);
		}
	}

	async loadModelJSON() {
		const modelConfigRequest = await this.fetch(this.path, this.requestInit);
		if (!modelConfigRequest.ok) {
			throw new Error(`Request to ${this.path} failed with status code ` +
				`${modelConfigRequest.status}. Please verify this URL points to ` +
				`the model JSON of the model to load.`);
		}
		let modelJSON;
		try {
			modelJSON = await modelConfigRequest.json();
		} catch (e) {
			let message = `Failed to parse model JSON of response from ${this.path}.`;
			// TODO(nsthorat): Remove this after some time when we're comfortable that
			// .pb files are mostly gone.
			if (this.path.endsWith('.pb')) {
				message += ' Your path contains a .pb file extension. ' +
					'Support for .pb models have been removed in TensorFlow.js 1.0 ' +
					'in favor of .json models. You can re-convert your Python ' +
					'TensorFlow model using the TensorFlow.js 1.0 conversion scripts ' +
					'or you can convert your.pb models with the \'pb2json\'' +
					'NPM script in the tensorflow/tfjs-converter repository.';
			} else {
				message += ' Please make sure the server is serving valid ' +
					'JSON for this request.';
			}
			throw new Error(message);
		}
		// We do not allow both modelTopology and weightsManifest to be missing.
		const modelTopology = modelJSON.modelTopology;
		const weightsManifest = modelJSON.weightsManifest;
		if (modelTopology == null && weightsManifest == null) {
			throw new Error(`The JSON from HTTP path ${this.path} contains neither model ` +
				`topology or manifest for weights.`);
		}
		return modelJSON;
	}

	/**
	 * Load model artifacts via HTTP request(s).
	 *
	 * See the documentation to `tf.io.http` for details on the saved
	 * artifacts.
	 *
	 * @returns The loaded model artifacts (if loading succeeds).
	 */
	async load() {
		if (this.loadOptions.streamWeights) {
			return this.loadStream();
		}
		const modelJSON = await this.loadModelJSON();
		return getModelArtifactsForJSON(modelJSON, (weightsManifest) => this.loadWeights(weightsManifest));
	}

	async loadStream() {
		const modelJSON = await this.loadModelJSON();
		const fetchURLs = await this.getWeightUrls(modelJSON.weightsManifest);
		const weightSpecs = getWeightSpecs(modelJSON.weightsManifest);
		const stream = () => streamWeights(fetchURLs, this.loadOptions);
		return Object.assign(Object.assign({}, modelJSON), {weightSpecs, getWeightStream: stream});
	}

	async getWeightUrls(weightsManifest) {
		const weightPath = Array.isArray(this.path) ? this.path[1] : this.path;
		const [prefix, suffix] = parseUrl(weightPath);
		const pathPrefix = this.weightPathPrefix || prefix;
		const fetchURLs = [];
		const urlPromises = [];
		for (const weightsGroup of weightsManifest) {
			for (const path of weightsGroup.paths) {
				if (this.weightUrlConverter != null) {
					urlPromises.push(this.weightUrlConverter(path));
				} else {
					fetchURLs.push(pathPrefix + path + suffix);
				}
			}
		}
		if (this.weightUrlConverter) {
			fetchURLs.push(...await Promise.all(urlPromises));
		}
		return fetchURLs;
	}

	async loadWeights(weightsManifest) {
		const fetchURLs = await this.getWeightUrls(weightsManifest);
		const weightSpecs = getWeightSpecs(weightsManifest);
		const buffers = await loadWeightsAsArrayBuffer(fetchURLs, this.loadOptions);
		return [weightSpecs, buffers];
	}
}

HTTPRequest.URL_SCHEME_REGEX = /^https?:\/\//;

/**
 * Extract the prefix and suffix of the url, where the prefix is the path before
 * the last file, and suffix is the search params after the last file.
 * ```
 * const url = 'http://tfhub.dev/model/1/tensorflowjs_model.pb?tfjs-format=file'
 * [prefix, suffix] = parseUrl(url)
 * // prefix = 'http://tfhub.dev/model/1/'
 * // suffix = '?tfjs-format=file'
 * ```
 * @param url the model url to be parsed.
 */
function parseUrl(url) {
	const lastSlash = url.lastIndexOf('/');
	const lastSearchParam = url.lastIndexOf('?');
	const prefix = url.substring(0, lastSlash);
	const suffix = lastSearchParam > lastSlash ? url.substring(lastSearchParam) : '';
	return [prefix + '/', suffix];
}

function isHTTPScheme(url) {
	return url.match(HTTPRequest.URL_SCHEME_REGEX) != null;
}

const httpRouter = (url, loadOptions) => {
	if (typeof fetch === 'undefined' &&
		(loadOptions == null || loadOptions.fetchFunc == null)) {
		// `http` uses `fetch` or `node-fetch`, if one wants to use it in
		// an environment that is not the browser or node they have to setup a
		// global fetch polyfill.
		return null;
	} else {
		let isHTTP = true;
		if (Array.isArray(url)) {
			isHTTP = url.every(urlItem => isHTTPScheme(urlItem));
		} else {
			isHTTP = isHTTPScheme(url);
		}
		if (isHTTP) {
			return http(url, loadOptions);
		}
	}
	return null;
};
IORouterRegistry.registerSaveRouter(httpRouter);
IORouterRegistry.registerLoadRouter(httpRouter);

/**
 * Creates an IOHandler subtype that sends model artifacts to HTTP server.
 *
 * An HTTP request of the `multipart/form-data` mime type will be sent to the
 * `path` URL. The form data includes artifacts that represent the topology
 * and/or weights of the model. In the case of Keras-style `tf.Model`, two
 * blobs (files) exist in form-data:
 *   - A JSON file consisting of `modelTopology` and `weightsManifest`.
 *   - A binary weights file consisting of the concatenated weight values.
 * These files are in the same format as the one generated by
 * [tfjs_converter](https://js.tensorflow.org/tutorials/import-keras.html).
 *
 * The following code snippet exemplifies the client-side code that uses this
 * function:
 *
 * ```js
 * const model = tf.sequential();
 * model.add(
 *     tf.layers.dense({units: 1, inputShape: [100], activation: 'sigmoid'}));
 *
 * const saveResult = await model.save(tf.io.http(
 *     'http://model-server:5000/upload', {requestInit: {method: 'PUT'}}));
 * console.log(saveResult);
 * ```
 *
 * If the default `POST` method is to be used, without any custom parameters
 * such as headers, you can simply pass an HTTP or HTTPS URL to `model.save`:
 *
 * ```js
 * const saveResult = await model.save('http://model-server:5000/upload');
 * ```
 *
 * The following GitHub Gist
 * https://gist.github.com/dsmilkov/1b6046fd6132d7408d5257b0976f7864
 * implements a server based on [flask](https://github.com/pallets/flask) that
 * can receive the request. Upon receiving the model artifacts via the request,
 * this particular server reconstitutes instances of [Keras
 * Models](https://keras.io/models/model/) in memory.
 *
 *
 * @param path A URL path to the model.
 *   Can be an absolute HTTP path (e.g.,
 *   'http://localhost:8000/model-upload)') or a relative path (e.g.,
 *   './model-upload').
 * @param requestInit Request configurations to be used when sending
 *    HTTP request to server using `fetch`. It can contain fields such as
 *    `method`, `credentials`, `headers`, `mode`, etc. See
 *    https://developer.mozilla.org/en-US/docs/Web/API/Request/Request
 *    for more information. `requestInit` must not have a body, because the
 * body will be set by TensorFlow.js. File blobs representing the model
 * topology (filename: 'model.json') and the weights of the model (filename:
 * 'model.weights.bin') will be appended to the body. If `requestInit` has a
 * `body`, an Error will be thrown.
 * @param loadOptions Optional configuration for the loading. It includes the
 *   following fields:
 *   - weightPathPrefix Optional, this specifies the path prefix for weight
 *     files, by default this is calculated from the path param.
 *   - fetchFunc Optional, custom `fetch` function. E.g., in Node.js,
 *     the `fetch` from node-fetch can be used here.
 *   - onProgress Optional, progress callback function, fired periodically
 *     before the load is completed.
 * @returns An instance of `IOHandler`.
 *
 * @doc {
 *   heading: 'Models',
 *   subheading: 'Loading',
 *   namespace: 'io',
 *   ignoreCI: true
 * }
 */
function http(path, loadOptions) {
	return new HTTPRequest(path, loadOptions);
}

/**
 * Deprecated. Use `tf.io.http`.
 * @param path
 * @param loadOptions
 */
function browserHTTPRequest(path, loadOptions) {
	return http(path, loadOptions);
}

/**
 * @license
 * Copyright 2018 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
class PassthroughLoader {
	constructor(modelArtifacts) {
		this.modelArtifacts = modelArtifacts;
	}

	load() {
		return this.modelArtifacts;
	}
}

class PassthroughSaver {
	constructor(saveHandler) {
		this.saveHandler = saveHandler;
	}

	save(modelArtifacts) {
		return this.saveHandler(modelArtifacts);
	}
}

class PassthroughAsync {
	constructor(handler) {
		if (handler.load) {
			this.load = () => Promise.resolve(handler.load());
		}
		if (handler.save) {
			this.save = (modelArtifacts) => Promise.resolve(handler.save(modelArtifacts));
		}
	}
}

/**
 * Creates an IOHandler that loads model artifacts from memory.
 *
 * When used in conjunction with `tf.loadLayersModel`, an instance of
 * `tf.LayersModel` (Keras-style) can be constructed from the loaded artifacts.
 *
 * ```js
 * const model = await tf.loadLayersModel(tf.io.fromMemory(
 *     modelTopology, weightSpecs, weightData));
 * ```
 *
 * @param modelArtifacts a object containing model topology (i.e., parsed from
 *   the JSON format).
 * @param weightSpecs An array of `WeightsManifestEntry` objects describing the
 *   names, shapes, types, and quantization of the weight data. Optional.
 * @param weightData A single `ArrayBuffer` containing the weight data,
 *   concatenated in the order described by the weightSpecs. Optional.
 * @param trainingConfig Model training configuration. Optional.
 *
 * @returns A passthrough `IOHandler` that simply loads the provided data.
 */
function fromMemory(modelArtifacts, weightSpecs, weightData, trainingConfig) {
	const args = arguments;
	return new PassthroughAsync(fromMemorySync(...args));
}

/**
 * Creates an IOHandler that loads model artifacts from memory.
 *
 * When used in conjunction with `tf.loadLayersModel`, an instance of
 * `tf.LayersModel` (Keras-style) can be constructed from the loaded artifacts.
 *
 * ```js
 * const model = await tf.loadLayersModel(tf.io.fromMemory(
 *     modelTopology, weightSpecs, weightData));
 * ```
 *
 * @param modelArtifacts a object containing model topology (i.e., parsed from
 *   the JSON format).
 * @param weightSpecs An array of `WeightsManifestEntry` objects describing the
 *   names, shapes, types, and quantization of the weight data. Optional.
 * @param weightData A single `ArrayBuffer` containing the weight data,
 *   concatenated in the order described by the weightSpecs. Optional.
 * @param trainingConfig Model training configuration. Optional.
 *
 * @returns A passthrough `IOHandlerSync` that simply loads the provided data.
 */
function fromMemorySync(modelArtifacts, weightSpecs, weightData, trainingConfig) {
	if (arguments.length === 1) {
		const isModelArtifacts = modelArtifacts.modelTopology != null ||
			modelArtifacts.weightSpecs != null;
		if (isModelArtifacts) {
			return new PassthroughLoader(modelArtifacts);
		} else {
			// Legacy support: with only modelTopology.
			// TODO(cais): Remove this deprecated API.
			console.warn('Please call tf.io.fromMemory() with only one argument. ' +
				'The argument should be of type ModelArtifacts. ' +
				'The multi-argument signature of tf.io.fromMemory() has been ' +
				'deprecated and will be removed in a future release.');
			return new PassthroughLoader({modelTopology: modelArtifacts});
		}
	} else {
		// Legacy support.
		// TODO(cais): Remove this deprecated API.
		console.warn('Please call tf.io.fromMemory() with only one argument. ' +
			'The argument should be of type ModelArtifacts. ' +
			'The multi-argument signature of tf.io.fromMemory() has been ' +
			'deprecated and will be removed in a future release.');
		return new PassthroughLoader({
			modelTopology: modelArtifacts,
			weightSpecs,
			weightData,
			trainingConfig
		});
	}
}

/**
 * Creates an IOHandler that passes saved model artifacts to a callback.
 *
 * ```js
 * function handleSave(artifacts) {
 *   // ... do something with the artifacts ...
 *   return {modelArtifactsInfo: {...}, ...};
 * }
 *
 * const saveResult = model.save(tf.io.withSaveHandler(handleSave));
 * ```
 *
 * @param saveHandler A function that accepts a `ModelArtifacts` and returns a
 *     promise that resolves to a `SaveResult`.
 */
function withSaveHandler(saveHandler) {
	return new PassthroughSaver(saveHandler);
}

/**
 * Creates an IOHandlerSync that passes saved model artifacts to a callback.
 *
 * ```js
 * function handleSave(artifacts) {
 *   // ... do something with the artifacts ...
 *   return {modelArtifactsInfo: {...}, ...};
 * }
 *
 * const saveResult = model.save(tf.io.withSaveHandler(handleSave));
 * ```
 *
 * @param saveHandler A function that accepts a `ModelArtifacts` and returns a
 *     `SaveResult`.
 */
function withSaveHandlerSync(saveHandler) {
	return new PassthroughSaver(saveHandler);
}

/**
 * @license
 * Copyright 2018 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Classes and functions for model management across multiple storage mediums.
 *
 * Supported client actions:
 * - Listing models on all registered storage mediums.
 * - Remove model by URL from any registered storage mediums, by using URL
 *   string.
 * - Moving or copying model from one path to another in the same medium or from
 *   one medium to another, by using URL strings.
 */
const URL_SCHEME_SUFFIX = '://';

class ModelStoreManagerRegistry {
	constructor() {
		this.managers = {};
	}

	static getInstance() {
		if (ModelStoreManagerRegistry.instance == null) {
			ModelStoreManagerRegistry.instance = new ModelStoreManagerRegistry();
		}
		return ModelStoreManagerRegistry.instance;
	}

	/**
	 * Register a save-handler router.
	 *
	 * @param saveRouter A function that maps a URL-like string onto an instance
	 * of `IOHandler` with the `save` method defined or `null`.
	 */
	static registerManager(scheme, manager) {
		assert(scheme != null, () => 'scheme must not be undefined or null.');
		if (scheme.endsWith(URL_SCHEME_SUFFIX)) {
			scheme = scheme.slice(0, scheme.indexOf(URL_SCHEME_SUFFIX));
		}
		assert(scheme.length > 0, () => 'scheme must not be an empty string.');
		const registry = ModelStoreManagerRegistry.getInstance();
		assert(registry.managers[scheme] == null, () => `A model store manager is already registered for scheme '${scheme}'.`);
		registry.managers[scheme] = manager;
	}

	static getManager(scheme) {
		const manager = ModelStoreManagerRegistry.getInstance().managers[scheme];
		if (manager == null) {
			throw new Error(`Cannot find model manager for scheme '${scheme}'`);
		}
		return manager;
	}

	static getSchemes() {
		return Object.keys(ModelStoreManagerRegistry.getInstance().managers);
	}
}

/**
 * Helper method for parsing a URL string into a scheme and a path.
 *
 * @param url E.g., 'localstorage://my-model'
 * @returns A dictionary with two fields: scheme and path.
 *   Scheme: e.g., 'localstorage' in the example above.
 *   Path: e.g., 'my-model' in the example above.
 */
function parseURL(url) {
	if (url.indexOf(URL_SCHEME_SUFFIX) === -1) {
		throw new Error(`The url string provided does not contain a scheme. ` +
			`Supported schemes are: ` +
			`${ModelStoreManagerRegistry.getSchemes().join(',')}`);
	}
	return {
		scheme: url.split(URL_SCHEME_SUFFIX)[0],
		path: url.split(URL_SCHEME_SUFFIX)[1],
	};
}

async function cloneModelInternal(sourceURL, destURL, deleteSource = false) {
	assert(sourceURL !== destURL, () => `Old path and new path are the same: '${sourceURL}'`);
	const loadHandlers = IORouterRegistry.getLoadHandlers(sourceURL);
	assert(loadHandlers.length > 0, () => `Copying failed because no load handler is found for source URL ${sourceURL}.`);
	assert(loadHandlers.length < 2, () => `Copying failed because more than one (${loadHandlers.length}) ` +
		`load handlers for source URL ${sourceURL}.`);
	const loadHandler = loadHandlers[0];
	const saveHandlers = IORouterRegistry.getSaveHandlers(destURL);
	assert(saveHandlers.length > 0, () => `Copying failed because no save handler is found for destination ` +
		`URL ${destURL}.`);
	assert(saveHandlers.length < 2, () => `Copying failed because more than one (${loadHandlers.length}) ` +
		`save handlers for destination URL ${destURL}.`);
	const saveHandler = saveHandlers[0];
	const sourceScheme = parseURL(sourceURL).scheme;
	const sourcePath = parseURL(sourceURL).path;
	const sameMedium = sourceScheme === parseURL(sourceURL).scheme;
	const modelArtifacts = await loadHandler.load();
	// If moving within the same storage medium, remove the old model as soon as
	// the loading is done. Without doing this, it is possible that the combined
	// size of the two models will cause the cloning to fail.
	if (deleteSource && sameMedium) {
		await ModelStoreManagerRegistry.getManager(sourceScheme)
									   .removeModel(sourcePath);
	}
	const saveResult = await saveHandler.save(modelArtifacts);
	// If moving between mediums, the deletion is done after the save succeeds.
	// This guards against the case in which saving to the destination medium
	// fails.
	if (deleteSource && !sameMedium) {
		await ModelStoreManagerRegistry.getManager(sourceScheme)
									   .removeModel(sourcePath);
	}
	return saveResult.modelArtifactsInfo;
}

/**
 * List all models stored in registered storage mediums.
 *
 * For a web browser environment, the registered mediums are Local Storage and
 * IndexedDB.
 *
 * ```js
 * // First create and save a model.
 * const model = tf.sequential();
 * model.add(tf.layers.dense(
 *     {units: 1, inputShape: [10], activation: 'sigmoid'}));
 * await model.save('localstorage://demo/management/model1');
 *
 * // Then list existing models.
 * console.log(JSON.stringify(await tf.io.listModels()));
 *
 * // Delete the model.
 * await tf.io.removeModel('localstorage://demo/management/model1');
 *
 * // List models again.
 * console.log(JSON.stringify(await tf.io.listModels()));
 * ```
 *
 * @returns A `Promise` of a dictionary mapping URLs of existing models to
 * their model artifacts info. URLs include medium-specific schemes, e.g.,
 *   'indexeddb://my/model/1'. Model artifacts info include type of the
 * model's topology, byte sizes of the topology, weights, etc.
 *
 * @doc {
 *   heading: 'Models',
 *   subheading: 'Management',
 *   namespace: 'io',
 *   ignoreCI: true
 * }
 */
async function listModels() {
	const schemes = ModelStoreManagerRegistry.getSchemes();
	const out = {};
	for (const scheme of schemes) {
		const schemeOut = await ModelStoreManagerRegistry.getManager(scheme).listModels();
		for (const path in schemeOut) {
			const url = scheme + URL_SCHEME_SUFFIX + path;
			out[url] = schemeOut[path];
		}
	}
	return out;
}

/**
 * Remove a model specified by URL from a registered storage medium.
 *
 * ```js
 * // First create and save a model.
 * const model = tf.sequential();
 * model.add(tf.layers.dense(
 *     {units: 1, inputShape: [10], activation: 'sigmoid'}));
 * await model.save('localstorage://demo/management/model1');
 *
 * // Then list existing models.
 * console.log(JSON.stringify(await tf.io.listModels()));
 *
 * // Delete the model.
 * await tf.io.removeModel('localstorage://demo/management/model1');
 *
 * // List models again.
 * console.log(JSON.stringify(await tf.io.listModels()));
 * ```
 *
 * @param url A URL to a stored model, with a scheme prefix, e.g.,
 *   'localstorage://my-model-1', 'indexeddb://my/model/2'.
 * @returns ModelArtifactsInfo of the deleted model (if and only if deletion
 *   is successful).
 * @throws Error if deletion fails, e.g., if no model exists at `path`.
 *
 * @doc {
 *   heading: 'Models',
 *   subheading: 'Management',
 *   namespace: 'io',
 *   ignoreCI: true
 * }
 */
async function removeModel(url) {
	const schemeAndPath = parseURL(url);
	const manager = ModelStoreManagerRegistry.getManager(schemeAndPath.scheme);
	return manager.removeModel(schemeAndPath.path);
}

/**
 * Copy a model from one URL to another.
 *
 * This function supports:
 *
 * 1. Copying within a storage medium, e.g.,
 *    `tf.io.copyModel('localstorage://model-1', 'localstorage://model-2')`
 * 2. Copying between two storage mediums, e.g.,
 *    `tf.io.copyModel('localstorage://model-1', 'indexeddb://model-1')`
 *
 * ```js
 * // First create and save a model.
 * const model = tf.sequential();
 * model.add(tf.layers.dense(
 *     {units: 1, inputShape: [10], activation: 'sigmoid'}));
 * await model.save('localstorage://demo/management/model1');
 *
 * // Then list existing models.
 * console.log(JSON.stringify(await tf.io.listModels()));
 *
 * // Copy the model, from Local Storage to IndexedDB.
 * await tf.io.copyModel(
 *     'localstorage://demo/management/model1',
 *     'indexeddb://demo/management/model1');
 *
 * // List models again.
 * console.log(JSON.stringify(await tf.io.listModels()));
 *
 * // Remove both models.
 * await tf.io.removeModel('localstorage://demo/management/model1');
 * await tf.io.removeModel('indexeddb://demo/management/model1');
 * ```
 *
 * @param sourceURL Source URL of copying.
 * @param destURL Destination URL of copying.
 * @returns ModelArtifactsInfo of the copied model (if and only if copying
 *   is successful).
 * @throws Error if copying fails, e.g., if no model exists at `sourceURL`, or
 *   if `oldPath` and `newPath` are identical.
 *
 * @doc {
 *   heading: 'Models',
 *   subheading: 'Management',
 *   namespace: 'io',
 *   ignoreCI: true
 * }
 */
async function copyModel(sourceURL, destURL) {
	const deleteSource = false;
	return cloneModelInternal(sourceURL, destURL, deleteSource);
}

/**
 * Move a model from one URL to another.
 *
 * This function supports:
 *
 * 1. Moving within a storage medium, e.g.,
 *    `tf.io.moveModel('localstorage://model-1', 'localstorage://model-2')`
 * 2. Moving between two storage mediums, e.g.,
 *    `tf.io.moveModel('localstorage://model-1', 'indexeddb://model-1')`
 *
 * ```js
 * // First create and save a model.
 * const model = tf.sequential();
 * model.add(tf.layers.dense(
 *     {units: 1, inputShape: [10], activation: 'sigmoid'}));
 * await model.save('localstorage://demo/management/model1');
 *
 * // Then list existing models.
 * console.log(JSON.stringify(await tf.io.listModels()));
 *
 * // Move the model, from Local Storage to IndexedDB.
 * await tf.io.moveModel(
 *     'localstorage://demo/management/model1',
 *     'indexeddb://demo/management/model1');
 *
 * // List models again.
 * console.log(JSON.stringify(await tf.io.listModels()));
 *
 * // Remove the moved model.
 * await tf.io.removeModel('indexeddb://demo/management/model1');
 * ```
 *
 * @param sourceURL Source URL of moving.
 * @param destURL Destination URL of moving.
 * @returns ModelArtifactsInfo of the copied model (if and only if copying
 *   is successful).
 * @throws Error if moving fails, e.g., if no model exists at `sourceURL`, or
 *   if `oldPath` and `newPath` are identical.
 *
 * @doc {
 *   heading: 'Models',
 *   subheading: 'Management',
 *   namespace: 'io',
 *   ignoreCI: true
 * }
 */
async function moveModel(sourceURL, destURL) {
	const deleteSource = true;
	return cloneModelInternal(sourceURL, destURL, deleteSource);
}

/**
 * @license
 * Copyright 2018 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
// Importing local_storage and indexed_db is necessary for the routers to be
// registered.

var io = /*#__PURE__*/Object.freeze({
	__proto__: null,
	CompositeArrayBuffer: CompositeArrayBuffer,
	browserFiles: browserFiles,
	browserHTTPRequest: browserHTTPRequest,
	concatenateArrayBuffers: concatenateArrayBuffers,
	copyModel: copyModel,
	decodeWeights: decodeWeights,
	decodeWeightsStream: decodeWeightsStream,
	encodeWeights: encodeWeights,
	fromMemory: fromMemory,
	fromMemorySync: fromMemorySync,
	getLoadHandlers: getLoadHandlers,
	getModelArtifactsForJSON: getModelArtifactsForJSON,
	getModelArtifactsForJSONSync: getModelArtifactsForJSONSync,
	getModelArtifactsInfoForJSON: getModelArtifactsInfoForJSON,
	getSaveHandlers: getSaveHandlers,
	getWeightSpecs: getWeightSpecs,
	http: http,
	isHTTPScheme: isHTTPScheme,
	listModels: listModels,
	loadWeights: loadWeights,
	moveModel: moveModel,
	registerLoadRouter: registerLoadRouter,
	registerSaveRouter: registerSaveRouter,
	removeModel: removeModel,
	weightsLoaderFactory: weightsLoaderFactory,
	withSaveHandler: withSaveHandler,
	withSaveHandlerSync: withSaveHandlerSync
});

/**
 * @license
 * Copyright 2020 Google Inc. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Casts a `tf.Tensor` to a new dtype.
 *
 * ```js
 * const x = tf.tensor1d([1.5, 2.5, 3]);
 * tf.cast(x, 'int32').print();
 * ```
 * @param x The input tensor to be casted.
 * @param dtype The dtype to cast the input tensor to.
 *
 * @doc {heading: 'Tensors', subheading: 'Transformations'}
 */
function cast_(x, dtype) {
	const $x = convertToTensor(x, 'x', 'cast');
	// Sanity checks.
	if (!isValidDtype(dtype)) {
		throw new Error(`Failed to cast to unknown dtype ${dtype}`);
	}
	if (dtype === 'string' && $x.dtype !== 'string' ||
		dtype !== 'string' && $x.dtype === 'string') {
		throw new Error('Only strings can be casted to strings');
	}
	const inputs = {x: $x};
	const attrs = {dtype};
	return ENGINE.runKernel(Cast, inputs, attrs);
}

const cast$1 = /* @__PURE__ */ op({cast_});

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Computes the dot product of two matrices, A * B. These must be matrices.
 *
 * ```js
 * const a = tf.tensor2d([1, 2], [1, 2]);
 * const b = tf.tensor2d([1, 2, 3, 4], [2, 2]);
 *
 * a.matMul(b).print();  // or tf.matMul(a, b)
 * ```
 * @param a First matrix in dot product operation.
 * @param b Second matrix in dot product operation.
 * @param transposeA If true, `a` is transposed before multiplication.
 * @param transposeB If true, `b` is transposed before multiplication.
 *
 * @doc {heading: 'Operations', subheading: 'Matrices'}
 */
function matMul_(a, b, transposeA = false, transposeB = false) {
	let $a = convertToTensor(a, 'a', 'matMul');
	let $b = convertToTensor(b, 'b', 'matMul');
	[$a, $b] = makeTypesMatch($a, $b);
	const inputs = {a: $a, b: $b};
	const attrs = {transposeA, transposeB};
	return ENGINE.runKernel(BatchMatMul, inputs, attrs);
}

const matMul$1 = /* @__PURE__ */ op({matMul_});

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Creates a one-hot `tf.Tensor`. The locations represented by `indices` take
 * value `onValue` (defaults to 1), while all other locations take value
 * `offValue` (defaults to 0). If `indices` is rank `R`, the output has rank
 * `R+1` with the last axis of size `depth`.
 * `indices` used to encode prediction class must start from 0. For example,
 *  if you have 3 classes of data, class 1 should be encoded as 0, class 2
 *  should be 1, and class 3 should be 2.
 *
 * ```js
 * tf.oneHot(tf.tensor1d([0, 1], 'int32'), 3).print();
 * ```
 *
 * @param indices `tf.Tensor` of indices with dtype `int32`. Indices must
 * start from 0.
 * @param depth The depth of the one hot dimension.
 * @param onValue A number used to fill in the output when the index matches
 * the location.
 * @param offValue A number used to fill in the output when the index does
 *     not match the location.
 * @param dtype The dtype of the output tensor, default to 'int32'.
 *
 * @doc {heading: 'Tensors', subheading: 'Creation'}
 */
function oneHot_(indices, depth, onValue = 1, offValue = 0, dtype = 'int32') {
	if (depth < 2) {
		throw new Error(`Error in oneHot: depth must be >=2, but it is ${depth}`);
	}
	const $indices = convertToTensor(indices, 'indices', 'oneHot', 'int32');
	const inputs = {indices: $indices};
	const attrs = {dtype, depth, onValue, offValue};
	return ENGINE.runKernel(OneHot, inputs, attrs);
}

const oneHot = /* @__PURE__ */ op({oneHot_});

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Returns the imaginary part of a complex (or real) tensor.
 *
 * Given a tensor input, this operation returns a tensor of type float that is
 * the imaginary part of each element in input considered as a complex number.
 * If input is real, a tensor of all zeros is returned.
 *
 * ```js
 * const x = tf.complex([-2.25, 3.25], [4.75, 5.75]);
 * tf.imag(x).print();
 * ```
 *
 * @doc {heading: 'Tensors', subheading: 'Creation'}
 */
function imag_(input) {
	const $input = convertToTensor(input, 'input', 'imag');
	const inputs = {input: $input};
	return ENGINE.runKernel(Imag, inputs);
}

const imag$1 = /* @__PURE__ */ op({imag_});

/**
 * @license
 * Copyright 2018 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Computes `-1 * x` element-wise.
 *
 * ```js
 * const x = tf.tensor2d([1, 2, -2, 0], [2, 2]);
 *
 * x.neg().print();  // or tf.neg(x)
 * ```
 *
 * @param x The input tensor.
 *
 * @doc {heading: 'Operations', subheading: 'Basic math'}
 */
function neg_(x) {
	const $x = convertToTensor(x, 'x', 'neg');
	const inputs = {x: $x};
	return ENGINE.runKernel(Neg, inputs);
}

const neg$1 = /* @__PURE__ */ op({neg_});

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Returns the real part of a complex (or real) tensor.
 *
 * Given a tensor input, this operation returns a tensor of type float that is
 * the real part of each element in input considered as a complex number.
 *
 * If the input is real, it simply makes a clone.
 *
 * ```js
 * const x = tf.complex([-2.25, 3.25], [4.75, 5.75]);
 * tf.real(x).print();
 * ```
 *
 * @doc {heading: 'Tensors', subheading: 'Creation'}
 */
function real_(input) {
	const $input = convertToTensor(input, 'input', 'real');
	const inputs = {input: $input};
	return ENGINE.runKernel(Real, inputs);
}

const real$1 = /* @__PURE__ */ op({real_});

/**
 * @license
 * Copyright 2018 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Transposes the `tf.Tensor`. Permutes the dimensions according to `perm`.
 *
 * The returned `tf.Tensor`'s dimension `i` will correspond to the input
 * dimension `perm[i]`. If `perm` is not given, it is set to `[n-1...0]`,
 * where `n` is the rank of the input `tf.Tensor`. Hence by default, this
 * operation performs a regular matrix transpose on 2-D input `tf.Tensor`s.
 *
 * ```js
 * const a = tf.tensor2d([1, 2, 3, 4, 5, 6], [2, 3]);
 *
 * a.transpose().print();  // or tf.transpose(a)
 * ```
 *
 * @param x The tensor to transpose.
 * @param perm The permutation of the dimensions of a.
 * @param conjugate Will conjugate complex input if true.
 *
 * @doc {heading: 'Operations', subheading: 'Matrices'}
 */
function transpose_(x, perm, conjugate) {
	const $x = convertToTensor(x, 'x', 'transpose');
	if (perm == null) {
		perm = $x.shape.map((s, i) => i).reverse();
	}
	assert($x.rank === perm.length, () => `Error in transpose: rank of input ${$x.rank} ` +
		`must match length of perm ${perm}.`);
	perm.forEach(axis => {
		assert(axis >= 0 && axis < $x.rank, () => `All entries in 'perm' must be between 0 and ${$x.rank - 1}` +
			` but got ${perm}`);
	});
	if ($x.rank <= 1) {
		return $x.clone();
	}
	const inputs = {x: $x};
	const attrs = {perm};
	if ($x.dtype === 'complex64') {
		return tidy(() => {
			let $real = real$1($x);
			let $imag = imag$1($x);
			$real = ENGINE.runKernel(Transpose, {x: $real}, attrs);
			$imag = ENGINE.runKernel(Transpose, {x: $imag}, attrs);
			if (conjugate) {
				$imag = neg$1($imag);
			}
			return complex$1($real, $imag);
		});
	}
	return ENGINE.runKernel(Transpose, inputs, attrs);
}

const transpose$1 = /* @__PURE__ */ op({transpose_});

/**
 * @license
 * Copyright 2018 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Computes the confusion matrix from true labels and predicted labels.
 *
 * ```js
 * const labels = tf.tensor1d([0, 1, 2, 1, 0], 'int32');
 * const predictions = tf.tensor1d([0, 2, 2, 1, 0], 'int32');
 * const numClasses = 3;
 * const out = tf.math.confusionMatrix(labels, predictions, numClasses);
 * out.print();
 * // Expected output matrix:
 * // [[2, 0, 0],
 * //  [0, 1, 1],
 * //  [0, 0, 1]]
 * ```
 *
 * @param labels The target labels, assumed to be 0-based integers
 *   for the classes. The shape is `[numExamples]`, where
 *   `numExamples` is the number of examples included.
 * @param predictions The predicted classes, assumed to be
 *   0-based integers for the classes. Must have the same shape as `labels`.
 * @param numClasses Number of all classes, as an integer.
 *   Its value must be larger than the largest element in `labels` and
 *   `predictions`.
 * @returns The confusion matrix as a int32-type 2D tensor. The value at
 *   row `r` and column `c` is the number of times examples of actual class
 *   `r` were predicted as class `c`.
 *
 * @doc {heading: 'Operations', subheading: 'Evaluation'}
 */
function confusionMatrix_(labels, predictions, numClasses) {
	const $labels = convertToTensor(labels, 'labels', 'confusionMatrix');
	const $predictions = convertToTensor(predictions, 'predictions', 'confusionMatrix');
	assert(numClasses == null || numClasses > 0 && Number.isInteger(numClasses), () => `If provided, numClasses must be a positive integer, ` +
		`but got ${numClasses}`);
	assert($labels.rank === 1, () => `Expected the rank of labels to be 1, but got ${$labels.rank}`);
	assert($predictions.rank === 1, () => `Expected the rank of predictions to be 1, ` +
		`but got ${$predictions.rank}`);
	assert($labels.shape[0] === $predictions.shape[0], () => `Mismatch in the number of examples: ` +
		`${$labels.shape[0]} vs. ${$predictions.shape[0]}. ` +
		`Labels and predictions should have the same number of elements.`);
	assert(numClasses > 0 && Number.isInteger(numClasses), () => `numClasses is required to be a positive integer, but got ` +
		`${numClasses}`);
	// TODO(cais): In the future, if oneHot supports tensors inputs for
	//   `numClasses`, `confusionMatrix` can make `numClasses` optional.
	const oneHotLabels = oneHot(cast$1($labels, 'int32'), numClasses);
	const oneHotPredictions = oneHot(cast$1($predictions, 'int32'), numClasses);
	const oneHotLabelsT = transpose$1(oneHotLabels);
	const product = matMul$1(oneHotLabelsT, oneHotPredictions);
	return cast$1(product, 'int32');
}

const confusionMatrix = /* @__PURE__ */ op({confusionMatrix_});

/**
 * @license
 * Copyright 2018 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Exports under the tf.math.* namespace.
 */

var math = /*#__PURE__*/Object.freeze({
	__proto__: null,
	confusionMatrix: confusionMatrix
});

/**
 * @license
 * Copyright 2017 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Returns the dimensions in the input shape that are broadcasted to
 * produce the provided output shape.
 *
 * The returned dimensions are 0-indexed and sorted. An example:
 * inShape = [4, 1, 3]
 * outShape = [5, 4, 3, 3]
 * result = [1]. Dimension 1 (2nd dimension of input) gets broadcasted 1 => 3.
 */
function getBroadcastDims(inShape, outShape) {
	const inRank = inShape.length;
	const dims = [];
	for (let i = 0; i < inRank; i++) {
		const dim = inRank - 1 - i;
		const a = inShape[dim] || 1;
		const b = outShape[outShape.length - 1 - i] || 1;
		if (b > 1 && a === 1) {
			dims.unshift(dim);
		}
	}
	return dims;
}

/**
 * Returns the axes in the output space that should be reduced to produce
 * the input space.
 */
function getReductionAxes(inShape, outShape) {
	const result = [];
	for (let i = 0; i < outShape.length; i++) {
		const inDim = inShape[inShape.length - i - 1];
		const outAxis = outShape.length - i - 1;
		const outDim = outShape[outAxis];
		if (inDim == null || (inDim === 1 && outDim > 1)) {
			result.unshift(outAxis);
		}
	}
	return result;
}

function assertAndGetBroadcastShape(shapeA, shapeB) {
	const l = Math.max(shapeA.length, shapeB.length);
	const result = new Array(l);
	for (let i = 0; i < l; i++) {
		let a = shapeA[shapeA.length - i - 1];
		if (a == null) {
			a = 1;
		}
		let b = shapeB[shapeB.length - i - 1];
		if (b == null) {
			b = 1;
		}
		if (a === 1) {
			result[l - i - 1] = b;
		} else if (b === 1) {
			result[l - i - 1] = a;
		} else if (a !== b) {
			const errMsg = `Operands could not be broadcast together with shapes ` +
				`${shapeA} and ${shapeB}.`;
			throw Error(errMsg);
		} else {
			result[l - i - 1] = a;
		}
	}
	return result;
}

var broadcast_util = /*#__PURE__*/Object.freeze({
	__proto__: null,
	assertAndGetBroadcastShape: assertAndGetBroadcastShape,
	getBroadcastDims: getBroadcastDims,
	getReductionAxes: getReductionAxes
});

/**
 * @license
 * Copyright 2018 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Creates rank-3 `tf.Tensor` with the provided values, shape and dtype.
 *
 * The same functionality can be achieved with `tf.tensor`, but in general
 * we recommend using `tf.tensor3d` as it makes the code more readable.
 *
 *  ```js
 * // Pass a nested array.
 * tf.tensor3d([[[1], [2]], [[3], [4]]]).print();
 * ```
 * ```js
 * // Pass a flat array and specify a shape.
 * tf.tensor3d([1, 2, 3, 4], [2, 2, 1]).print();
 * ```
 *
 * @param values The values of the tensor. Can be nested array of numbers,
 *     or a flat array, or a `TypedArray`.
 * @param shape The shape of the tensor. If not provided,  it is inferred from
 *     `values`.
 * @param dtype The data type.
 *
 * @doc {heading: 'Tensors', subheading: 'Creation'}
 */
function tensor3d(values, shape, dtype) {
	assertNonNull(values);
	if (shape != null && shape.length !== 3) {
		throw new Error('tensor3d() requires shape to have three numbers');
	}
	const inferredShape = inferShape(values, dtype);
	if (inferredShape.length !== 3 && inferredShape.length !== 1) {
		throw new Error('tensor3d() requires values to be number[][][] or flat/TypedArray');
	}
	if (inferredShape.length === 1 && shape == null) {
		throw new Error('tensor3d() requires shape to be provided when `values` ' +
			'are a flat array');
	}
	return makeTensor(values, shape, inferredShape, dtype);
}

/**
 * @license
 * Copyright 2019 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
let fromPixels2DContext;
let hasToPixelsWarned = false;

/**
 * Creates a `tf.Tensor` from an image.
 *
 * ```js
 * const image = new ImageData(1, 1);
 * image.data[0] = 100;
 * image.data[1] = 150;
 * image.data[2] = 200;
 * image.data[3] = 255;
 *
 * tf.browser.fromPixels(image).print();
 * ```
 *
 * @param pixels The input image to construct the tensor from. The
 * supported image types are all 4-channel. You can also pass in an image
 * object with following attributes:
 * `{data: Uint8Array; width: number; height: number}`
 * @param numChannels The number of channels of the output tensor. A
 * numChannels value less than 4 allows you to ignore channels. Defaults to
 * 3 (ignores alpha channel of input image).
 *
 * @returns A Tensor3D with the shape `[height, width, numChannels]`.
 *
 * Note: fromPixels can be lossy in some cases, same image may result in
 * slightly different tensor values, if rendered by different rendering
 * engines. This means that results from different browsers, or even same
 * browser with CPU and GPU rendering engines can be different. See discussion
 * in details:
 * https://github.com/tensorflow/tfjs/issues/5482
 *
 * @doc {heading: 'Browser', namespace: 'browser', ignoreCI: true}
 */
function fromPixels_(pixels, numChannels = 3) {
	// Sanity checks.
	if (numChannels > 4) {
		throw new Error('Cannot construct Tensor with more than 4 channels from pixels.');
	}
	if (pixels == null) {
		throw new Error('pixels passed to tf.browser.fromPixels() can not be null');
	}
	let isPixelData = false;
	let isImageData = false;
	let isVideo = false;
	let isImage = false;
	let isCanvasLike = false;
	let isImageBitmap = false;
	if (pixels.data instanceof Uint8Array) {
		isPixelData = true;
	} else if (typeof (ImageData) !== 'undefined' && pixels instanceof ImageData) {
		isImageData = true;
	} else if (typeof (HTMLVideoElement) !== 'undefined' &&
		pixels instanceof HTMLVideoElement) {
		isVideo = true;
	} else if (typeof (HTMLImageElement) !== 'undefined' &&
		pixels instanceof HTMLImageElement) {
		isImage = true;
		// tslint:disable-next-line: no-any
	} else if (pixels.getContext != null) {
		isCanvasLike = true;
	} else if (typeof (ImageBitmap) !== 'undefined' && pixels instanceof ImageBitmap) {
		isImageBitmap = true;
	} else {
		throw new Error('pixels passed to tf.browser.fromPixels() must be either an ' +
			`HTMLVideoElement, HTMLImageElement, HTMLCanvasElement, ImageData ` +
			`in browser, or OffscreenCanvas, ImageData in webworker` +
			` or {data: Uint32Array, width: number, height: number}, ` +
			`but was ${pixels.constructor.name}`);
	}
	// If the current backend has 'FromPixels' registered, it has a more
	// efficient way of handling pixel uploads, so we call that.
	const kernel = getKernel(FromPixels, ENGINE.backendName);
	if (kernel != null) {
		const inputs = {pixels};
		const attrs = {numChannels};
		return ENGINE.runKernel(FromPixels, inputs, attrs);
	}
	const [width, height] = isVideo ?
		[
			pixels.videoWidth,
			pixels.videoHeight
		] :
		[pixels.width, pixels.height];
	let vals;
	if (isCanvasLike) {
		vals =
			// tslint:disable-next-line:no-any
			pixels.getContext('2d').getImageData(0, 0, width, height).data;
	} else if (isImageData || isPixelData) {
		vals = pixels.data;
	} else if (isImage || isVideo || isImageBitmap) {
		if (fromPixels2DContext == null) {
			if (typeof document === 'undefined') {
				if (typeof OffscreenCanvas !== 'undefined' &&
					typeof OffscreenCanvasRenderingContext2D !== 'undefined') {
					// @ts-ignore
					fromPixels2DContext = new OffscreenCanvas(1, 1).getContext('2d');
				} else {
					throw new Error('Cannot parse input in current context. ' +
						'Reason: OffscreenCanvas Context2D rendering is not supported.');
				}
			} else {
				fromPixels2DContext = document.createElement('canvas').getContext('2d', {willReadFrequently: true});
			}
		}
		fromPixels2DContext.canvas.width = width;
		fromPixels2DContext.canvas.height = height;
		fromPixels2DContext.drawImage(pixels, 0, 0, width, height);
		vals = fromPixels2DContext.getImageData(0, 0, width, height).data;
	}
	let values;
	if (numChannels === 4) {
		values = new Int32Array(vals);
	} else {
		const numPixels = width * height;
		values = new Int32Array(numPixels * numChannels);
		for (let i = 0; i < numPixels; i++) {
			for (let channel = 0; channel < numChannels; ++channel) {
				values[i * numChannels + channel] = vals[i * 4 + channel];
			}
		}
	}
	const outShape = [height, width, numChannels];
	return tensor3d(values, outShape, 'int32');
}

// Helper functions for |fromPixelsAsync| to check whether the input can
// be wrapped into imageBitmap.
function isPixelData(pixels) {
	return (pixels != null) && (pixels.data instanceof Uint8Array);
}

function isImageBitmapFullySupported() {
	return typeof window !== 'undefined' &&
		typeof (ImageBitmap) !== 'undefined' &&
		window.hasOwnProperty('createImageBitmap');
}

function isNonEmptyPixels(pixels) {
	return pixels != null && pixels.width !== 0 && pixels.height !== 0;
}

function canWrapPixelsToImageBitmap(pixels) {
	return isImageBitmapFullySupported() && !(pixels instanceof ImageBitmap) &&
		isNonEmptyPixels(pixels) && !isPixelData(pixels);
}

/**
 * Creates a `tf.Tensor` from an image in async way.
 *
 * ```js
 * const image = new ImageData(1, 1);
 * image.data[0] = 100;
 * image.data[1] = 150;
 * image.data[2] = 200;
 * image.data[3] = 255;
 *
 * (await tf.browser.fromPixelsAsync(image)).print();
 * ```
 * This API is the async version of fromPixels. The API will first
 * check |WRAP_TO_IMAGEBITMAP| flag, and try to wrap the input to
 * imageBitmap if the flag is set to true.
 *
 * @param pixels The input image to construct the tensor from. The
 * supported image types are all 4-channel. You can also pass in an image
 * object with following attributes:
 * `{data: Uint8Array; width: number; height: number}`
 * @param numChannels The number of channels of the output tensor. A
 * numChannels value less than 4 allows you to ignore channels. Defaults to
 * 3 (ignores alpha channel of input image).
 *
 * @doc {heading: 'Browser', namespace: 'browser', ignoreCI: true}
 */
async function fromPixelsAsync(pixels, numChannels = 3) {
	let inputs = null;
	// Check whether the backend needs to wrap |pixels| to imageBitmap and
	// whether |pixels| can be wrapped to imageBitmap.
	if (env().getBool('WRAP_TO_IMAGEBITMAP') &&
		canWrapPixelsToImageBitmap(pixels)) {
		// Force the imageBitmap creation to not do any premultiply alpha
		// ops.
		let imageBitmap;
		try {
			// wrap in try-catch block, because createImageBitmap may not work
			// properly in some browsers, e.g.
			// https://bugzilla.mozilla.org/show_bug.cgi?id=1335594
			// tslint:disable-next-line: no-any
			imageBitmap = await createImageBitmap(pixels, {premultiplyAlpha: 'none'});
		} catch (e) {
			imageBitmap = null;
		}
		// createImageBitmap will clip the source size.
		// In some cases, the input will have larger size than its content.
		// E.g. new Image(10, 10) but with 1 x 1 content. Using
		// createImageBitmap will clip the size from 10 x 10 to 1 x 1, which
		// is not correct. We should avoid wrapping such resouce to
		// imageBitmap.
		if (imageBitmap != null && imageBitmap.width === pixels.width &&
			imageBitmap.height === pixels.height) {
			inputs = imageBitmap;
		} else {
			inputs = pixels;
		}
	} else {
		inputs = pixels;
	}
	return fromPixels_(inputs, numChannels);
}

function validateImgTensor(img) {
	if (img.rank !== 2 && img.rank !== 3) {
		throw new Error(`toPixels only supports rank 2 or 3 tensors, got rank ${img.rank}.`);
	}
	const depth = img.rank === 2 ? 1 : img.shape[2];
	if (depth > 4 || depth === 2) {
		throw new Error(`toPixels only supports depth of size ` +
			`1, 3 or 4 but got ${depth}`);
	}
	if (img.dtype !== 'float32' && img.dtype !== 'int32') {
		throw new Error(`Unsupported type for toPixels: ${img.dtype}.` +
			` Please use float32 or int32 tensors.`);
	}
}

function validateImageOptions(imageOptions) {
	const alpha = (imageOptions === null || imageOptions === void 0 ? void 0 : imageOptions.alpha) || 1;
	if (alpha > 1 || alpha < 0) {
		throw new Error(`Alpha value ${alpha} is suppoed to be in range [0 - 1].`);
	}
}

/**
 * Draws a `tf.Tensor` of pixel values to a byte array or optionally a
 * canvas.
 *
 * When the dtype of the input is 'float32', we assume values in the range
 * [0-1]. Otherwise, when input is 'int32', we assume values in the range
 * [0-255].
 *
 * Returns a promise that resolves when the canvas has been drawn to.
 *
 * @param img A rank-2 tensor with shape `[height, width]`, or a rank-3 tensor
 * of shape `[height, width, numChannels]`. If rank-2, draws grayscale. If
 * rank-3, must have depth of 1, 3 or 4. When depth of 1, draws
 * grayscale. When depth of 3, we draw with the first three components of
 * the depth dimension corresponding to r, g, b and alpha = 1. When depth of
 * 4, all four components of the depth dimension correspond to r, g, b, a.
 * @param canvas The canvas to draw to.
 *
 * @doc {heading: 'Browser', namespace: 'browser'}
 */
async function toPixels(img, canvas) {
	let $img = convertToTensor(img, 'img', 'toPixels');
	if (!(img instanceof Tensor)) {
		// Assume int32 if user passed a native array.
		const originalImgTensor = $img;
		$img = cast$1(originalImgTensor, 'int32');
		originalImgTensor.dispose();
	}
	validateImgTensor($img);
	const [height, width] = $img.shape.slice(0, 2);
	const depth = $img.rank === 2 ? 1 : $img.shape[2];
	const data = await $img.data();
	const multiplier = $img.dtype === 'float32' ? 255 : 1;
	const bytes = new Uint8ClampedArray(width * height * 4);
	for (let i = 0; i < height * width; ++i) {
		const rgba = [0, 0, 0, 255];
		for (let d = 0; d < depth; d++) {
			const value = data[i * depth + d];
			if ($img.dtype === 'float32') {
				if (value < 0 || value > 1) {
					throw new Error(`Tensor values for a float32 Tensor must be in the ` +
						`range [0 - 1] but encountered ${value}.`);
				}
			} else if ($img.dtype === 'int32') {
				if (value < 0 || value > 255) {
					throw new Error(`Tensor values for a int32 Tensor must be in the ` +
						`range [0 - 255] but encountered ${value}.`);
				}
			}
			if (depth === 1) {
				rgba[0] = value * multiplier;
				rgba[1] = value * multiplier;
				rgba[2] = value * multiplier;
			} else {
				rgba[d] = value * multiplier;
			}
		}
		const j = i * 4;
		bytes[j + 0] = Math.round(rgba[0]);
		bytes[j + 1] = Math.round(rgba[1]);
		bytes[j + 2] = Math.round(rgba[2]);
		bytes[j + 3] = Math.round(rgba[3]);
	}
	if (canvas != null) {
		if (!hasToPixelsWarned) {
			const kernel = getKernel(Draw, ENGINE.backendName);
			if (kernel != null) {
				console.warn('tf.browser.toPixels is not efficient to draw tensor on canvas. ' +
					'Please try tf.browser.draw instead.');
				hasToPixelsWarned = true;
			}
		}
		canvas.width = width;
		canvas.height = height;
		const ctx = canvas.getContext('2d');
		const imageData = new ImageData(bytes, width, height);
		ctx.putImageData(imageData, 0, 0);
	}
	if ($img !== img) {
		$img.dispose();
	}
	return bytes;
}

/**
 * Draws a `tf.Tensor` to a canvas.
 *
 * When the dtype of the input is 'float32', we assume values in the range
 * [0-1]. Otherwise, when input is 'int32', we assume values in the range
 * [0-255].
 *
 * @param image The tensor to draw on the canvas. Must match one of
 * these shapes:
 *   - Rank-2 with shape `[height, width`]: Drawn as grayscale.
 *   - Rank-3 with shape `[height, width, 1]`: Drawn as grayscale.
 *   - Rank-3 with shape `[height, width, 3]`: Drawn as RGB with alpha set in
 *     `imageOptions` (defaults to 1, which is opaque).
 *   - Rank-3 with shape `[height, width, 4]`: Drawn as RGBA.
 * @param canvas The canvas to draw to.
 * @param options The configuration arguments for image to be drawn and the
 *     canvas to draw to.
 *
 * @doc {heading: 'Browser', namespace: 'browser'}
 */
function draw(image, canvas, options) {
	let $img = convertToTensor(image, 'img', 'draw');
	if (!(image instanceof Tensor)) {
		// Assume int32 if user passed a native array.
		const originalImgTensor = $img;
		$img = cast$1(originalImgTensor, 'int32');
		originalImgTensor.dispose();
	}
	validateImgTensor($img);
	validateImageOptions(options === null || options === void 0 ? void 0 : options.imageOptions);
	const inputs = {image: $img};
	const attrs = {canvas, options};
	ENGINE.runKernel(Draw, inputs, attrs);
}

const fromPixels = /* @__PURE__ */ op({fromPixels_});

var browser = /*#__PURE__*/Object.freeze({
	__proto__: null,
	draw: draw,
	fromPixels: fromPixels,
	fromPixelsAsync: fromPixelsAsync,
	toPixels: toPixels
});

/**
 * Validate gather nd inputs.
 *
 * @param tensor The tensor contains the source values.
 * @param indices The tensor contains the indices to slice the source.
 *
 * @returns [resultShape, numUpdates, sliceSize, strides]
 */
function prepareAndValidate(tensor, indices) {
	const tensorRank = tensor.shape.length;
	const indicesRank = indices.shape.length;
	if (tensorRank < 1) {
		throw new Error('tf.gatherND() expects the input to be rank 1 or higher,' +
			` but the rank was ${tensorRank}.`);
	}
	if (indicesRank < 1) {
		throw new Error('tf.gatherND() expects the indices to be rank 1 or higher,' +
			` but the rank was ${indicesRank}.`);
	}
	if (indices.dtype !== 'int32') {
		throw new Error('tf.gatherND() expects the indices to be int32 type,' +
			` but the dtype was ${indices.dtype}.`);
	}
	if (indices.shape[indicesRank - 1] > tensorRank) {
		throw new Error('index innermost dimension length must be <= tensor rank; saw: ' +
			`${indices.shape[indicesRank - 1]} vs. ${tensorRank}`);
	}
	if (sizeFromShape(tensor.shape) === 0) {
		throw new Error('Requested more than 0 entries, but input is empty.' +
			` Input shape: ${tensor.shape}.`);
	}
	const indicesShape = indices.shape;
	const sliceRank = indicesShape[indicesShape.length - 1];
	// The result shape is
	//   indices.shape[:-1] + params.shape[indices.shape[-1]:]
	let nResult = 1;
	for (let i = 0; i < indicesShape.length - 1; ++i) {
		nResult *= indicesShape[i];
	}
	const inputShape = tensor.shape;
	const resultShape = indicesShape.slice();
	resultShape.pop();
	let sliceSize = 1;
	for (let i = sliceRank; i < tensorRank; ++i) {
		sliceSize *= inputShape[i];
		resultShape.push(inputShape[i]);
	}
	const strides = [...computeStrides(tensor.shape).map(stride => stride / sliceSize),
		1].slice(0, sliceRank);
	return [resultShape, nResult, sliceSize, strides];
}

var gather_nd_util = /*#__PURE__*/Object.freeze({
	__proto__: null,
	prepareAndValidate: prepareAndValidate
});

/**
 * Check whether updates.shape = indices.shape[:batchDim] +
 * shape[sliceDim:]
 *
 * @param x The input tensor.
 */
function validateUpdateShape(shape, indices, updates) {
	const sliceDim = (indices.rank > 1) ? indices.shape[indices.rank - 1] : 1;
	const batchDim = (indices.rank > 1) ? indices.rank - 1 : 1;
	const shapeError = 'Must have updates.shape = indices.shape[:batchDim] + ' +
		`shape[sliceDim:], got updates.shape: ${updates.shape}` +
		`, indices.shape: ${indices.shape}, shape: ${shape}` +
		`, sliceDim: ${sliceDim}, and batchDim: ${batchDim}.`;
	if (updates.rank < batchDim) {
		throw new Error(shapeError + ` update.rank < ${batchDim}. `);
	}
	if (shape.length < sliceDim + (updates.rank - batchDim)) {
		throw new Error(shapeError +
			` Output shape length < ${sliceDim + (updates.rank - batchDim)}`);
	}
	if (updates.rank !== batchDim + shape.length - sliceDim) {
		throw new Error(shapeError + ` update.rank != ${batchDim + shape.length - sliceDim}`);
	}
	for (let d = 0; d < batchDim; ++d) {
		if (updates.shape[d] !== indices.shape[d]) {
			throw new Error(shapeError +
				` updates.shape[${d}] (${updates.shape[d]}) != indices.shape[${d}] (${indices.shape[d]}).`);
		}
	}
	for (let d = 0; d < updates.rank - batchDim; ++d) {
		if (updates.shape[d + batchDim] !== shape[d + sliceDim]) {
			throw new Error(shapeError +
				` updates.shape[${d + batchDim}] (${updates.shape[d + batchDim]}) != shape[${d + batchDim}] (${shape[d + batchDim]})`);
		}
	}
}

/**
 * Validate scatter nd inputs.
 *
 * @param update The tensor contains the update values.
 * @param indices The tensor contains the indices for the update values.
 * @param shape The shape of the output tensor.
 */
function validateInput$1(updates, indices, shape) {
	if (indices.rank < 1) {
		throw new Error('tf.scatterND() expects the indices to be rank 1 or higher,' +
			` but the rank was ${indices.rank}.`);
	}
	if (updates.rank < 1) {
		throw new Error('tf.scatterND() expects the updates to be rank 1 or higher,' +
			` but the rank was ${updates.rank}.`);
	}
	if (indices.dtype !== 'int32') {
		throw new Error(`The dtype of 'indices' should be int32, but got dtype: ${indices.dtype}`);
	}
	if (shape.length < 1) {
		throw new Error(`Output rank must be greater or equal to 1, but got shape: ${shape}`);
	}
	if (shape.length === 0) {
		if (indices.size === 0) {
			throw new Error(`Indices specified for empty output. indices shape: ${indices.shape}`);
		}
		if (updates.size === 0) {
			throw new Error(`Updates specified for empty output. updates shape: ${updates.shape}`);
		}
	}
	validateUpdateShape(shape, indices, updates);
}

/**
 * Calculate the shape information for the output.
 *
 * @param update The tensor contains the update values.
 * @param indices The tensor contains the indices for the update values.
 * @param shape The shape of the output tensor.
 *
 * @returns ScatterShapeInfo
 */
function calculateShapes(updates, indices, shape) {
	// Calculate the number of dimensions in indices
	const indicesRank = indices.shape.length;
	const sliceRank = (indicesRank > 1) ? indices.shape[indicesRank - 1] : 1;
	// Calculate the number of elements that make up each slice of our updated
	// tensor. This allows us to work with flattened tensors and copy over whole
	// slices at a time.
	const totalNd = shape.length;
	let sliceSize = 1;
	for (let i = sliceRank; i < totalNd; ++i) {
		sliceSize *= shape[i];
	}
	const safeSliceDim = (sliceRank < 1) ? 1 : sliceRank;
	const numUpdates = sizeFromShape(indices.shape) / safeSliceDim;
	const strides = [...computeStrides(shape.slice(0, sliceRank)), 1];
	const outputSize = sizeFromShape(shape);
	return {sliceRank, numUpdates, sliceSize, strides, outputSize};
}

var scatter_nd_util = /*#__PURE__*/Object.freeze({
	__proto__: null,
	calculateShapes: calculateShapes,
	validateInput: validateInput$1,
	validateUpdateShape: validateUpdateShape
});

/**
 * @license
 * Copyright 2021 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
const NEW_AXIS = -2;
const SHRINK_AXIS = -1;

function assertParamsValid(input, begin, size) {
	const inputRank = input.shape.length;
	assert(inputRank === begin.length, () => `Error in slice${inputRank}D: Length of begin ${begin} must ` +
		`match the rank of the array (${inputRank}).`);
	assert(inputRank === size.length, () => `Error in slice${inputRank}D: Length of size ${size} must ` +
		`match the rank of the array (${inputRank}).`);
	for (let i = 0; i < inputRank; ++i) {
		assert(begin[i] + size[i] <= input.shape[i], () => `Error in slice${inputRank}D: begin[${i}] + size[${i}] ` +
			`(${begin[i] + size[i]}) would overflow input.shape[${i}] (${input.shape[i]})`);
	}
}

/** Converts a binary mask to an array of axes. Used in stridedSlice(). */
function maskToAxes(mask) {
	const axes = [];
	let axis = 0;
	while (mask > 0) {
		if (mask & 1) {
			axes.push(axis);
		}
		mask /= 2;
		axis++;
	}
	return axes;
}

/** Computes the output shape given the strided slice params. */
function computeOutShape$2(begin, end, strides) {
	const size = [];
	for (let axis = 0; axis < begin.length; axis++) {
		size[axis] = Math.ceil((end[axis] - begin[axis]) / strides[axis]);
	}
	return size;
}

// Creates full selection at the elided dimensions. If the dimension matches
// the ellipsis mask, override the current stride value. Otherwise, insert.
function stridesWithElidedDims(strides, ellipsisInsertionIndex, numElidedAxes, inputShape) {
	const newStrides = [...strides];
	for (let i = newStrides.length; i < inputShape.length; i++) {
		newStrides.push(1);
	}
	for (let i = 0; i < numElidedAxes; i++) {
		if (i === 0) {
			newStrides[ellipsisInsertionIndex] = 1;
		} else {
			newStrides.splice(ellipsisInsertionIndex, 0 /* num elements to delete */, 1 /* element to add */);
			newStrides.pop();
		}
	}
	return newStrides;
}

function unnormalizeAxis(ellipsisInsertionIndex, numElidedAxes, normalizedAxis) {
	if (normalizedAxis <= ellipsisInsertionIndex) {
		return normalizedAxis;
	}
	return normalizedAxis - (numElidedAxes - 1);
}

function getElidedAxes(numElidedAxes, ellipsisInsertionIndex) {
	const elidedAxes = [];
	for (let i = 0; i < numElidedAxes; i++) {
		elidedAxes.push(ellipsisInsertionIndex + i);
	}
	return elidedAxes;
}

// Normalize the start, end and strides.
function getNormalizedAxes(inputShape, ellipsisAxes, numInterpolatedAxes, begin, end, strides, beginMask, endMask, ellipsisMask) {
	const inputRank = inputShape.length;
	let normalizedBegin = new Array(inputRank), normalizedEnd = new Array(inputRank), normalizedStrides = new Array(inputRank);
	if (ellipsisAxes.length && numInterpolatedAxes > 0) {
		const fullIndex = ellipsisAxes[0];
		// The ellipsis applies to the masked index as well as any dimensions
		// that are interpolated.
		const numElidedAxes = numInterpolatedAxes + 1;
		normalizedBegin = startIndicesWithElidedDims(beginMask, fullIndex, numElidedAxes, begin, inputShape);
		normalizedEnd = stopIndicesWithElidedDims(endMask, fullIndex, numElidedAxes, end, inputShape);
		normalizedStrides =
			stridesWithElidedDims(strides, fullIndex, numElidedAxes, inputShape);
	} else {
		for (let axis = 0; axis < inputRank; axis++) {
			normalizedBegin[axis] = startForAxis(beginMask, begin, strides, inputShape, axis, ellipsisMask);
			normalizedEnd[axis] =
				stopForAxis(endMask, end, strides, inputShape, axis, ellipsisMask);
			normalizedStrides[axis] = stridesForAxis(strides, axis, ellipsisMask);
		}
	}
	return {
		begin: normalizedBegin,
		end: normalizedEnd,
		strides: normalizedStrides
	};
}

// Creates full selection at the elided dimensions. If the dimension matches
// the ellipsis mask, override the current start value. Otherwise, insert.
function startIndicesWithElidedDims(beginMask, ellipsisInsertionIndex, numElidedAxes, originalBegin, inputShape) {
	const newIndices = [...inputShape];
	const elidedAxes = getElidedAxes(numElidedAxes, ellipsisInsertionIndex);
	for (let axis = 0; axis < newIndices.length; axis++) {
		if (elidedAxes.indexOf(axis) > -1) {
			newIndices[axis] = 0;
		} else {
			const originalAxis = unnormalizeAxis(ellipsisInsertionIndex, numElidedAxes, axis);
			let originalValue = originalBegin[originalAxis];
			if (beginMask & 1 << originalAxis) {
				originalValue = 0;
			}
			newIndices[axis] = originalValue;
		}
	}
	return newIndices;
}

// Creates full selection at the elided dimensions. If the dimension matches
// the ellipsis mask, override the current stop value. Otherwise, insert.
function stopIndicesWithElidedDims(endMask, ellipsisInsertionIndex, numElidedAxes, originalEnd, inputShape) {
	const newIndices = [...inputShape];
	const elidedAxes = getElidedAxes(numElidedAxes, ellipsisInsertionIndex);
	for (let axis = 0; axis < newIndices.length; axis++) {
		if (elidedAxes.indexOf(axis) > -1) {
			newIndices[axis] = Number.MAX_SAFE_INTEGER;
		} else {
			const originalAxis = unnormalizeAxis(ellipsisInsertionIndex, numElidedAxes, axis);
			let originalValue = originalEnd[originalAxis];
			if (endMask & 1 << originalAxis) {
				originalValue = Number.MAX_SAFE_INTEGER;
			}
			newIndices[axis] = originalValue;
		}
	}
	for (let i = 0; i < newIndices.length; i++) {
		// Handle negative indices
		const axisSize = inputShape[i];
		if (newIndices[i] < 0) {
			newIndices[i] += axisSize;
		}
		newIndices[i] = clamp(0, newIndices[i], inputShape[i]);
	}
	return newIndices;
}

function stridesForAxis(strides, axis, ellipsisMask) {
	let stride = strides[axis];
	if (ellipsisMask & (1 << axis) || stride == null) {
		stride = 1;
	}
	return stride;
}

function startForAxis(beginMask, startIndices, strides, inputShape, axis, ellipsisMask) {
	// Begin with the specified index
	let start = startIndices[axis];
	const stride = strides[axis] || 1;
	// Check the axis bit from right of masked axes, or the begin index is not set
	// for the axis.
	if (beginMask & 1 << axis || ellipsisMask & 1 << axis || start == null) {
		if (stride > 0) {
			// Forward iteration - use the first element. These values will get
			// clamped below (Note: We could have set them to 0 and axis_size-1, but
			// use lowest() and max() to maintain symmetry with StopForAxis())
			start = Number.MIN_SAFE_INTEGER;
		} else {
			// Backward iteration - use the last element.
			start = Number.MAX_SAFE_INTEGER;
		}
	}
	// Handle negative indices
	const axisSize = inputShape[axis];
	if (start < 0) {
		start += axisSize;
	}
	// Clamping
	start = clamp(0, start, axisSize - 1);
	return start;
}

function stopForAxis(endMask, stopIndices, strides, inputShape, axis, ellipsisMask) {
	// Begin with the specified index
	let stop = stopIndices[axis];
	const stride = strides[axis] || 1;
	// Check the axis bit from right of masked axes, or if the stop index is not
	// set for this axis.
	if (endMask & (1 << axis) || ellipsisMask & (1 << axis) || stop == null) {
		if (stride > 0) {
			// Forward iteration - use the last element. These values will get
			// clamped below
			stop = Number.MAX_SAFE_INTEGER;
		} else {
			// Backward iteration - use the first element.
			stop = Number.MIN_SAFE_INTEGER;
		}
	}
	// Handle negative indices
	const axisSize = inputShape[axis];
	if (stop < 0) {
		stop += axisSize;
	}
	// Clamping
	// Because the end index points one past the last element, we need slightly
	// different clamping ranges depending on the direction.
	if (stride > 0) {
		// Forward iteration
		stop = clamp(0, stop, axisSize);
	} else {
		// Backward iteration
		stop = clamp(-1, stop, axisSize - 1);
	}
	return stop;
}

/**
 * Returns true if the slice occupies a continous set of elements in the
 * 'flat' space.
 */
function isSliceContinous(shape, begin, size) {
	// Index of the first axis that has size > 1.
	let firstNonOneAxis = size.length;
	for (let i = 0; i < size.length; i++) {
		if (size[i] > 1) {
			firstNonOneAxis = i;
			break;
		}
	}
	for (let i = firstNonOneAxis + 1; i < size.length; i++) {
		if (begin[i] > 0 || size[i] !== shape[i]) {
			return false;
		}
	}
	return true;
}

function computeFlatOffset(begin, strides) {
	let flatOffset = begin.length > 0 ? begin[begin.length - 1] : 1;
	for (let i = 0; i < begin.length - 1; i++) {
		flatOffset += begin[i] * strides[i];
	}
	return flatOffset;
}

function parseSliceParams(x, begin, size) {
	// The following logic allows for more ergonomic calls.
	let begin_;
	const xRank = x.shape.length;
	if (typeof begin === 'number') {
		begin_ = [begin, ...new Array(xRank - 1).fill(0)];
	} else if (begin.length < xRank) {
		begin_ = begin.concat(new Array(xRank - begin.length).fill(0));
	} else {
		begin_ = begin.slice();
	}
	begin_.forEach(d => {
		assert(d !== -1, () => 'slice() does not support negative begin indexing.');
	});
	let size_;
	if (size == null) {
		size_ = new Array(xRank).fill(-1);
	} else if (typeof size === 'number') {
		size_ = [size, ...new Array(xRank - 1).fill(-1)];
	} else if (size.length < xRank) {
		size_ = size.concat(new Array(xRank - size.length).fill(-1));
	} else {
		size_ = size;
	}
	size_ = size_.map((d, i) => {
		if (d >= 0) {
			return d;
		} else {
			assert(d === -1, () => `Negative size values should be exactly -1 but got ` +
				`${d} for the slice() size at index ${i}.`);
			return x.shape[i] - begin_[i];
		}
	});
	return [begin_, size_];
}

// Convert the slicing specification from a sparse representation to a dense
// representation. This means that all ellipses and newaxis are expanded out.
function sliceInfo(xShape, begin, end, strides, beginMask, endMask, ellipsisMask, newAxisMask, shrinkAxisMask) {
	let stridesNonNull;
	if (strides == null) {
		stridesNonNull = new Array(begin.length);
		stridesNonNull.fill(1);
	} else {
		stridesNonNull = strides;
	}
	// Only one non-zero bit is allowed in ellipsisMask, which means ellipsisMask
	// is a power of 2. Use bit compares to ensure ellipsisMask is 0 or a power
	// of 2. When i is a power of 2, i & (i - 1) is always 0.
	// Also ref:
	// https://stackoverflow.com/questions/600293/how-to-check-if-a-number-is-a-power-of-2
	if (ellipsisMask != null && (ellipsisMask & (ellipsisMask - 1)) !== 0) {
		throw new Error('Multiple ellipses in slice is not allowed.');
	}
	// Step 1: Account for ellipsis and new axis.
	// Check for ellipsis and count how many non-newaxis there are after.
	let ellipsisSeen = false;
	const sparseSpec = {
		dims: stridesNonNull.length,
		numAddAxisAfterEllipsis: 0,
		begin: begin.slice(),
		end: end.slice(),
		strides: stridesNonNull.slice(),
		beginMask,
		endMask,
		ellipsisMask,
		newAxisMask,
		shrinkAxisMask
	};
	for (let i = 0; i < sparseSpec.dims; i++) {
		if (ellipsisSeen && ((1 << i) & newAxisMask) !== 0) {
			sparseSpec.numAddAxisAfterEllipsis++;
		}
		if ((1 << i) & ellipsisMask) {
			ellipsisSeen = true;
		}
	}
	// If no ellipsis insert one at the end.
	if (!ellipsisSeen) {
		sparseSpec.ellipsisMask |= (1 << sparseSpec.dims);
		sparseSpec.dims++; // this effects loop iteration below
	}
	// Step 2: Make a sparse spec into a full index spec.
	//
	// The sparse spec deos not correspond to the number of dimensions.
	// Make a dense spec that cooresponds to the number of dimensions.
	//
	// For example suppose foo[...,3:] on foo.shape = [2, 2, 3] then we need to
	// produce the missing beginMask for the first two dimensions i.e. from
	// beginMaskSpec = 0, endMaskSpec = 2, we achieve beginMask = 6 (110),
	// endMask = 7 (111).
	const denseSpec = {
		dims: xShape.length,
		beginMask: 0,
		endMask: 0,
		beginValid: false,
		endValid: false
	};
	buildDenseSpec(sparseSpec, denseSpec);
	// Step 3: Make implicit ranges (non-zero beginMasks and endMasks) explicit
	// and bounds check.
	let isIdentity = true;
	let sliceDim0 = true;
	let isSimpleSlice = true;
	const processingShape = [];
	const finalShape = [];
	for (let i = 0; i < xShape.length; ++i) {
		if (denseSpec.strides[i] === 0) {
			throw Error(`strides[${i}] must be non-zero`);
		}
		const shrinkI = !!(denseSpec.shrinkAxisMask & (1 << i));
		const dimI = xShape[i];
		if (dimI === -1) {
			processingShape.push(shrinkI ? 1 : -1);
			continue;
		}
		const masks = [denseSpec.beginMask & (1 << i), denseSpec.endMask & (1 << i)];
		const validRange = [
			denseSpec.strides[i] > 0 ? 0 : -1,
			denseSpec.strides[i] > 0 ? dimI : dimI - 1
		];
		if (shrinkI && denseSpec.strides[i] <= 0) {
			throw Error('only stride 1 allowed on non-range indexing.');
		}
		isSimpleSlice = isSimpleSlice && (denseSpec.strides[i] === 1);
		const beginAndEndMasked = !!((denseSpec.beginMask & (1 << i)) && (denseSpec.endMask & (1 << i)));
		if (denseSpec.beginValid && denseSpec.endValid) {
			if (shrinkI) {
				// If we are shrinking, the end index is now possibly incorrect. In
				// particular foo[-1] produces sparseBegin = -1, sparseEnd = 0.
				// and canonical puts these to n-1 and 0, which implies a degenerate
				// interval. Fortunately, it is now safe to re-create end as begin + 1.
				const xFwd = denseSpec.begin[i] < 0 ? dimI + denseSpec.begin[i] :
					denseSpec.begin[i];
				denseSpec.begin[i] = xFwd;
				denseSpec.end[i] = denseSpec.begin[i] + 1;
				if (xFwd < 0 || xFwd >= dimI) {
					throw Error(`slice index ${denseSpec.begin[i]} of dimension ${i} out of bounds.`);
				}
			} else {
				denseSpec.begin[i] = canonical(denseSpec.begin[i], 0, denseSpec.strides[i], dimI, masks, validRange);
				denseSpec.end[i] = canonical(denseSpec.end[i], 1, denseSpec.strides[i], dimI, masks, validRange);
			}
			// Update optimization values
			const takeAllInDimension = denseSpec.strides[i] === 1 &&
				denseSpec.begin[i] === 0 && denseSpec.end[i] === dimI;
			isIdentity = isIdentity && takeAllInDimension;
			sliceDim0 = sliceDim0 &&
				((i === 0 && denseSpec.strides[i] === 1) || takeAllInDimension);
		} else {
			isIdentity =
				isIdentity && ((denseSpec.strides[i] === 1) && beginAndEndMasked);
			sliceDim0 = sliceDim0 &&
				((i === 0 && denseSpec.strides[i] === 1) || beginAndEndMasked);
		}
		// Compute the processing shape (the intermediate Eigen will produce)
		let intervalLength;
		let knownInterval = false;
		if (denseSpec.beginValid && denseSpec.endValid) {
			intervalLength = denseSpec.end[i] - denseSpec.begin[i];
			knownInterval = true;
		} else if (shrinkI) {
			// The dimension is still known as 1 for the processingShape, but will be
			// discarded for the final shape.
			intervalLength = 1;
			knownInterval = true;
		} else if (beginAndEndMasked) {
			// Even if we don't have values for begin or end, we do know that this
			// dimension covers the whole interval. If we have shape information for
			// this dimension, that tells us the interval length.
			if (dimI >= 0) {
				if (denseSpec.strides[i] < 0) {
					intervalLength = -dimI;
				} else {
					intervalLength = dimI;
				}
				knownInterval = true;
			}
		}
		if (knownInterval) {
			let sizeI;
			// Hold zero if the interval is degenerate, otherwise account for
			// remainder
			if (intervalLength === 0 ||
				((intervalLength < 0) !== (denseSpec.strides[i] < 0))) {
				sizeI = 0;
			} else {
				sizeI = Math.trunc(intervalLength / denseSpec.strides[i]) +
					(intervalLength % denseSpec.strides[i] !== 0 ? 1 : 0);
			}
			processingShape.push(sizeI);
		} else {
			processingShape.push(-1);
		}
	}
	// Step 4: Compute the final shape
	//
	// newAxis will increase dimension by 1 (with a one-size dimension)
	// slices like foo[3, ...] will reduce dimension by 1.
	// This cannot be done earlier, because it depends on Step 3.
	for (let denseDim = 0; denseDim < denseSpec.finalShapeGatherIndices.length; ++denseDim) {
		const gatherIndex = denseSpec.finalShapeGatherIndices[denseDim];
		if (gatherIndex >= 0) {
			finalShape.push(processingShape[gatherIndex]);
		} else if (gatherIndex === NEW_AXIS) {
			finalShape.push(1);
		}
	}
	const finalShapeSparse = finalShape.filter((dim, i) => denseSpec.finalShapeGatherIndices[i] !== NEW_AXIS);
	return {
		finalShapeSparse,
		finalShape,
		isIdentity,
		sliceDim0,
		isSimpleSlice,
		begin: denseSpec.begin,
		end: denseSpec.end,
		strides: denseSpec.strides
	};
}

function buildDenseSpec(sparse, dense) {
	dense.beginMask = 0;
	dense.endMask = 0;
	dense.shrinkAxisMask = 0;
	let fullIndex = 0;
	dense.beginValid = sparse.begin != null;
	dense.endValid = sparse.end != null;
	dense.begin = new Array(dense.dims);
	dense.end = new Array(dense.dims);
	dense.strides = new Array(dense.dims);
	dense.finalShapeGatherIndices = [];
	dense.finalShapeGatherIndicesSparse = [];
	dense.inputShapeGatherIndicesSparse = new Array(dense.dims);
	for (let i = 0; i < sparse.dims; i++) {
		if ((1 << i) & sparse.ellipsisMask) {
			// Only the bit that has ellipsis will fall in this condition.
			// Expand the ellipsis into the appropriate indices
			// Note: this only works because we guaranteed one ellipsis.
			const nextIndex = Math.min(dense.dims - (sparse.dims - i) + 1 + sparse.numAddAxisAfterEllipsis, dense.dims);
			for (; fullIndex < nextIndex; fullIndex++) {
				// newAxis aren't real axis so you have to skip.
				dense.begin[fullIndex] = 0;
				dense.end[fullIndex] = 0;
				dense.strides[fullIndex] = 1;
				dense.beginMask |= (1 << fullIndex);
				dense.endMask |= (1 << fullIndex);
				dense.finalShapeGatherIndices.push(fullIndex);
				dense.finalShapeGatherIndicesSparse.push(-1);
				dense.inputShapeGatherIndicesSparse[fullIndex] = i;
			}
		} else if ((1 << i) & sparse.newAxisMask) {
			// Only the bit that has newAxis will fall in this condition.
			dense.finalShapeGatherIndices.push(NEW_AXIS);
			dense.finalShapeGatherIndicesSparse.push(-1);
		} else {
			if (fullIndex === dense.begin.length) {
				throw Error(`Index out of range using input dim ${fullIndex}; input ` +
					`has only ${dense.dims} dims, ${dense.begin.length}.`);
			}
			// Gather slicing spec into appropriate index.
			if (sparse.begin != null) {
				dense.begin[fullIndex] = sparse.begin[i];
			}
			if (sparse.end != null) {
				dense.end[fullIndex] = sparse.end[i];
			}
			dense.strides[fullIndex] = sparse.strides[i];
			if (sparse.beginMask & (1 << i)) {
				dense.beginMask |= (1 << fullIndex);
			}
			if (sparse.endMask & (1 << i)) {
				dense.endMask |= (1 << fullIndex);
			}
			// If shrink, record where to get the dimensionality from (i.e. newAxis)
			// creates a fake 1 size dimension. Also remember shrink axis (now in
			// dense form) so we can ignore dense.end below.
			if (sparse.shrinkAxisMask & (1 << i)) {
				dense.finalShapeGatherIndices.push(SHRINK_AXIS);
				dense.finalShapeGatherIndicesSparse.push(-1);
				dense.shrinkAxisMask |= (1 << fullIndex);
			} else {
				dense.finalShapeGatherIndices.push(fullIndex);
				// Remember that where in the sparse shape the dense dim comes from.
				dense.finalShapeGatherIndicesSparse.push(i);
			}
			dense.inputShapeGatherIndicesSparse[fullIndex] = i;
			fullIndex++;
		}
	}
}

function canonical(x, c, strideI, dimI, masks, validRange) {
	if (masks[c]) {
		return strideI > 0 ? validRange[c] : validRange[(c + 1) & 1];
	} else {
		const xFwd = x < 0 ? dimI + x : x; // make negative indices positive
		return xFwd < validRange[0] ? validRange[0] :
			xFwd > validRange[1] ? validRange[1] : xFwd;
	}
}

var slice_util = /*#__PURE__*/Object.freeze({
	__proto__: null,
	assertParamsValid: assertParamsValid,
	computeFlatOffset: computeFlatOffset,
	computeOutShape: computeOutShape$2,
	getNormalizedAxes: getNormalizedAxes,
	isSliceContinous: isSliceContinous,
	maskToAxes: maskToAxes,
	parseSliceParams: parseSliceParams,
	sliceInfo: sliceInfo,
	startForAxis: startForAxis,
	startIndicesWithElidedDims: startIndicesWithElidedDims,
	stopForAxis: stopForAxis,
	stopIndicesWithElidedDims: stopIndicesWithElidedDims,
	stridesForAxis: stridesForAxis,
	stridesWithElidedDims: stridesWithElidedDims
});

/**
 * @license
 * Copyright 2018 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Maps to mapping between the custom object and its name.
 *
 * After registering a custom class, these two maps will add key-value pairs
 * for the class object and the registered name.
 *
 * Therefore we can get the relative registered name by calling
 * getRegisteredName() function.
 *
 * For example:
 * GLOBAL_CUSTOM_OBJECT: {key=registeredName: value=corresponding
 * CustomObjectClass}
 *
 * GLOBAL_CUSTOM_NAMES: {key=CustomObjectClass: value=corresponding
 * registeredName}
 *
 */
const GLOBAL_CUSTOM_OBJECT = new Map();
const GLOBAL_CUSTOM_NAMES = new Map();

/**
 * Serializable defines the serialization contract.
 *
 * TFJS requires serializable classes to return their className when asked
 * to avoid issues with minification.
 */
class Serializable {
	/**
	 * Return the class name for this class to use in serialization contexts.
	 *
	 * Generally speaking this will be the same thing that constructor.name
	 * would have returned.  However, the class name needs to be robust
	 * against minification for serialization/deserialization to work properly.
	 *
	 * There's also places such as initializers.VarianceScaling, where
	 * implementation details between different languages led to different
	 * class hierarchies and a non-leaf node is used for serialization purposes.
	 */
	getClassName() {
		return this.constructor
			.className;
	}

	/**
	 * Creates an instance of T from a ConfigDict.
	 *
	 * This works for most descendants of serializable.  A few need to
	 * provide special handling.
	 * @param cls A Constructor for the class to instantiate.
	 * @param config The Configuration for the object.
	 */
	/** @nocollapse */
	static fromConfig(cls, config) {
		return new cls(config);
	}
}

/**
 * Maps string keys to class constructors.
 *
 * Used during (de)serialization from the cross-language JSON format, which
 * requires the class name in the serialization format matches the class
 * names as used in Python, should it exist.
 */
class SerializationMap {
	constructor() {
		this.classNameMap = {};
	}

	/**
	 * Returns the singleton instance of the map.
	 */
	static getMap() {
		if (SerializationMap.instance == null) {
			SerializationMap.instance = new SerializationMap();
		}
		return SerializationMap.instance;
	}

	/**
	 * Registers the class as serializable.
	 */
	static register(cls) {
		SerializationMap.getMap().classNameMap[cls.className] =
			[cls, cls.fromConfig];
	}
}

/**
 * Register a class with the serialization map of TensorFlow.js.
 *
 * This is often used for registering custom Layers, so they can be
 * serialized and deserialized.
 *
 * Example 1. Register the class without package name and specified name.
 *
 * ```js
 * class MyCustomLayer extends tf.layers.Layer {
 *   static className = 'MyCustomLayer';
 *
 *   constructor(config) {
 *     super(config);
 *   }
 * }
 * tf.serialization.registerClass(MyCustomLayer);
 * console.log(tf.serialization.GLOBALCUSTOMOBJECT.get("Custom>MyCustomLayer"));
 * console.log(tf.serialization.GLOBALCUSTOMNAMES.get(MyCustomLayer));
 * ```
 *
 * Example 2. Register the class with package name: "Package" and specified
 * name: "MyLayer".
 * ```js
 * class MyCustomLayer extends tf.layers.Layer {
 *   static className = 'MyCustomLayer';
 *
 *   constructor(config) {
 *     super(config);
 *   }
 * }
 * tf.serialization.registerClass(MyCustomLayer, "Package", "MyLayer");
 * console.log(tf.serialization.GLOBALCUSTOMOBJECT.get("Package>MyLayer"));
 * console.log(tf.serialization.GLOBALCUSTOMNAMES.get(MyCustomLayer));
 * ```
 *
 * Example 3. Register the class with specified name: "MyLayer".
 * ```js
 * class MyCustomLayer extends tf.layers.Layer {
 *   static className = 'MyCustomLayer';
 *
 *   constructor(config) {
 *     super(config);
 *   }
 * }
 * tf.serialization.registerClass(MyCustomLayer, undefined, "MyLayer");
 * console.log(tf.serialization.GLOBALCUSTOMOBJECT.get("Custom>MyLayer"));
 * console.log(tf.serialization.GLOBALCUSTOMNAMES.get(MyCustomLayer));
 * ```
 *
 * Example 4. Register the class with specified package name: "Package".
 * ```js
 * class MyCustomLayer extends tf.layers.Layer {
 *   static className = 'MyCustomLayer';
 *
 *   constructor(config) {
 *     super(config);
 *   }
 * }
 * tf.serialization.registerClass(MyCustomLayer, "Package");
 * console.log(tf.serialization.GLOBALCUSTOMOBJECT
 * .get("Package>MyCustomLayer"));
 * console.log(tf.serialization.GLOBALCUSTOMNAMES
 * .get(MyCustomLayer));
 * ```
 *
 * @param cls The class to be registered. It must have a public static member
 *   called `className` defined and the value must be a non-empty string.
 * @param pkg The package name that this class belongs to. This used to define
 *     the key in GlobalCustomObject. If not defined, it defaults to `Custom`.
 * @param name The name that user specified. It defaults to the actual name of
 *     the class as specified by its static `className` property.
 * @doc {heading: 'Models', subheading: 'Serialization', ignoreCI: true}
 */
function registerClass(cls, pkg, name) {
	assert(cls.className != null, () => `Class being registered does not have the static className ` +
		`property defined.`);
	assert(typeof cls.className === 'string', () => `className is required to be a string, but got type ` +
		typeof cls.className);
	assert(cls.className.length > 0, () => `Class being registered has an empty-string as its className, ` +
		`which is disallowed.`);
	if (typeof pkg === 'undefined') {
		pkg = 'Custom';
	}
	if (typeof name === 'undefined') {
		name = cls.className;
	}
	const className = name;
	const registerName = pkg + '>' + className;
	SerializationMap.register(cls);
	GLOBAL_CUSTOM_OBJECT.set(registerName, cls);
	GLOBAL_CUSTOM_NAMES.set(cls, registerName);
	return cls;
}

/**
 * Get the registered name of a class. If the class has not been registered,
 * return the class name.
 *
 * @param cls The class we want to get register name for. It must have a public
 *     static member called `className` defined.
 * @returns registered name or class name.
 */
function getRegisteredName(cls) {
	if (GLOBAL_CUSTOM_NAMES.has(cls)) {
		return GLOBAL_CUSTOM_NAMES.get(cls);
	} else {
		return cls.className;
	}
}

var serialization = /*#__PURE__*/Object.freeze({
	__proto__: null,
	Serializable: Serializable,
	SerializationMap: SerializationMap,
	getRegisteredName: getRegisteredName,
	registerClass: registerClass
});

/**
 * @license
 * Copyright 2017 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
const TEST_EPSILON_FLOAT32 = 1e-3;
const TEST_EPSILON_FLOAT16 = 1e-1;

function expectArraysClose(actual, expected, epsilon) {
	if (epsilon == null) {
		epsilon = testEpsilon();
	}
	return expectArraysPredicate(actual, expected, (a, b) => areClose(a, b, epsilon));
}

function testEpsilon() {
	return ENGINE.backend.floatPrecision() === 32 ? TEST_EPSILON_FLOAT32 :
		TEST_EPSILON_FLOAT16;
}

function expectArraysPredicate(actual, expected, predicate) {
	let checkClassType = true;
	if (isTypedArray(actual) || isTypedArray(expected)) {
		checkClassType = false;
	}
	if (isTypedArray(actual) && isTypedArray(expected)) {
		checkClassType = true;
	}
	if (checkClassType) {
		const aType = actual.constructor.name;
		const bType = expected.constructor.name;
		if (aType !== bType) {
			throw new Error(`Arrays are of different type. Actual: ${aType}. ` +
				`Expected: ${bType}`);
		}
	}
	if (Array.isArray(actual) && Array.isArray(expected)) {
		const actualShape = inferShape(actual);
		const expectedShape = inferShape(expected);
		if (!arraysEqual(actualShape, expectedShape)) {
			throw new Error(`Arrays have different shapes. ` +
				`Actual: [${actualShape}]. Expected: [${expectedShape}]`);
		}
	}
	const actualFlat = isTypedArray(actual) ? actual : flatten(actual);
	const expectedFlat = isTypedArray(expected) ?
		expected :
		flatten(expected);
	if (actualFlat.length !== expectedFlat.length) {
		throw new Error(`Arrays have different lengths actual: ${actualFlat.length} vs ` +
			`expected: ${expectedFlat.length}.\n` +
			`Actual:   ${actualFlat}.\n` +
			`Expected: ${expectedFlat}.`);
	}
	for (let i = 0; i < expectedFlat.length; ++i) {
		const a = actualFlat[i];
		const e = expectedFlat[i];
		if (!predicate(a, e)) {
			throw new Error(`Arrays differ: actual[${i}] = ${a}, expected[${i}] = ${e}.\n` +
				`Actual:   ${actualFlat}.\n` +
				`Expected: ${expectedFlat}.`);
		}
	}
	if (typeof expect !== 'undefined') {
		expect().nothing();
	}
}

function expectPromiseToFail(fn, done) {
	fn().then(() => done.fail(), () => done());
	if (typeof expect !== 'undefined') {
		expect().nothing();
	}
}

function expectArraysEqual(actual, expected) {
	const exp = typeof expected === 'string' || typeof expected === 'number' ||
	typeof expected === 'boolean' ?
		[expected] :
		expected;
	if (isString(actual) || isString(actual[0]) ||
		isString(expected) || isString(expected[0])) {
		// tslint:disable-next-line: triple-equals
		return expectArraysPredicate(actual, exp, (a, b) => a == b);
	}
	return expectArraysPredicate(actual, expected, (a, b) => areClose(a, b, 0));
}

function expectNumbersClose(a, e, epsilon) {
	if (epsilon == null) {
		epsilon = testEpsilon();
	}
	if (!areClose(a, e, epsilon)) {
		throw new Error(`Numbers differ: actual === ${a}, expected === ${e}`);
	}
	if (typeof expect !== 'undefined') {
		expect().nothing();
	}
}

function areClose(a, e, epsilon) {
	if (!isFinite(a) && !isFinite(e)) {
		return true;
	}
	if (isNaN(a) || isNaN(e) || Math.abs(a - e) > epsilon) {
		return false;
	}
	return true;
}

function expectValuesInRange(actual, low, high) {
	for (let i = 0; i < actual.length; i++) {
		if (actual[i] < low || actual[i] > high) {
			throw new Error(`Value out of range:${actual[i]} low: ${low}, high: ${high}`);
		}
	}
}

function expectArrayBuffersEqual(actual, expected) {
	// Safari does not like comparing ArrayBuffers directly. Wrapping in
	// a Float32Array solves this issue.
	const actualArray = new Float32Array(actual);
	const expectedArray = new Float32Array(expected);
	if (actualArray.length !== expectedArray.length) {
		throw new Error('Expected ArrayBuffer to be of length ' +
			`${expectedArray.length}, but it was ${actualArray.length}`);
	}
	for (let i = 0; i < expectedArray.length; i++) {
		if (actualArray[i] !== expectedArray[i]) {
			throw new Error(`Expected ArrayBuffer value at ${i} to be ` +
				`${expectedArray[i]} but got ${actualArray[i]} instead`);
		}
	}
}

/** Encodes strings into utf-8 bytes. */
function encodeStrings(a) {
	for (let i = 0; i < a.length; i++) {
		const val = a[i];
		if (Array.isArray(val)) {
			encodeStrings(val);
		} else {
			a[i] = encodeString(val);
		}
	}
	return a;
}

/** Creates an HTMLVideoElement with autoplay-friendly default settings. */
function createVideoElement(source) {
	const video = document.createElement('video');
	if ('playsInline' in video) {
		// tslint:disable-next-line:no-any
		video.playsInline = true;
	}
	video.muted = true;
	video.loop = true;
	video.style.position = 'fixed';
	video.style.left = '0px';
	video.style.top = '0px';
	video.preload = 'auto';
	video.appendChild(source);
	return new Promise(resolve => {
		video.addEventListener('loadeddata', _ => resolve(video));
		video.load();
	});
}

async function play(video) {
	await video.play();
	if ('requestVideoFrameCallback' in video) {
		await new Promise(resolve => {
			// tslint:disable-next-line:no-any
			video.requestVideoFrameCallback(resolve);
		});
	}
}

var test_util = /*#__PURE__*/Object.freeze({
	__proto__: null,
	TEST_EPSILON_FLOAT16: TEST_EPSILON_FLOAT16,
	createVideoElement: createVideoElement,
	encodeStrings: encodeStrings,
	expectArrayBuffersEqual: expectArrayBuffersEqual,
	expectArraysClose: expectArraysClose,
	expectArraysEqual: expectArraysEqual,
	expectNumbersClose: expectNumbersClose,
	expectPromiseToFail: expectPromiseToFail,
	expectValuesInRange: expectValuesInRange,
	play: play,
	testEpsilon: testEpsilon
});

/** @license See the LICENSE file. */
// This code is auto-generated, do not modify this file!
const version$1 = '4.22.0';

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Adds two `tf.Tensor`s element-wise, A + B. Supports broadcasting.
 *
 *
 * ```js
 * const a = tf.tensor1d([1, 2, 3, 4]);
 * const b = tf.tensor1d([10, 20, 30, 40]);
 *
 * a.add(b).print();  // or tf.add(a, b)
 * ```
 *
 * ```js
 * // Broadcast add a with b.
 * const a = tf.scalar(5);
 * const b = tf.tensor1d([10, 20, 30, 40]);
 *
 * a.add(b).print();  // or tf.add(a, b)
 * ```
 * @param a The first `tf.Tensor` to add.
 * @param b The second `tf.Tensor` to add. Must have the same type as `a`.
 *
 * @doc {heading: 'Operations', subheading: 'Arithmetic'}
 */
function add_(a, b) {
	let $a = convertToTensor(a, 'a', 'add');
	let $b = convertToTensor(b, 'b', 'add');
	[$a, $b] = makeTypesMatch($a, $b);
	const inputs = {a: $a, b: $b};
	return ENGINE.runKernel(Add, inputs);
}

const add$1 = /* @__PURE__ */ op({add_});

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Divides two `tf.Tensor`s element-wise, A / B. Supports broadcasting.
 * The result is rounded with floor function.
 *
 *
 * ```js
 * const a = tf.tensor1d([1, 4, 9, 16]);
 * const b = tf.tensor1d([1, 2, 3, 4]);
 *
 * a.floorDiv(b).print();  // or tf.div(a, b)
 * ```
 *
 * ```js
 * // Broadcast div a with b.
 * const a = tf.tensor1d([2, 4, 6, 8]);
 * const b = tf.scalar(2);
 *
 * a.floorDiv(b).print();  // or tf.floorDiv(a, b)
 * ```
 *
 * @param a The first tensor as the numerator.
 * @param b The second tensor as the denominator. Must have the same dtype as
 * `a`.
 *
 * @doc {heading: 'Operations', subheading: 'Arithmetic'}
 */
function floorDiv_(a, b) {
	let $a = convertToTensor(a, 'a', 'floorDiv');
	let $b = convertToTensor(b, 'b', 'floorDiv');
	[$a, $b] = makeTypesMatch($a, $b);
	const inputs = {a: $a, b: $b};
	return ENGINE.runKernel(FloorDiv, inputs);
}

const floorDiv = /* @__PURE__ */ op({floorDiv_});

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Divides two `tf.Tensor`s element-wise, A / B. Supports broadcasting.
 *
 * ```js
 * const a = tf.tensor1d([1, 4, 9, 16]);
 * const b = tf.tensor1d([1, 2, 3, 4]);
 *
 * a.div(b).print();  // or tf.div(a, b)
 * ```
 *
 * ```js
 * // Broadcast div a with b.
 * const a = tf.tensor1d([2, 4, 6, 8]);
 * const b = tf.scalar(2);
 *
 * a.div(b).print();  // or tf.div(a, b)
 * ```
 *
 * @param a The first tensor as the numerator.
 * @param b The second tensor as the denominator. Must have the same dtype as
 * `a`.
 *
 * @doc {heading: 'Operations', subheading: 'Arithmetic'}
 */
function div_(a, b) {
	let $a = convertToTensor(a, 'a', 'div');
	let $b = convertToTensor(b, 'b', 'div');
	[$a, $b] = makeTypesMatch($a, $b);
	if ($a.dtype === 'int32' && $b.dtype === 'int32') {
		return floorDiv($a, $b);
	}
	const inputs = {a: $a, b: $b};
	const attrs = {};
	// tslint:disable-next-line: no-unnecessary-type-assertion
	return ENGINE.runKernel(RealDiv, inputs, attrs);
}

const div$1 = /* @__PURE__ */ op({div_});

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Multiplies two `tf.Tensor`s element-wise, A * B. Supports broadcasting.
 *
 * We also expose `tf.mulStrict` which has the same signature as this op and
 * asserts that `a` and `b` are the same shape (does not broadcast).
 *
 * ```js
 * const a = tf.tensor1d([1, 2, 3, 4]);
 * const b = tf.tensor1d([2, 3, 4, 5]);
 *
 * a.mul(b).print();  // or tf.mul(a, b)
 * ```
 *
 * ```js
 * // Broadcast mul a with b.
 * const a = tf.tensor1d([1, 2, 3, 4]);
 * const b = tf.scalar(5);
 *
 * a.mul(b).print();  // or tf.mul(a, b)
 * ```
 * @param a The first tensor to multiply.
 * @param b The second tensor to multiply. Must have the same dtype as `a`.
 *
 * @doc {heading: 'Operations', subheading: 'Arithmetic'}
 */
function mul_(a, b) {
	let $a = convertToTensor(a, 'a', 'mul');
	let $b = convertToTensor(b, 'b', 'mul');
	[$a, $b] = makeTypesMatch($a, $b);
	const inputs = {a: $a, b: $b};
	return ENGINE.runKernel(Multiply, inputs);
}

const mul = /* @__PURE__ */ op({mul_});

/**
 * @license
 * Copyright 2018 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Computes absolute value element-wise: `abs(x)`
 *
 * ```js
 * const x = tf.tensor1d([-1, 2, -3, 4]);
 *
 * x.abs().print();  // or tf.abs(x)
 * ```
 * @param x The input `tf.Tensor`.
 *
 * @doc {heading: 'Operations', subheading: 'Basic math'}
 */
function abs_(x) {
	const $x = convertToTensor(x, 'x', 'abs');
	if ($x.dtype === 'complex64') {
		const inputs = {x: $x};
		return ENGINE.runKernel(ComplexAbs, inputs);
	} else {
		const inputs = {x: $x};
		return ENGINE.runKernel(Abs, inputs);
	}
}

const abs$1 = /* @__PURE__ */ op({abs_});

/**
 * @license
 * Copyright 2018 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Computes acos of the input `tf.Tensor` element-wise: `acos(x)`
 *
 * ```js
 * const x = tf.tensor1d([0, 1, -1, .7]);
 *
 * x.acos().print();  // or tf.acos(x)
 * ```
 * @param x The input tensor.
 * @doc {heading: 'Operations', subheading: 'Basic math'}
 */
function acos_(x) {
	const $x = convertToTensor(x, 'x', 'acos');
	const inputs = {x: $x};
	return ENGINE.runKernel(Acos, inputs);
}

const acos = /* @__PURE__ */ op({acos_});

/**
 * @license
 * Copyright 2018 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Computes the inverse hyperbolic cos of the input `tf.Tensor` element-wise:
 * `acosh(x)`
 *
 * ```js
 * const x = tf.tensor1d([10, 1, 3, 5.7]);
 *
 * x.acosh().print();  // or tf.acosh(x)
 * ```
 * @param x The input tensor.
 *
 * @doc {heading: 'Operations', subheading: 'Basic math'}
 */
function acosh_(x) {
	const $x = convertToTensor(x, 'x', 'acosh');
	const inputs = {x: $x};
	return ENGINE.runKernel(Acosh, inputs);
}

const acosh = /* @__PURE__ */ op({acosh_});

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Adds a list of `tf.Tensor`s element-wise, each with the same shape and dtype.
 *
 * ```js
 * const a = tf.tensor1d([1, 2]);
 * const b = tf.tensor1d([3, 4]);
 * const c = tf.tensor1d([5, 6]);
 *
 * tf.addN([a, b, c]).print();
 * ```
 * @param tensors A list of tensors with the same shape and dtype.
 * @doc {heading: 'Operations', subheading: 'Arithmetic'}
 */
function addN_(tensors) {
	assert(Array.isArray(tensors), () => 'The argument passed to tf.addN() must be a list of tensors');
	assert(tensors.length >= 1, () => `Must pass at least one tensor to tf.addN(), but got ` +
		`${tensors.length}`);
	const $tensors = tensors.map((t, i) => convertToTensor(t, `tensors${i}`, 'addN'));
	const firstTensor = $tensors[0];
	$tensors.forEach(t => {
		if (t.dtype !== firstTensor.dtype) {
			throw new Error('All tensors passed to tf.addN() must have the same dtype');
		}
	});
	$tensors.forEach(t => {
		if (!arraysEqual(t.shape, firstTensor.shape)) {
			throw new Error('All tensors passed to tf.addN() must have the same shape');
		}
	});
	const inputs = $tensors;
	return ENGINE.runKernel(AddN, inputs);
}

const addN = /* @__PURE__ */ op({addN_});

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Computes the logical and of elements across dimensions of a `tf.Tensor`.
 *
 * Reduces the input along the dimensions given in `axes`. Unless `keepDims`
 * is true, the rank of the `tf.Tensor` is reduced by 1 for each entry in
 * `axes`. If `keepDims` is true, the reduced dimensions are retained with
 * length 1. If `axes` has no entries, all dimensions are reduced, and a
 * `tf.Tensor` with a single element is returned.
 *
 * ```js
 * const x = tf.tensor1d([1, 1, 1], 'bool');
 *
 * x.all().print();  // or tf.all(x)
 * ```
 *
 * ```js
 * const x = tf.tensor2d([1, 1, 0, 0], [2, 2], 'bool');
 *
 * const axis = 1;
 * x.all(axis).print();  // or tf.all(x, axis)
 * ```
 *
 * @param x The input tensor. Must be of dtype bool.
 * @param axis The dimension(s) to reduce. By default it reduces
 *     all dimensions.
 * @param keepDims If true, retains reduced dimensions with size 1.
 *
 * @doc {heading: 'Operations', subheading: 'Reduction'}
 */
function all_(x, axis = null, keepDims = false) {
	const $x = convertToTensor(x, 'x', 'all', 'bool');
	const inputs = {x: $x};
	const attrs = {axis, keepDims};
	return ENGINE.runKernel(All, inputs, attrs);
}

const all = /* @__PURE__ */ op({all_});

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Computes the logical or of elements across dimensions of a `tf.Tensor`.
 *
 * Reduces the input along the dimensions given in `axes`. Unless `keepDims`
 * is true, the rank of the `tf.Tensor` is reduced by 1 for each entry in
 * `axes`. If `keepDims` is true, the reduced dimensions are retained with
 * length 1. If `axes` has no entries, all dimensions are reduced, and a
 * `tf.Tensor` with a single element is returned.
 *
 * ```js
 * const x = tf.tensor1d([1, 1, 1], 'bool');
 *
 * x.any().print();  // or tf.any(x)
 * ```
 *
 * ```js
 * const x = tf.tensor2d([1, 1, 0, 0], [2, 2], 'bool');
 *
 * const axis = 1;
 * x.any(axis).print();  // or tf.any(x, axis)
 * ```
 *
 * @param x The input tensor. Must be of dtype bool.
 * @param axis The dimension(s) to reduce. By default it reduces
 *     all dimensions.
 * @param keepDims If true, retains reduced dimensions with size 1.
 *
 * @doc {heading: 'Operations', subheading: 'Reduction'}
 */
function any_(x, axis = null, keepDims = false) {
	const $x = convertToTensor(x, 'x', 'any', 'bool');
	const inputs = {x: $x};
	const attrs = {axis, keepDims};
	return ENGINE.runKernel(Any, inputs, attrs);
}

// tslint:disable-next-line:variable-name
const any = /* @__PURE__ */ op({any_});

/**
 * @license
 * Copyright 2020 Google Inc. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Returns the indices of the maximum values along an `axis`.
 *
 * The result has the same shape as `input` with the dimension along `axis`
 * removed.
 *
 * ```js
 * const x = tf.tensor1d([1, 2, 3]);
 *
 * x.argMax().print();  // or tf.argMax(x)
 * ```
 *
 * ```js
 * const x = tf.tensor2d([1, 2, 4, 3], [2, 2]);
 *
 * const axis = 1;
 * x.argMax(axis).print();  // or tf.argMax(x, axis)
 * ```
 *
 * @param x The input tensor.
 * @param axis The dimension to reduce. Defaults to 0 (outer-most dimension).
 *
 * @doc {heading: 'Operations', subheading: 'Reduction'}
 */
function argMax_(x, axis = 0) {
	const $x = convertToTensor(x, 'x', 'argMax');
	const inputs = {x: $x};
	const attrs = {axis};
	return ENGINE.runKernel(ArgMax, inputs, attrs);
}

const argMax = /* @__PURE__ */ op({argMax_});

/**
 * @license
 * Copyright 2020 Google Inc. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Returns the indices of the minimum values along an `axis`.
 *
 * The result has the same shape as `input` with the dimension along `axis`
 * removed.
 *
 * ```js
 * const x = tf.tensor1d([1, 2, 3]);
 *
 * x.argMin().print();  // or tf.argMin(x)
 * ```
 *
 * ```js
 * const x = tf.tensor2d([1, 2, 4, 3], [2, 2]);
 *
 * const axis = 1;
 * x.argMin(axis).print();  // or tf.argMin(x, axis)
 * ```
 *
 * @param x The input tensor.
 * @param axis The dimension to reduce. Defaults to 0 (outer-most dimension).
 *
 * @doc {heading: 'Operations', subheading: 'Reduction'}
 */
function argMin_(x, axis = 0) {
	const $x = convertToTensor(x, 'x', 'argMin');
	const inputs = {x: $x};
	const attrs = {axis};
	return ENGINE.runKernel(ArgMin, inputs, attrs);
}

const argMin = /* @__PURE__ */ op({argMin_});

/**
 * @license
 * Copyright 2018 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Computes asin of the input `tf.Tensor` element-wise: `asin(x)`
 *
 * ```js
 * const x = tf.tensor1d([0, 1, -1, .7]);
 *
 * x.asin().print();  // or tf.asin(x)
 * ```
 * @param x The input tensor.
 * @doc {heading: 'Operations', subheading: 'Basic math'}
 */
function asin_(x) {
	const $x = convertToTensor(x, 'x', 'asin');
	const inputs = {x: $x};
	return ENGINE.runKernel(Asin, inputs);
}

const asin = /* @__PURE__ */ op({asin_});

/**
 * @license
 * Copyright 2018 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Computes inverse hyperbolic sin of the input `tf.Tensor` element-wise:
 * `asinh(x)`
 *
 * ```js
 * const x = tf.tensor1d([0, 1, -1, .7]);
 *
 * x.asinh().print();  // or tf.asinh(x)
 * ```
 * @param x The input tensor.
 *
 * @doc {heading: 'Operations', subheading: 'Basic math'}
 */
function asinh_(x) {
	const $x = convertToTensor(x, 'x', 'asinh');
	const inputs = {x: $x};
	return ENGINE.runKernel(Asinh, inputs);
}

const asinh = /* @__PURE__ */ op({asinh_});

/**
 * @license
 * Copyright 2018 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Computes atan of the input `tf.Tensor` element-wise: `atan(x)`
 *
 * ```js
 * const x = tf.tensor1d([0, 1, -1, .7]);
 *
 * x.atan().print();  // or tf.atan(x)
 * ```
 * @param x The input tensor.
 *
 * @doc {heading: 'Operations', subheading: 'Basic math'}
 */
function atan_(x) {
	const $x = convertToTensor(x, 'x', 'atan');
	const inputs = {x: $x};
	return ENGINE.runKernel(Atan, inputs);
}

const atan = /* @__PURE__ */ op({atan_});

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Computes arctangent of `tf.Tensor`s a / b element-wise: `atan2(a, b)`.
 * Supports broadcasting.
 *
 * ```js
 * const a = tf.tensor1d([1.0, 1.0, -1.0, .7]);
 * const b = tf.tensor1d([2.0, 13.0, 3.5, .21]);
 *
 * tf.atan2(a, b).print()
 * ```
 *
 * @param a The first tensor.
 * @param b The second tensor. Must have the same dtype as `a`.
 *
 * @doc {heading: 'Operations', subheading: 'Basic math'}
 */
function atan2_(a, b) {
	let $a = convertToTensor(a, 'a', 'atan2');
	let $b = convertToTensor(b, 'b', 'atan2');
	[$a, $b] = makeTypesMatch($a, $b);
	const inputs = {a: $a, b: $b};
	return ENGINE.runKernel(Atan2, inputs);
}

const atan2 = /* @__PURE__ */ op({atan2_});

/**
 * @license
 * Copyright 2018 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Computes inverse hyperbolic tan of the input `tf.Tensor` element-wise:
 * `atanh(x)`
 *
 * ```js
 * const x = tf.tensor1d([0, .1, -.1, .7]);
 *
 * x.atanh().print();  // or tf.atanh(x)
 * ```
 * @param x The input tensor.
 *
 * @doc {heading: 'Operations', subheading: 'Basic math'}
 */
function atanh_(x) {
	const $x = convertToTensor(x, 'x', 'atanh');
	const inputs = {x: $x};
	return ENGINE.runKernel(Atanh, inputs);
}

const atanh = /* @__PURE__ */ op({atanh_});

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 *
 * @param inputShape Input tensor shape is of the following dimensions:
 *     `[batch, height, width, inChannels]`.
 * @param filterShape The filter shape is of the following dimensions:
 *     `[filterHeight, filterWidth, depth]`.
 * @param strides The strides of the sliding window for each dimension of the
 *     input tensor: `[strideHeight, strideWidth]`.
 *     If `strides` is a single number,
 *     then `strideHeight == strideWidth`.
 * @param pad The type of padding algorithm.
 *    - `same` and stride 1: output will be of same size as input,
 *       regardless of filter size.
 *    - `valid`: output will be smaller than input if filter is larger
 *       than 1*1x1.
 *    - For more info, see this guide:
 *     [https://www.tensorflow.org/api_docs/python/tf/nn/convolution](
 *          https://www.tensorflow.org/api_docs/python/tf/nn/convolution)
 * @param dataFormat The data format of the input and output data.
 *     Defaults to 'NHWC'.
 * @param dilations The dilation rates: `[dilationHeight, dilationWidth]`.
 *     Defaults to `[1, 1]`. If `dilations` is a single number, then
 *     `dilationHeight == dilationWidth`.
 */
function computeDilation2DInfo(inputShape, filterShape, strides, pad, dataFormat = 'NHWC', dilations) {
	// `computerConv2DInfo` require filterShape to be in the dimension of:
	// `[filterHeight, filterWidth, depth, outDepth]`, dilation2d doesn't have
	// outDepth, it should have the same depth as the input.
	// Input shape: [batch, height, width, inChannels]
	const inputChannels = inputShape[3];
	const $filterShape = [...filterShape, inputChannels];
	const $dataFormat = convertConv2DDataFormat(dataFormat);
	return computeConv2DInfo(inputShape, $filterShape, strides, dilations, pad, null /* roundingMode */, null /* depthWise */, $dataFormat);
}

function computePool2DInfo(inShape, filterSize, strides, dilations, pad, roundingMode, dataFormat = 'channelsLast') {
	const [filterHeight, filterWidth] = parseTupleParam(filterSize);
	let filterShape;
	if (dataFormat === 'channelsLast') {
		filterShape = [filterHeight, filterWidth, inShape[3], inShape[3]];
	} else if (dataFormat === 'channelsFirst') {
		filterShape = [filterHeight, filterWidth, inShape[1], inShape[1]];
	} else {
		throw new Error(`Unknown dataFormat ${dataFormat}`);
	}
	return computeConv2DInfo(inShape, filterShape, strides, dilations, pad, roundingMode, false, dataFormat);
}

/**
 * Computes the information for a forward pass of a pooling3D operation.
 */
function computePool3DInfo(inShape, filterSize, strides, dilations, pad, roundingMode, dataFormat = 'NDHWC') {
	const [filterDepth, filterHeight, filterWidth] = parse3TupleParam(filterSize);
	let filterShape;
	let $dataFormat;
	if (dataFormat === 'NDHWC') {
		$dataFormat = 'channelsLast';
		filterShape =
			[filterDepth, filterHeight, filterWidth, inShape[4], inShape[4]];
	} else if (dataFormat === 'NCDHW') {
		$dataFormat = 'channelsFirst';
		filterShape =
			[filterDepth, filterHeight, filterWidth, inShape[1], inShape[1]];
	} else {
		throw new Error(`Unknown dataFormat ${dataFormat}`);
	}
	return computeConv3DInfo(inShape, filterShape, strides, dilations, pad, false, $dataFormat, roundingMode);
}

/**
 * Computes the information for a forward pass of a convolution/pooling
 * operation.
 */
function computeConv2DInfo(inShape, filterShape, strides, dilations, pad, roundingMode, depthwise = false, dataFormat = 'channelsLast') {
	let [batchSize, inHeight, inWidth, inChannels] = [-1, -1, -1, -1];
	if (dataFormat === 'channelsLast') {
		[batchSize, inHeight, inWidth, inChannels] = inShape;
	} else if (dataFormat === 'channelsFirst') {
		[batchSize, inChannels, inHeight, inWidth] = inShape;
	} else {
		throw new Error(`Unknown dataFormat ${dataFormat}`);
	}
	const [filterHeight, filterWidth, , filterChannels] = filterShape;
	const [strideHeight, strideWidth] = parseTupleParam(strides);
	const [dilationHeight, dilationWidth] = parseTupleParam(dilations);
	const effectiveFilterHeight = getEffectiveFilterSize(filterHeight, dilationHeight);
	const effectiveFilterWidth = getEffectiveFilterSize(filterWidth, dilationWidth);
	const {
		padInfo,
		outHeight,
		outWidth
	} = getPadAndOutInfo(pad, inHeight, inWidth, strideHeight, strideWidth, effectiveFilterHeight, effectiveFilterWidth, roundingMode, dataFormat);
	const outChannels = depthwise ? filterChannels * inChannels : filterChannels;
	let outShape;
	if (dataFormat === 'channelsFirst') {
		outShape = [batchSize, outChannels, outHeight, outWidth];
	} else if (dataFormat === 'channelsLast') {
		outShape = [batchSize, outHeight, outWidth, outChannels];
	}
	return {
		batchSize,
		dataFormat,
		inHeight,
		inWidth,
		inChannels,
		outHeight,
		outWidth,
		outChannels,
		padInfo,
		strideHeight,
		strideWidth,
		filterHeight,
		filterWidth,
		effectiveFilterHeight,
		effectiveFilterWidth,
		dilationHeight,
		dilationWidth,
		inShape,
		outShape,
		filterShape
	};
}

/**
 * Computes the information for a forward pass of a 3D convolution/pooling
 * operation.
 */
function computeConv3DInfo(inShape, filterShape, strides, dilations, pad, depthwise = false, dataFormat = 'channelsLast', roundingMode) {
	let [batchSize, inDepth, inHeight, inWidth, inChannels] = [-1, -1, -1, -1, -1];
	if (dataFormat === 'channelsLast') {
		[batchSize, inDepth, inHeight, inWidth, inChannels] = inShape;
	} else if (dataFormat === 'channelsFirst') {
		[batchSize, inChannels, inDepth, inHeight, inWidth] = inShape;
	} else {
		throw new Error(`Unknown dataFormat ${dataFormat}`);
	}
	const [filterDepth, filterHeight, filterWidth, , filterChannels] = filterShape;
	const [strideDepth, strideHeight, strideWidth] = parse3TupleParam(strides);
	const [dilationDepth, dilationHeight, dilationWidth] = parse3TupleParam(dilations);
	const effectiveFilterDepth = getEffectiveFilterSize(filterDepth, dilationDepth);
	const effectiveFilterHeight = getEffectiveFilterSize(filterHeight, dilationHeight);
	const effectiveFilterWidth = getEffectiveFilterSize(filterWidth, dilationWidth);
	const {
		padInfo,
		outDepth,
		outHeight,
		outWidth
	} = get3DPadAndOutInfo(pad, inDepth, inHeight, inWidth, strideDepth, strideHeight, strideWidth, effectiveFilterDepth, effectiveFilterHeight, effectiveFilterWidth, roundingMode);
	const outChannels = depthwise ? filterChannels * inChannels : filterChannels;
	let outShape;
	if (dataFormat === 'channelsFirst') {
		outShape = [batchSize, outChannels, outDepth, outHeight, outWidth];
	} else if (dataFormat === 'channelsLast') {
		outShape = [batchSize, outDepth, outHeight, outWidth, outChannels];
	}
	return {
		batchSize,
		dataFormat,
		inDepth,
		inHeight,
		inWidth,
		inChannels,
		outDepth,
		outHeight,
		outWidth,
		outChannels,
		padInfo,
		strideDepth,
		strideHeight,
		strideWidth,
		filterDepth,
		filterHeight,
		filterWidth,
		effectiveFilterDepth,
		effectiveFilterHeight,
		effectiveFilterWidth,
		dilationDepth,
		dilationHeight,
		dilationWidth,
		inShape,
		outShape,
		filterShape
	};
}

function computeOutputShape2D(inShape, fieldSize, stride, zeroPad, roundingMode) {
	if (zeroPad == null) {
		zeroPad = computeDefaultPad(inShape, fieldSize, stride);
	}
	const inputRows = inShape[0];
	const inputCols = inShape[1];
	const outputRows = round$1((inputRows - fieldSize + 2 * zeroPad) / stride + 1, roundingMode);
	const outputCols = round$1((inputCols - fieldSize + 2 * zeroPad) / stride + 1, roundingMode);
	return [outputRows, outputCols];
}

function computeOutputShape4D(inShape, filterShape, outChannels, strides, zeroPad, roundingMode) {
	if (zeroPad == null) {
		zeroPad = computeDefaultPad(inShape, filterShape[0], strides[0]);
	}
	const outShape = [0, 0, 0, outChannels];
	for (let index = 0; index < 3; index++) {
		if (inShape[index] + 2 * zeroPad >= filterShape[index]) {
			outShape[index] = round$1((inShape[index] - filterShape[index] + 2 * zeroPad) / strides[index] +
				1, roundingMode);
		}
	}
	return outShape;
}

function computeDefaultPad(inputShape, fieldSize, stride, dilation = 1) {
	const effectiveFieldSize = getEffectiveFilterSize(fieldSize, dilation);
	return Math.floor((inputShape[0] * (stride - 1) - stride + effectiveFieldSize) / 2);
}

function parseTupleParam(param) {
	if (typeof param === 'number') {
		return [param, param, param];
	}
	if (param.length === 2) {
		return [param[0], param[1], 1];
	}
	return param;
}

function parse3TupleParam(param) {
	return typeof param === 'number' ? [param, param, param] : param;
}

/* See https://www.tensorflow.org/api_docs/python/tf/nn/atrous_conv2d
 * Atrous convolution is equivalent to standard convolution with upsampled
 * filters with effective_filter_height =
 * filter_height + (filter_height - 1) * (dilation - 1)
 * and effective_filter_width =
 * filter_width + (filter_width - 1) * (dilation - 1),
 * produced by inserting dilation - 1 zeros along consecutive elements across
 * the filters' spatial dimensions.
 * When there is a dilation, this converts a filter dimension to the
 * effective filter dimension, so it can be used in a standard convolution.
 */
function getEffectiveFilterSize(filterSize, dilation) {
	if (dilation <= 1) {
		return filterSize;
	}
	return filterSize + (filterSize - 1) * (dilation - 1);
}

function getPadAndOutInfo(pad, inHeight, inWidth, strideHeight, strideWidth, filterHeight, filterWidth, roundingMode, dataFormat) {
	let padInfo;
	let outHeight;
	let outWidth;
	if (typeof pad === 'number') {
		const padType = (pad === 0) ? 'VALID' : 'NUMBER';
		padInfo = {top: pad, bottom: pad, left: pad, right: pad, type: padType};
		const outShape = computeOutputShape2D([inHeight, inWidth], filterHeight, strideHeight, pad, roundingMode);
		outHeight = outShape[0];
		outWidth = outShape[1];
	} else if (pad === 'same') {
		outHeight = Math.ceil(inHeight / strideHeight);
		outWidth = Math.ceil(inWidth / strideWidth);
		const padAlongHeight = Math.max(0, (outHeight - 1) * strideHeight + filterHeight - inHeight);
		const padAlongWidth = Math.max(0, (outWidth - 1) * strideWidth + filterWidth - inWidth);
		const top = Math.floor(padAlongHeight / 2);
		const bottom = padAlongHeight - top;
		const left = Math.floor(padAlongWidth / 2);
		const right = padAlongWidth - left;
		padInfo = {top, bottom, left, right, type: 'SAME'};
	} else if (pad === 'valid') {
		padInfo = {top: 0, bottom: 0, left: 0, right: 0, type: 'VALID'};
		outHeight = Math.ceil((inHeight - filterHeight + 1) / strideHeight);
		outWidth = Math.ceil((inWidth - filterWidth + 1) / strideWidth);
	} else if (typeof pad === 'object') {
		const top = dataFormat === 'channelsLast' ? pad[1][0] : pad[2][0];
		const bottom = dataFormat === 'channelsLast' ? pad[1][1] : pad[2][1];
		const left = dataFormat === 'channelsLast' ? pad[2][0] : pad[3][0];
		const right = dataFormat === 'channelsLast' ? pad[2][1] : pad[3][1];
		const padType = (top === 0 && bottom === 0 && left === 0 && right === 0) ?
			'VALID' :
			'EXPLICIT';
		padInfo = {top, bottom, left, right, type: padType};
		outHeight = round$1((inHeight - filterHeight + top + bottom) / strideHeight + 1, roundingMode);
		outWidth = round$1((inWidth - filterWidth + left + right) / strideWidth + 1, roundingMode);
	} else {
		throw Error(`Unknown padding parameter: ${pad}`);
	}
	return {padInfo, outHeight, outWidth};
}

function get3DPadAndOutInfo(pad, inDepth, inHeight, inWidth, strideDepth, strideHeight, strideWidth, filterDepth, filterHeight, filterWidth, roundingMode) {
	let padInfo;
	let outDepth;
	let outHeight;
	let outWidth;
	if (pad === 'valid') {
		pad = 0;
	}
	if (typeof pad === 'number') {
		const padType = (pad === 0) ? 'VALID' : 'NUMBER';
		padInfo = {
			top: pad,
			bottom: pad,
			left: pad,
			right: pad,
			front: pad,
			back: pad,
			type: padType
		};
		const outShape = computeOutputShape4D([inDepth, inHeight, inWidth, 1], [filterDepth, filterHeight, filterWidth], 1, [strideDepth, strideHeight, strideWidth], pad, roundingMode);
		outDepth = outShape[0];
		outHeight = outShape[1];
		outWidth = outShape[2];
	} else if (pad === 'same') {
		outDepth = Math.ceil(inDepth / strideDepth);
		outHeight = Math.ceil(inHeight / strideHeight);
		outWidth = Math.ceil(inWidth / strideWidth);
		const padAlongDepth = (outDepth - 1) * strideDepth + filterDepth - inDepth;
		const padAlongHeight = (outHeight - 1) * strideHeight + filterHeight - inHeight;
		const padAlongWidth = (outWidth - 1) * strideWidth + filterWidth - inWidth;
		const front = Math.floor(padAlongDepth / 2);
		const back = padAlongDepth - front;
		const top = Math.floor(padAlongHeight / 2);
		const bottom = padAlongHeight - top;
		const left = Math.floor(padAlongWidth / 2);
		const right = padAlongWidth - left;
		padInfo = {top, bottom, left, right, front, back, type: 'SAME'};
	} else {
		throw Error(`Unknown padding parameter: ${pad}`);
	}
	return {padInfo, outDepth, outHeight, outWidth};
}

/**
 * Rounds a value depending on the rounding mode
 * @param value
 * @param roundingMode A string from: 'ceil', 'round', 'floor'. If none is
 *     provided, it will default to truncate.
 */
function round$1(value, roundingMode) {
	if (!roundingMode) {
		return Math.trunc(value);
	}
	switch (roundingMode) {
		case 'round':
			// used for Caffe Conv
			return Math.round(value);
		case 'ceil':
			// used for Caffe Pool
			return Math.ceil(value);
		case 'floor':
			return Math.floor(value);
		default:
			throw new Error(`Unknown roundingMode ${roundingMode}`);
	}
}

function tupleValuesAreOne(param) {
	const [dimA, dimB, dimC] = parseTupleParam(param);
	return dimA === 1 && dimB === 1 && dimC === 1;
}

function eitherStridesOrDilationsAreOne(strides, dilations) {
	return tupleValuesAreOne(strides) || tupleValuesAreOne(dilations);
}

function stridesOrDilationsArePositive(values) {
	return parseTupleParam(values).every(value => value > 0);
}

/**
 * Convert Conv2D dataFormat from 'NHWC'|'NCHW' to
 *    'channelsLast'|'channelsFirst'
 * @param dataFormat in 'NHWC'|'NCHW' mode
 * @return dataFormat in 'channelsLast'|'channelsFirst' mode
 * @throws unknown dataFormat
 */
function convertConv2DDataFormat(dataFormat) {
	if (dataFormat === 'NHWC') {
		return 'channelsLast';
	} else if (dataFormat === 'NCHW') {
		return 'channelsFirst';
	} else {
		throw new Error(`Unknown dataFormat ${dataFormat}`);
	}
}

/**
 * Check validity of pad when using dimRoundingMode.
 * @param opDesc A string of op description
 * @param pad The type of padding algorithm.
 *   - `same` and stride 1: output will be of same size as input,
 *       regardless of filter size.
 *   - `valid` output will be smaller than input if filter is larger
 *       than 1x1.
 *   - For more info, see this guide:
 *     [https://www.tensorflow.org/api_docs/python/tf/nn/convolution](
 *          https://www.tensorflow.org/api_docs/python/tf/nn/convolution)
 * @param dimRoundingMode A string from: 'ceil', 'round', 'floor'. If none is
 *     provided, it will default to truncate.
 * @throws unknown padding parameter
 */
function checkPadOnDimRoundingMode(opDesc, pad, dimRoundingMode) {
	if (dimRoundingMode != null) {
		if (typeof pad === 'string') {
			throw Error(`Error in ${opDesc}: pad must be an integer when using ` +
				`dimRoundingMode ${dimRoundingMode} but got pad ${pad}.`);
		} else if (typeof pad === 'number') {
			assert(isInt(pad), () => `Error in ${opDesc}: pad must be an integer when using ` +
				`dimRoundingMode ${dimRoundingMode} but got pad ${pad}.`);
		} else if (typeof pad === 'object') {
			pad.forEach(p => {
				p.forEach(v => {
					assert(isInt(v), () => `Error in ${opDesc}: pad must be an integer when using ` +
						`dimRoundingMode ${dimRoundingMode} but got pad ${v}.`);
				});
			});
		} else {
			throw Error(`Error in ${opDesc}: Unknown padding parameter: ${pad}`);
		}
	}
}

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Reshapes a `tf.Tensor` to a given shape.
 *
 * Given an input tensor, returns a new tensor with the same values as the
 * input tensor with shape `shape`.
 *
 * If one component of shape is the special value -1, the size of that
 * dimension is computed so that the total size remains constant. In
 * particular, a shape of [-1] flattens into 1-D. At most one component of
 * shape can be -1.
 *
 * If shape is 1-D or higher, then the operation returns a tensor with shape
 * shape filled with the values of tensor. In this case, the number of
 * elements implied by shape must be the same as the number of elements in
 * tensor.
 *
 * ```js
 * const x = tf.tensor1d([1, 2, 3, 4]);
 * x.reshape([2, 2]).print();
 * ```
 *
 * @param x The input tensor to be reshaped.
 * @param shape An array of integers defining the output tensor shape.
 *
 * @doc {heading: 'Tensors', subheading: 'Transformations'}
 */
function reshape_(x, shape) {
	const $x = convertToTensor(x, 'x', 'reshape', 'string_or_numeric');
	const inputs = {x: $x};
	const attrs = {shape};
	return ENGINE.runKernel(Reshape, inputs, attrs);
}

const reshape$1 = /* @__PURE__ */ op({reshape_});

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Computes the 2D average pooling of an image.
 *
 * @param x The input tensor, of rank 4 or rank 3 of shape
 *     `[batch, height, width, inChannels]`. If rank 3, batch of 1 is assumed.
 * @param filterSize The filter size: `[filterHeight, filterWidth]`. If
 *     `filterSize` is a single number, then `filterHeight == filterWidth`.
 * @param strides The strides of the pooling: `[strideHeight, strideWidth]`. If
 *     `strides` is a single number, then `strideHeight == strideWidth`.
 * @param pad The type of padding algorithm:
 *    - `same` and stride 1: output will be of same size as input,
 *       regardless of filter size.
 *    - `valid`: output will be smaller than input if filter is larger
 *       than 1x1.
 *    - For more info, see this guide:
 *     [https://www.tensorflow.org/api_docs/python/tf/nn/convolution](
 *         https://www.tensorflow.org/api_docs/python/tf/nn/convolution)
 * @param dimRoundingMode A string from: 'ceil', 'round', 'floor'. If none is
 *     provided, it will default to truncate.
 *
 * @doc {heading: 'Operations', subheading: 'Convolution'}
 */
function avgPool_(x, filterSize, strides, pad, dimRoundingMode) {
	const $x = convertToTensor(x, 'x', 'avgPool', 'float32');
	const dilations = 1;
	assert(eitherStridesOrDilationsAreOne(strides, dilations), () => 'Error in avgPool: Either strides or dilations must be 1. ' +
		`Got strides ${strides} and dilations '${dilations}'`);
	let x4D = $x;
	let reshapedTo4D = false;
	if ($x.rank === 3) {
		reshapedTo4D = true;
		x4D = reshape$1($x, [1, $x.shape[0], $x.shape[1], $x.shape[2]]);
	}
	assert(x4D.rank === 4, () => `Error in avgPool: x must be rank 4 but got rank ${x4D.rank}.`);
	checkPadOnDimRoundingMode('avgPool', pad, dimRoundingMode);
	const inputs = {x: x4D};
	const attrs = {filterSize, strides, pad, dimRoundingMode};
	// tslint:disable-next-line: no-unnecessary-type-assertion
	let res = ENGINE.runKernel(AvgPool, inputs, attrs);
	res = cast$1(res, $x.dtype);
	if (reshapedTo4D) {
		return reshape$1(res, [res.shape[1], res.shape[2], res.shape[3]]);
	}
	return res;
}

const avgPool = /* @__PURE__ */ op({avgPool_});

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Computes the 3D average pooling.
 *
 * ```js
 * const x = tf.tensor5d([1, 2, 3, 4, 5, 6, 7, 8], [1, 2, 2, 2, 1]);
 * const result = tf.avgPool3d(x, 2, 1, 'valid');
 * result.print();
 * ```
 *
 * @param x The input tensor, of rank 5 or rank 4 of shape
 *     `[batch, depth, height, width, inChannels]`.
 * @param filterSize The filter size:
 *     `[filterDepth, filterHeight, filterWidth]`.
 *     If `filterSize` is a single number,
 *     then `filterDepth == filterHeight == filterWidth`.
 * @param strides The strides of the pooling:
 *     `[strideDepth, strideHeight, strideWidth]`.
 *     If `strides` is a single number,
 *     then `strideDepth == strideHeight == strideWidth`.
 * @param pad The type of padding algorithm.
 *    - `same` and stride 1: output will be of same size as input,
 *       regardless of filter size.
 *    - `valid`: output will be smaller than input if filter is larger
 *       than 1*1x1.
 *    - For more info, see this guide:
 *     [https://www.tensorflow.org/api_docs/python/tf/nn/convolution](
 *          https://www.tensorflow.org/api_docs/python/tf/nn/convolution)
 * @param dimRoundingMode A string from: 'ceil', 'round', 'floor'. If none is
 *     provided, it will default to truncate.
 * @param dataFormat An optional string from: "NDHWC", "NCDHW". Defaults to
 *     "NDHWC". Specify the data format of the input and output data. With the
 *     default format "NDHWC", the data is stored in the order of: [batch,
 *     depth, height, width, channels]. Only "NDHWC" is currently supported.
 *
 * @doc {heading: 'Operations', subheading: 'Convolution'}
 */
function avgPool3d_(x, filterSize, strides, pad, dimRoundingMode, dataFormat = 'NDHWC') {
	const $x = convertToTensor(x, 'x', 'avgPool3d', 'float32');
	let x5D = $x;
	let reshapedTo5D = false;
	if ($x.rank === 4) {
		reshapedTo5D = true;
		x5D = reshape$1($x, [1, $x.shape[0], $x.shape[1], $x.shape[2], $x.shape[3]]);
	}
	assert(x5D.rank === 5, () => `Error in avgPool3d: x must be rank 5 but got rank ${x5D.rank}.`);
	assert(dataFormat === 'NDHWC', () => `Error in avgPool3d: Only NDHWC is currently supported, ` +
		`but got dataFormat of ${dataFormat}`);
	assert((typeof strides === 'number' && strides > 0) ||
		(Array.isArray(strides) && strides[0] > 0 && strides[1] > 0 &&
			strides[2] > 0), () => `Error in avgPool3d: Stride must be > 0, but got '${strides}'`);
	checkPadOnDimRoundingMode('avgPool3d', pad, dimRoundingMode);
	const inputs = {x: x5D};
	const attrs = {filterSize, strides, pad, dimRoundingMode, dataFormat};
	// tslint:disable-next-line: no-unnecessary-type-assertion
	let res = ENGINE.runKernel(AvgPool3D, inputs, attrs);
	res = cast$1(res, x5D.dtype);
	if (reshapedTo5D) {
		return reshape$1(res, [res.shape[1], res.shape[2], res.shape[3], res.shape[4]]);
	}
	return res;
}

const avgPool3d = /* @__PURE__ */ op({avgPool3d_});

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Creates a new tensor with the same values and shape as the specified
 * tensor.
 *
 * ```js
 * const x = tf.tensor([1, 2]);
 *
 * x.clone().print();
 * ```
 *
 * @param x The tensor to clone.
 *
 * @doc {heading: 'Tensors', subheading: 'Creation'}
 */
function clone_(x) {
	const $x = convertToTensor(x, 'x', 'clone', 'string_or_numeric');
	const inputs = {x: $x};
	// Note this op is called tf.identity in python. Hence the kernel name used
	// here.
	return ENGINE.runKernel(Identity, inputs);
}

const clone = /* @__PURE__ */ op({clone_});

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Concatenates a list of `tf.Tensor`s along a given axis.
 *
 * The tensors ranks and types must match, and their sizes must match in all
 * dimensions except `axis`.
 *
 * Also available are stricter rank-specific methods that assert that
 * `tensors` are of the given rank:
 *   - `tf.concat1d`
 *   - `tf.concat2d`
 *   - `tf.concat3d`
 *   - `tf.concat4d`
 *
 * Except `tf.concat1d` (which does not have axis param), all methods have
 * same signature as this method.
 *
 * ```js
 * const a = tf.tensor1d([1, 2]);
 * const b = tf.tensor1d([3, 4]);
 * a.concat(b).print();  // or a.concat(b)
 * ```
 *
 * ```js
 * const a = tf.tensor1d([1, 2]);
 * const b = tf.tensor1d([3, 4]);
 * const c = tf.tensor1d([5, 6]);
 * tf.concat([a, b, c]).print();
 * ```
 *
 * ```js
 * const a = tf.tensor2d([[1, 2], [10, 20]]);
 * const b = tf.tensor2d([[3, 4], [30, 40]]);
 * const axis = 1;
 * tf.concat([a, b], axis).print();
 * ```
 * @param tensors A list of tensors to concatenate.
 * @param axis The axis to concatenate along. Defaults to 0 (the first dim).
 *
 * @doc {heading: 'Tensors', subheading: 'Slicing and Joining'}
 */
function concat_(tensors, axis = 0) {
	assert(tensors.length >= 1, () => 'Pass at least one tensor to concat');
	const $tensors = convertToTensorArray(tensors, 'tensors', 'concat', 'string_or_numeric');
	if ($tensors[0].dtype === 'complex64') {
		$tensors.forEach(tensor => {
			if (tensor.dtype !== 'complex64') {
				throw new Error(`Cannot concatenate complex64 tensors with a tensor
          with dtype ${tensor.dtype}. `);
			}
		});
	}
	if ($tensors.length === 1) {
		return clone($tensors[0]);
	}
	const inputs = $tensors;
	const attr = {axis};
	return ENGINE.runKernel(Concat, inputs, attr);
}

const concat = /* @__PURE__ */ op({concat_});

/**
 * @license
 * Copyright 2018 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Computes sigmoid element-wise, `1 / (1 + exp(-x))`
 *
 * ```js
 * const x = tf.tensor1d([0, -1, 2, -3]);
 *
 * x.sigmoid().print();  // or tf.sigmoid(x)
 * ```
 * @param x The input tensor.
 *
 * @doc {heading: 'Operations', subheading: 'Basic math'}
 */
function sigmoid_(x) {
	const $x = convertToTensor(x, 'x', 'sigmoid', 'float32');
	const inputs = {x: $x};
	return ENGINE.runKernel(Sigmoid, inputs);
}

const sigmoid$1 = /* @__PURE__ */ op({sigmoid_});

/**
 * @license
 * Copyright 2018 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Extracts a slice from a `tf.Tensor` starting at coordinates `begin`
 * and is of size `size`.
 *
 * Also available are stricter rank-specific methods with the same signature
 * as this method that assert that `x` is of the given rank:
 *   - `tf.slice1d`
 *   - `tf.slice2d`
 *   - `tf.slice3d`
 *   - `tf.slice4d`
 *
 * ```js
 * const x = tf.tensor1d([1, 2, 3, 4]);
 *
 * x.slice([1], [2]).print();
 * ```
 *
 * ```js
 * const x = tf.tensor2d([1, 2, 3, 4], [2, 2]);
 *
 * x.slice([1, 0], [1, 2]).print();
 * ```
 * @param x The input `tf.Tensor` to slice from.
 * @param begin The coordinates to start the slice from. The length can be
 *     less than the rank of x - the rest of the axes will have implicit 0 as
 *     start. Can also be a single number, in which case it specifies the
 *     first axis.
 * @param size The size of the slice. The length can be less than the rank of
 *     x - the rest of the axes will have implicit -1. A value of -1 requests
 *     the rest of the dimensions in the axis. Can also be a single number,
 *     in which case it specifies the size of the first axis.
 *
 * @doc {heading: 'Tensors', subheading: 'Slicing and Joining'}
 */
function slice_(x, begin, size) {
	const $x = convertToTensor(x, 'x', 'slice', 'string_or_numeric');
	if ($x.rank === 0) {
		throw new Error('Slicing scalar is not possible');
	}
	const inputs = {x: $x};
	const attrs = {begin, size};
	return ENGINE.runKernel(Slice, inputs, attrs);
}

const slice$1 = /* @__PURE__ */ op({slice_});

/**
 * @license
 * Copyright 2018 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Computes hyperbolic tangent of the input `tf.Tensor` element-wise: `tanh(x)`
 *
 * ```js
 * const x = tf.tensor1d([0, 1, -1, 70]);
 *
 * x.tanh().print();  // or tf.tanh(x)
 * ```
 * @param x The input tensor.
 *
 * @doc {heading: 'Operations', subheading: 'Basic math'}
 */
function tanh_(x) {
	const $x = convertToTensor(x, 'x', 'tanh', 'float32');
	const inputs = {x: $x};
	return ENGINE.runKernel(Tanh, inputs);
}

const tanh = /* @__PURE__ */ op({tanh_});

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Computes the next state and output of a BasicLSTMCell.
 *
 * Returns `[newC, newH]`.
 *
 * Derived from tf.contrib.rnn.BasicLSTMCell.
 *
 * @param forgetBias Forget bias for the cell.
 * @param lstmKernel The weights for the cell.
 * @param lstmBias The bias for the cell.
 * @param data The input to the cell.
 * @param c Previous cell state.
 * @param h Previous cell output.
 *
 * @doc {heading: 'Operations', subheading: 'RNN'}
 */
function basicLSTMCell_(forgetBias, lstmKernel, lstmBias, data, c, h) {
	const $forgetBias = convertToTensor(forgetBias, 'forgetBias', 'basicLSTMCell');
	const $lstmKernel = convertToTensor(lstmKernel, 'lstmKernel', 'basicLSTMCell');
	const $lstmBias = convertToTensor(lstmBias, 'lstmBias', 'basicLSTMCell');
	const $data = convertToTensor(data, 'data', 'basicLSTMCell');
	const $c = convertToTensor(c, 'c', 'basicLSTMCell');
	const $h = convertToTensor(h, 'h', 'basicLSTMCell');
	const combined = concat([$data, $h], 1);
	const weighted = matMul$1(combined, $lstmKernel);
	const res = add$1(weighted, $lstmBias);
	// i = input_gate, j = new_input, f = forget_gate, o = output_gate
	const batchSize = res.shape[0];
	const sliceCols = res.shape[1] / 4;
	const sliceSize = [batchSize, sliceCols];
	const i = slice$1(res, [0, 0], sliceSize);
	const j = slice$1(res, [0, sliceCols], sliceSize);
	const f = slice$1(res, [0, sliceCols * 2], sliceSize);
	const o = slice$1(res, [0, sliceCols * 3], sliceSize);
	const newC = add$1(mul(sigmoid$1(i), tanh(j)), mul($c, sigmoid$1(add$1($forgetBias, f))));
	const newH = mul(tanh(newC), sigmoid$1(o));
	return [newC, newH];
}

const basicLSTMCell = /* @__PURE__ */ op({basicLSTMCell_});

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * This operation reshapes the "batch" dimension 0 into `M + 1` dimensions of
 * shape `blockShape + [batch]`, interleaves these blocks back into the grid
 * defined by the spatial dimensions `[1, ..., M]`, to obtain a result with
 * the same rank as the input. The spatial dimensions of this intermediate
 * result are then optionally cropped according to `crops` to produce the
 * output. This is the reverse of `tf.spaceToBatchND`. See below for a precise
 * description.
 *
 * ```js
 * const x = tf.tensor4d([1, 2, 3, 4], [4, 1, 1, 1]);
 * const blockShape = [2, 2];
 * const crops = [[0, 0], [0, 0]];
 *
 * x.batchToSpaceND(blockShape, crops).print();
 * ```
 *
 * @param x A `tf.Tensor`. N-D with `x.shape` = `[batch] + spatialShape +
 * remainingShape`, where spatialShape has `M` dimensions.
 * @param blockShape A 1-D array. Must have shape `[M]`, all values must
 * be >= 1.
 * @param crops A 2-D array.  Must have shape `[M, 2]`, all values must be >= 0.
 * `crops[i] = [cropStart, cropEnd]` specifies the amount to crop from input
 * dimension `i + 1`, which corresponds to spatial dimension `i`. It is required
 * that `cropStart[i] + cropEnd[i] <= blockShape[i] * inputShape[i + 1]`
 *
 * This operation is equivalent to the following steps:
 *
 * 1. Reshape `x` to `reshaped` of shape: `[blockShape[0], ...,
 * blockShape[M-1], batch / prod(blockShape), x.shape[1], ...,
 * x.shape[N-1]]`
 *
 * 2. Permute dimensions of `reshaped` to produce `permuted` of shape `[batch /
 * prod(blockShape),x.shape[1], blockShape[0], ..., x.shape[M],
 * blockShape[M-1],x.shape[M+1], ..., x.shape[N-1]]`
 *
 * 3. Reshape `permuted` to produce `reshapedPermuted` of shape `[batch /
 * prod(blockShape),x.shape[1] * blockShape[0], ..., x.shape[M] *
 * blockShape[M-1],x.shape[M+1], ..., x.shape[N-1]]`
 *
 * 4. Crop the start and end of dimensions `[1, ..., M]` of `reshapedPermuted`
 * according to `crops` to produce the output of shape: `[batch /
 * prod(blockShape),x.shape[1] * blockShape[0] - crops[0,0] - crops[0,1],
 * ..., x.shape[M] * blockShape[M-1] - crops[M-1,0] -
 * crops[M-1,1],x.shape[M+1], ..., x.shape[N-1]]`
 *
 * @doc {heading: 'Tensors', subheading: 'Transformations'}
 */
function batchToSpaceND_(x, blockShape, crops) {
	const $x = convertToTensor(x, 'x', 'batchToSpaceND');
	const prod = blockShape.reduce((a, b) => a * b);
	assert($x.rank >= 1 + blockShape.length, () => `input rank is ${$x.rank} but should be > than blockShape.length ${blockShape.length}`);
	assert(crops.length === blockShape.length, () => `crops.length is ${crops.length} but should be equal to blockShape.length  ${blockShape.length}`);
	assert($x.shape[0] % prod === 0, () => `input tensor batch is ${$x.shape[0]} but is not divisible by the product of ` +
		`the elements of blockShape ${blockShape.join(' * ')} === ${prod}`);
	const inputs = {x: $x};
	const attrs = {blockShape, crops};
	return ENGINE.runKernel(BatchToSpaceND, inputs, attrs);
}

const batchToSpaceND = /* @__PURE__ */ op({batchToSpaceND_});

function xAs4D(x) {
	let x4D;
	if (x.rank === 0 || x.rank === 1) {
		x4D = reshape$1(x, [1, 1, 1, x.size]);
	} else if (x.rank === 2) {
		x4D = reshape$1(x, [1, 1, x.shape[0], x.shape[1]]);
	} else if (x.rank === 3) {
		x4D = reshape$1(x, [1, x.shape[0], x.shape[1], x.shape[2]]);
	} else {
		x4D = x;
	}
	return x4D;
}

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Batch normalization.
 *
 * As described in
 * [http://arxiv.org/abs/1502.03167](http://arxiv.org/abs/1502.03167).
 *
 * Mean, variance, scale, and offset can be of two shapes:
 *   - The same shape as the input.
 *   - In the common case, the depth dimension is the last dimension of x, so
 *     the values would be a `tf.Tensor1D` of shape [depth].
 *
 * Also available are stricter rank-specific methods with the same signature
 * as this method that assert that parameters passed are of given rank
 *   - `tf.batchNorm2d`
 *   - `tf.batchNorm3d`
 *   - `tf.batchNorm4d`
 *
 * @param x The input Tensor.
 * @param mean A mean Tensor.
 * @param variance A variance Tensor.
 * @param offset An offset Tensor.
 * @param scale A scale Tensor.
 * @param varianceEpsilon A small float number to avoid dividing by 0.
 *
 * @doc {heading: 'Operations', subheading: 'Normalization'}
 */
function batchNorm_(x, mean, variance, offset, scale, varianceEpsilon) {
	if (varianceEpsilon == null) {
		varianceEpsilon = 0.001;
	}
	const $x = convertToTensor(x, 'x', 'batchNorm');
	const $mean = convertToTensor(mean, 'mean', 'batchNorm');
	const $variance = convertToTensor(variance, 'variance', 'batchNorm');
	let $scale;
	if (scale != null) {
		$scale = convertToTensor(scale, 'scale', 'batchNorm');
	}
	let $offset;
	if (offset != null) {
		$offset = convertToTensor(offset, 'offset', 'batchNorm');
	}
	assert($mean.rank === $variance.rank, () => 'Batch normalization gradient requires mean and variance to have ' +
		'equal ranks.');
	assert($offset == null || $mean.rank === $offset.rank, () => 'Batch normalization gradient requires mean and offset to have ' +
		'equal ranks.');
	assert($scale == null || $mean.rank === $scale.rank, () => 'Batch normalization gradient requires mean and scale to have ' +
		'equal ranks.');
	const x4D = xAs4D($x);
	const inputs = {
		x: x4D,
		scale: $scale,
		offset: $offset,
		mean: $mean,
		variance: $variance
	};
	const attrs = {varianceEpsilon};
	// tslint:disable-next-line: no-unnecessary-type-assertion
	const res = ENGINE.runKernel(FusedBatchNorm, inputs, attrs);
	return reshape$1(res, $x.shape);
}

const batchNorm = /* @__PURE__ */ op({batchNorm_});

/**
 * Batch normalization, strictly for 2D. For the more relaxed version, see
 * `tf.batchNorm`.
 *
 * @param x The input Tensor.
 * @param mean A mean Tensor.
 * @param variance A variance Tensor.
 * @param offset An offset Tensor.
 * @param scale A scale Tensor.
 * @param varianceEpsilon A small float number to avoid dividing by 0.
 */
function batchNorm2d_(x, mean, variance, offset, scale, varianceEpsilon) {
	const $x = convertToTensor(x, 'x', 'batchNorm');
	const $mean = convertToTensor(mean, 'mean', 'batchNorm');
	const $variance = convertToTensor(variance, 'variance', 'batchNorm');
	let $scale;
	if (scale != null) {
		$scale = convertToTensor(scale, 'scale', 'batchNorm');
	}
	let $offset;
	if (offset != null) {
		$offset = convertToTensor(offset, 'offset', 'batchNorm');
	}
	assert($x.rank === 2, () => `Error in batchNorm2D: x must be rank 2 but got rank ` +
		`${$x.rank}.`);
	assert($mean.rank === 2 || $mean.rank === 1, () => `Error in batchNorm2D: mean must be rank 2 or rank 1 but ` +
		`got rank ${$mean.rank}.`);
	assert($variance.rank === 2 || $variance.rank === 1, () => `Error in batchNorm2D: variance must be rank 2 or rank 1 ` +
		`but got rank ${$variance.rank}.`);
	if ($scale != null) {
		assert($scale.rank === 2 || $scale.rank === 1, () => `Error in batchNorm2D: scale must be rank 2 or rank 1 ` +
			`but got rank ${$scale.rank}.`);
	}
	if ($offset != null) {
		assert($offset.rank === 2 || $offset.rank === 1, () => `Error in batchNorm2D: offset must be rank 2 or rank 1 ` +
			`but got rank ${$offset.rank}.`);
	}
	return batchNorm($x, $mean, $variance, $offset, $scale, varianceEpsilon);
}

const batchNorm2d = /* @__PURE__ */ op({batchNorm2d_});

/**
 * Batch normalization, strictly for 3D. For the more relaxed version, see
 * `tf.batchNorm`.
 *
 * @param x The input Tensor.
 * @param mean A mean Tensor.
 * @param variance A variance Tensor.
 * @param offset An offset Tensor.
 * @param scale A scale Tensor.
 * @param varianceEpsilon A small float number to avoid dividing by 0.
 */
function batchNorm3d_(x, mean, variance, offset, scale, varianceEpsilon) {
	const $x = convertToTensor(x, 'x', 'batchNorm');
	const $mean = convertToTensor(mean, 'mean', 'batchNorm');
	const $variance = convertToTensor(variance, 'variance', 'batchNorm');
	let $scale;
	if (scale != null) {
		$scale = convertToTensor(scale, 'scale', 'batchNorm');
	}
	let $offset;
	if (offset != null) {
		$offset = convertToTensor(offset, 'offset', 'batchNorm');
	}
	assert($x.rank === 3, () => `Error in batchNorm3D: x must be rank 3 but got rank ` +
		`${$x.rank}.`);
	assert($mean.rank === 3 || $mean.rank === 1, () => `Error in batchNorm3D: mean must be rank 3 or rank 1 but ` +
		`got rank ${$mean.rank}.`);
	assert($variance.rank === 3 || $variance.rank === 1, () => `Error in batchNorm3D: variance must be rank 3 or rank 1 ` +
		`but got rank ${$variance.rank}.`);
	if ($scale != null) {
		assert($scale.rank === 3 || $scale.rank === 1, () => `Error in batchNorm3D: scale must be rank 3 or rank 1 ` +
			`but got rank ${$scale.rank}.`);
	}
	if ($offset != null) {
		assert($offset.rank === 3 || $offset.rank === 1, () => `Error in batchNorm3D: offset must be rank 3 or rank 1 ` +
			`but got rank ${$offset.rank}.`);
	}
	return batchNorm($x, $mean, $variance, $offset, $scale, varianceEpsilon);
}

const batchNorm3d = /* @__PURE__ */ op({batchNorm3d_});

/**
 * Batch normalization, strictly for 4D. For the more relaxed version, see
 * `tf.batchNorm`.
 *
 * @param x The input Tensor.
 * @param mean A mean Tensor.
 * @param variance A variance Tensor.
 * @param offset An offset Tensor.
 * @param scale A scale Tensor.
 * @param varianceEpsilon A small float number to avoid dividing by 0.
 */
function batchNorm4d_(x, mean, variance, offset, scale, varianceEpsilon) {
	const $x = convertToTensor(x, 'x', 'batchNorm');
	const $mean = convertToTensor(mean, 'mean', 'batchNorm');
	const $variance = convertToTensor(variance, 'variance', 'batchNorm');
	let $scale;
	if (scale != null) {
		$scale = convertToTensor(scale, 'scale', 'batchNorm');
	}
	let $offset;
	if (offset != null) {
		$offset = convertToTensor(offset, 'offset', 'batchNorm');
	}
	assert($x.rank === 4, () => `Error in batchNorm4D: x must be rank 4 but got rank ` +
		`${$x.rank}.`);
	assert($mean.rank === 4 || $mean.rank === 1, () => `Error in batchNorm4D: mean must be rank 4 or rank 1 but ` +
		`got rank ${$mean.rank}.`);
	assert($variance.rank === 4 || $variance.rank === 1, () => `Error in batchNorm4D: variance must be rank 4 or rank 1 ` +
		`but got rank ${$variance.rank}.`);
	if ($scale != null) {
		assert($scale.rank === 4 || $scale.rank === 1, () => `Error in batchNorm4D: scale must be rank 4 or rank 1 ` +
			`but got rank ${$scale.rank}.`);
	}
	if ($offset != null) {
		assert($offset.rank === 4 || $offset.rank === 1, () => `Error in batchNorm4D: offset must be rank 4 or rank 1 ` +
			`but got rank ${$offset.rank}.`);
	}
	return batchNorm($x, $mean, $variance, $offset, $scale, varianceEpsilon);
}

const batchNorm4d = /* @__PURE__ */ op({batchNorm4d_});

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Outputs a vector with length `size` and the same dtype as `weights`.
 *
 * If `weights` are empty, then index `i` stores the number of times the value
 * `i` is counted in `x`. If `weights` are non-empty, then index `i` stores the
 * sum of the value in `weights` at each index where the corresponding value in
 * `x` is `i`.
 *
 * Values in `x` outside of the range [0, size) are ignored.
 *
 * @param x The input int tensor, rank 1.
 * @param weights The weights tensor, must have the same shape as x, or a
 *     length-0 Tensor, in which case it acts as all weights equal to 1.
 * @param size Non-negative integer.
 *
 * @doc {heading: 'Operations', subheading: 'Reduction'}
 */
function bincount_(x, weights, size) {
	const $x = convertToTensor(x, 'x', 'bincount');
	const $weights = convertToTensor(weights, 'weights', 'bincount');
	assert($x.dtype === 'int32', () => `Error in bincount: input ` +
		`dtype must be int32, but got ${$x.dtype}`);
	assert(size >= 0, () => `size must be non-negative, but got ${size}.`);
	assert($weights.size === $x.size || $weights.size === 0, () => `Error in bincount: weights must have the same size as input or` +
		`0-length, but got input shape: ${$x.shape}, weights shape: ` +
		`${$weights.shape}.`);
	const inputs = {x: $x, weights: $weights};
	const attrs = {size};
	return ENGINE.runKernel(Bincount, inputs, attrs);
}

const bincount = /* @__PURE__ */ op({bincount_});

/**
 * @license
 * Copyright 2023 Google LLC.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Bitwise `AND` operation for input tensors.
 *
 * Given two input tensors, returns a new tensor
 * with the `AND` calculated values.
 *
 * The method supports int32 values
 *
 *
 * ```js
 * const x = tf.tensor1d([0, 5, 3, 14], 'int32');
 * const y = tf.tensor1d([5, 0, 7, 11], 'int32');
 * tf.bitwiseAnd(x, y).print();
 * ```
 *
 * @param x The input tensor to be calculated.
 * @param y The input tensor to be calculated.
 *
 * @doc {heading: 'Operations', subheading: 'Logical'}
 */
function bitwiseAnd_(x, y) {
	const $x = convertToTensor(x, 'x', 'bitwiseAnd');
	const $y = convertToTensor(y, 'y', 'bitwiseAnd');
	if (!arraysEqual($x.shape, $y.shape)) {
		throw new Error(`BitwiseAnd: Tensors must have the same shape. x: ${$x.shape}, y: ${$y.shape}`);
	}
	if ($x.dtype !== 'int32' || $y.dtype !== 'int32') {
		throw new Error(`BitwiseAnd: Only supports 'int32' values in tensor, found type of x: ${$x.dtype} and type of y: ${$y.dtype}`);
	}
	const inputs = {a: $x, b: $y};
	return ENGINE.runKernel(BitwiseAnd, inputs);
}

const bitwiseAnd = /* @__PURE__ */ op({bitwiseAnd_});

/**
 * @license
 * Copyright 2021 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Return the shape of s0 op s1 with broadcast.
 *
 * compute r0, the broadcasted shape as a tensor.
 * s0, s1 and r0 are all integer vectors.
 *
 * This function returns the shape of the result of an operation between
 * two tensors of size s0 and s1 performed with broadcast.
 *
 * @param s0 A tensor representing a shape
 * @param s1 A tensor representing a shape
 *
 * @doc {heading: 'Tensors', subheading: 'Transformations'}
 */
function broadcastArgs_(s0, s1) {
	const shape1Input = convertToTensor(s0, 's0', 'broadcastArgs', 'int32');
	const shape2Input = convertToTensor(s1, 's1', 'broadcastArgs', 'int32');
	if (shape1Input.rank !== 1) {
		throw new Error('broadcastArgs(): first input must be a vector (rank=1). ' +
			`Has rank ${shape1Input.rank}`);
	}
	if (shape2Input.rank !== 1) {
		throw new Error('broadcastArgs(): second input must be a vector (rank=1). ' +
			`Has rank ${shape2Input.rank}`);
	}
	const inputs = {s0: shape1Input, s1: shape2Input};
	return ENGINE.runKernel(BroadcastArgs, inputs);
}

const broadcastArgs = /* @__PURE__ */ op({broadcastArgs_});

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Broadcast an array to a compatible shape NumPy-style.
 *
 * The tensor's shape is compared to the broadcast shape from end to beginning.
 * Ones are prepended to the tensor's shape until it has the same length as
 * the broadcast shape. If input.shape[i]==shape[i], the (i+1)-th axis is
 * already broadcast-compatible. If input.shape[i]==1 and shape[i]==N, then
 * the input tensor is tiled N times along that axis (using tf.tile).
 *
 * @param input The tensor that is to be broadcasted.
 * @param shape The input is to be broadcast to this shape.
 *
 * @doc {heading: 'Tensors', subheading: 'Transformations'}
 */
function broadcastTo_(x, shape) {
	let input = convertToTensor(x, 'broadcastTo', 'x');
	const xShape = input.shape;
	assertNonNegativeIntegerDimensions(shape);
	if (shape.length < input.rank) {
		throw new Error(`broadcastTo(): shape.length=${shape.length} < input.rank=${input.rank}.`);
	}
	if (shape.length > input.rank) {
		const newShape = input.shape.slice();
		while (newShape.length < shape.length) {
			newShape.unshift(1);
		}
		input = reshape$1(input, newShape);
	}
	const inputShape = input.shape;
	const reps = Array.from(shape);
	for (let i = shape.length - 1; i >= 0; i--) {
		if (inputShape[i] === shape[i]) {
			reps[i] = 1;
		} else if (input.shape[i] !== 1) {
			throw new Error(`broadcastTo(): [${xShape}] cannot be broadcast to [${shape}].`);
		}
	}
	const axes = reps.map((n, i) => n > 1 ? i : -1).filter(i => i >= 0);
	if (axes.length === 0) {
		return clone(input);
	}
	// TODO call broadcastTo kernel directly once backends implement broadcstTo
	const inputs = {x: input};
	const attrs = {reps};
	return ENGINE.runKernel(Tile, inputs, attrs);
}

const broadcastTo = /* @__PURE__ */ op({broadcastTo_});

/**
 * @license
 * Copyright 2020 Google Inc. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Creates an empty `tf.TensorBuffer` with the specified `shape` and `dtype`.
 *
 * The values are stored in CPU as `TypedArray`. Fill the buffer using
 * `buffer.set()`, or by modifying directly `buffer.values`.
 *
 * When done, call `buffer.toTensor()` to get an immutable `tf.Tensor` with
 * those values.
 *
 * ```js
 * // Create a buffer and set values at particular indices.
 * const buffer = tf.buffer([2, 2]);
 * buffer.set(3, 0, 0);
 * buffer.set(5, 1, 0);
 *
 * // Convert the buffer back to a tensor.
 * buffer.toTensor().print();
 * ```
 *
 * @param shape An array of integers defining the output tensor shape.
 * @param dtype The dtype of the buffer. Defaults to 'float32'.
 * @param values The values of the buffer as `TypedArray`. Defaults to
 * zeros.
 *
 * @doc {heading: 'Tensors', subheading: 'Creation'}
 */
function buffer(shape, dtype = 'float32', values) {
	dtype = dtype || 'float32';
	assertNonNegativeIntegerDimensions(shape);
	return new TensorBuffer(shape, dtype, values);
}

/**
 * @license
 * Copyright 2018 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Computes ceiling of input `tf.Tensor` element-wise: `ceil(x)`
 *
 * ```js
 * const x = tf.tensor1d([.6, 1.1, -3.3]);
 *
 * x.ceil().print();  // or tf.ceil(x)
 * ```
 * @param x The input Tensor.
 *
 * @doc {heading: 'Operations', subheading: 'Basic math'}
 */
function ceil_(x) {
	const $x = convertToTensor(x, 'x', 'ceil', 'float32');
	const inputs = {x: $x};
	return ENGINE.runKernel(Ceil, inputs);
}

const ceil = /* @__PURE__ */ op({ceil_});

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Creates a `tf.Tensor` filled with a scalar value.
 *
 * ```js
 * tf.fill([2, 2], 4).print();
 * ```
 *
 * @param shape An array of integers defining the output tensor shape.
 * @param value The scalar value to fill the tensor with.
 * @param dtype The type of an element in the resulting tensor. Defaults to
 *     'float32' if the given param value is a number, otherwise 'string'.
 *
 * @doc {heading: 'Tensors', subheading: 'Creation'}
 */
function fill$1(shape, value, dtype) {
	assertNonNegativeIntegerDimensions(shape);
	dtype = dtype || inferDtype(value);
	const attrs = {shape, value, dtype};
	return ENGINE.runKernel(Fill, {}, attrs);
}

/**
 * @license
 * Copyright 2018 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Clips values element-wise. `max(min(x, clipValueMax), clipValueMin)`
 *
 * ```js
 * const x = tf.tensor1d([-1, 2, -3, 4]);
 *
 * x.clipByValue(-2, 3).print();  // or tf.clipByValue(x, -2, 3)
 * ```
 * @param x The input tensor.
 * @param clipValueMin Lower bound of range to be clipped to.
 * @param clipValueMax Upper bound of range to be clipped to.
 *
 * @doc {heading: 'Operations', subheading: 'Basic math'}
 */
function clipByValue_(x, clipValueMin, clipValueMax) {
	const $x = convertToTensor(x, 'x', 'clipByValue');
	assert((clipValueMin <= clipValueMax), () => `Error in clip: min (${clipValueMin}) must be ` +
		`less than or equal to max (${clipValueMax}).`);
	if (clipValueMin === clipValueMax) {
		return fill$1($x.shape, clipValueMin, $x.dtype);
	}
	const inputs = {x: $x};
	const attrs = {clipValueMin, clipValueMax};
	return ENGINE.runKernel(ClipByValue, inputs, attrs);
}

const clipByValue$1 = /* @__PURE__ */ op({clipByValue_});

/**
 * Concatenates a list of`tf.Tensor1D`s along an axis. See `concat` for details.
 *
 * For example, if:
 * A: shape(3) = |r1, g1, b1|
 * B: shape(2) = |r2, g2|
 * C = tf.concat1d([A, B]) == |r1, g1, b1, r2, g2|
 *
 * @param tensors A list of`tf.Tensor`s to concatenate.
 * @return The concatenated array.
 */
function concat1d_(tensors) {
	return concat(tensors, 0 /* axis */);
}

const concat1d = /* @__PURE__ */ op({concat1d_});

/**
 * Concatenates a list of`tf.Tensor2D`s along an axis. See `concat` for details.
 *
 * For example, if:
 * A: shape(2, 3) = | r1, g1, b1 |
 *                  | r2, g2, b2 |
 *
 * B: shape(2, 3) = | r3, g3, b3 |
 *                  | r4, g4, b4 |
 *
 * C = tf.concat2d([A, B], axis)
 *
 * if axis = 0:
 * C: shape(4, 3) = | r1, g1, b1 |
 *                  | r2, g2, b2 |
 *                  | r3, g3, b3 |
 *                  | r4, g4, b4 |
 *
 * if axis = 1:
 * C = shape(2, 6) = | r1, g1, b1, r3, g3, b3 |
 *                   | r2, g2, b2, r4, g4, b4 |
 *
 *
 * @param tensors A list of `tf.Tensor`s to concatenate.
 * @param axis The axis to concatenate along.
 * @return The concatenated array.
 */
function concat2d_(tensors, axis) {
	return concat(tensors, axis);
}

const concat2d = /* @__PURE__ */ op({concat2d_});

/**
 * Concatenates a list of `tf.Tensor3D`s along an axis.
 * See `concat` for details.
 *
 * For example, if:
 * A: shape(2, 1, 3) = | r1, g1, b1 |
 *                     | r2, g2, b2 |
 *
 * B: shape(2, 1, 3) = | r3, g3, b3 |
 *                     | r4, g4, b4 |
 *
 * C = tf.concat3d([A, B], axis)
 *
 * if axis = 0:
 * C: shape(4, 1, 3) = | r1, g1, b1 |
 *                     | r2, g2, b2 |
 *                     | r3, g3, b3 |
 *                     | r4, g4, b4 |
 *
 * if axis = 1:
 * C: shape(2, 2, 3) = | r1, g1, b1, r3, g3, b3 |
 *                     | r2, g2, b2, r4, g4, b4 |
 *
 * if axis = 2:
 * C = shape(2, 1, 6) = | r1, g1, b1, r3, g3, b3 |
 *                      | r2, g2, b2, r4, g4, b4 |
 *
 * @param tensors A list of`tf.Tensor`s to concatenate.
 * @param axis The axis to concate along.
 * @return The concatenated array.
 */
function concat3d_(tensors, axis) {
	return concat(tensors, axis);
}

const concat3d = /* @__PURE__ */ op({concat3d_});

/**
 * Concatenates a list of `tf.Tensor4D`s along an axis.
 * See `concat` for details.
 *
 * @param tensors A list of `tf.Tensor`s to concatenate.
 * @param axis The axis to concate along.
 * @return The concatenated array.
 */
function concat4d_(tensors, axis) {
	return concat(tensors, axis);
}

const concat4d = /* @__PURE__ */ op({concat4d_});

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Computes a 2D convolution over the input x.
 *
 * @param x The input tensor, of rank 4 or rank 3, of shape
 *     `[batch, height, width, inChannels]`. If rank 3, batch of 1 is
 * assumed.
 * @param filter The filter, rank 4, of shape
 *     `[filterHeight, filterWidth, inDepth, outDepth]`.
 * @param strides The strides of the convolution: `[strideHeight,
 * strideWidth]`.
 * @param pad The type of padding algorithm.
 *    - `same` and stride 1: output will be of same size as input,
 *       regardless of filter size.
 *    - `valid`: output will be smaller than input if filter is larger
 *       than 1x1.
 *   - For more info, see this guide:
 *     [https://www.tensorflow.org/api_docs/python/tf/nn/convolution](
 *          https://www.tensorflow.org/api_docs/python/tf/nn/convolution)
 * @param dataFormat: An optional string from: "NHWC", "NCHW". Defaults to
 *     "NHWC". Specify the data format of the input and output data. With the
 *     default format "NHWC", the data is stored in the order of: [batch,
 *     height, width, channels].
 * @param dilations The dilation rates: `[dilationHeight, dilationWidth]`
 *     in which we sample input values across the height and width dimensions
 *     in atrous convolution. Defaults to `[1, 1]`. If `dilations` is a single
 *     number, then `dilationHeight == dilationWidth`. If it is greater than
 *     1, then all values of `strides` must be 1.
 * @param dimRoundingMode A string from: 'ceil', 'round', 'floor'. If none is
 *     provided, it will default to truncate.
 *
 * @doc {heading: 'Operations', subheading: 'Convolution'}
 */
function conv2d_(x, filter, strides, pad, dataFormat = 'NHWC', dilations = [1, 1], dimRoundingMode) {
	const $x = convertToTensor(x, 'x', 'conv2d', 'float32');
	const $filter = convertToTensor(filter, 'filter', 'conv2d', 'float32');
	let x4D = $x;
	let reshapedTo4D = false;
	if ($x.rank === 3) {
		reshapedTo4D = true;
		x4D = reshape$1($x, [1, $x.shape[0], $x.shape[1], $x.shape[2]]);
	}
	assert(x4D.rank === 4, () => `Error in conv2d: input must be rank 4, but got rank ${x4D.rank}.`);
	assert($filter.rank === 4, () => `Error in conv2d: filter must be rank 4, but got rank ` +
		`${$filter.rank}.`);
	checkPadOnDimRoundingMode('conv2d', pad, dimRoundingMode);
	const inDepth = dataFormat === 'NHWC' ? x4D.shape[3] : x4D.shape[1];
	assert(inDepth === $filter.shape[2], () => `Error in conv2d: depth of input (${inDepth}) must match ` +
		`input depth for filter ${$filter.shape[2]}.`);
	assert(eitherStridesOrDilationsAreOne(strides, dilations), () => 'Error in conv2D: Either strides or dilations must be 1. ' +
		`Got strides ${strides} and dilations '${dilations}'`);
	assert(stridesOrDilationsArePositive(dilations), () => 'Error in conv2D: Dilated rates should be larger than 0.');
	assert(stridesOrDilationsArePositive(strides), () => 'Error in conv2D: Strides should be larger than 0.');
	const inputs = {x: x4D, filter: $filter};
	const attrs = {strides, pad, dataFormat, dilations, dimRoundingMode};
	// tslint:disable-next-line: no-unnecessary-type-assertion
	const res = ENGINE.runKernel(Conv2D, inputs, attrs);
	if (reshapedTo4D) {
		return reshape$1(res, [res.shape[1], res.shape[2], res.shape[3]]);
	}
	return res;
}

const conv2d$1 = /* @__PURE__ */ op({conv2d_});

/**
 * Computes a 1D convolution over the input x.
 *
 * @param x The input tensor, of rank 3 or rank 2, of shape
 *     `[batch, width, inChannels]`. If rank 2, batch of 1 is assumed.
 * @param filter The filter, rank 3, of shape
 *     `[filterWidth, inDepth, outDepth]`.
 * @param stride The number of entries by which the filter is moved right at
 *     each step.
 * @param pad The type of padding algorithm.
 *    - `same` and stride 1: output will be of same size as input,
 *       regardless of filter size.
 *    - `valid`: output will be smaller than input if filter is larger
 *       than 1x1.
 *   - For more info, see this guide:
 *     [https://www.tensorflow.org/api_docs/python/tf/nn/convolution](
 *          https://www.tensorflow.org/api_docs/python/tf/nn/convolution)
 * @param dataFormat An optional string from "NWC", "NCW". Defaults to "NWC",
 *     the data is stored in the order of [batch, in_width, in_channels]. Only
 *     "NWC" is currently supported.
 * @param dilation The dilation rate in which we sample input values in
 *     atrous convolution. Defaults to `1`. If it is greater than 1, then
 *     stride must be `1`.
 * @param dimRoundingMode A string from: 'ceil', 'round', 'floor'. If none is
 *     provided, it will default to truncate.
 *
 * @doc {heading: 'Operations', subheading: 'Convolution'}
 */
function conv1d_(x, filter, stride, pad, dataFormat = 'NWC', dilation = 1, dimRoundingMode) {
	const $x = convertToTensor(x, 'x', 'conv1d');
	const $filter = convertToTensor(filter, 'filter', 'conv1d');
	let x3D = $x;
	let reshapedTo3D = false;
	if ($x.rank === 2) {
		reshapedTo3D = true;
		x3D = reshape$1($x, [1, $x.shape[0], $x.shape[1]]);
	}
	assert(x3D.rank === 3, () => `Error in conv1d: input must be rank 3, but got rank ${x3D.rank}.`);
	assert($filter.rank === 3, () => `Error in conv1d: filter must be rank 3, but got rank ` +
		`${$filter.rank}.`);
	checkPadOnDimRoundingMode('conv1d', pad, dimRoundingMode);
	assert(x3D.shape[2] === $filter.shape[1], () => `Error in conv1d: depth of input (${x3D.shape[2]}) must match ` +
		`input depth for filter ${$filter.shape[1]}.`);
	assert(eitherStridesOrDilationsAreOne(stride, dilation), () => 'Error in conv1D: Either stride or dilation must be 1. ' +
		`Got stride ${stride} and dilation '${dilation}'`);
	assert(stridesOrDilationsArePositive(dilation), () => 'Error in conv1D: Dilated rates should be larger than 0.');
	assert(stridesOrDilationsArePositive(stride), () => 'Error in conv1D: Stride should be larger than 0.');
	assert(dataFormat === 'NWC', () => `Error in conv1d: got dataFormat of ${dataFormat} but only NWC is currently supported.`);
	const filter4D = reshape$1($filter, [1, $filter.shape[0], $filter.shape[1], $filter.shape[2]]);
	const input4D = reshape$1(x3D, [x3D.shape[0], 1, x3D.shape[1], x3D.shape[2]]);
	const strides = [1, stride];
	const dilations = [1, dilation];
	const conv2dDataFormat = 'NHWC';
	const res = conv2d$1(input4D, filter4D, strides, pad, conv2dDataFormat, dilations, dimRoundingMode);
	if (reshapedTo3D) {
		return reshape$1(res, [res.shape[2], res.shape[3]]);
	}
	return reshape$1(res, [res.shape[0], res.shape[2], res.shape[3]]);
}

const conv1d = /* @__PURE__ */ op({conv1d_});

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Computes the derivative of the input of a 2D convolution.
 *
 * @param xShape The shape of the input: [batch, height, width, inDepth].
 * If length of 3, batch of 1 is assumed.
 * @param dy The derivative of the output, of rank 4 or rank 3 of shape
 *   `[batch, outHeight, outWidth, outDepth]`. If rank 3, batch of 1 is
 * assumed.
 * @param filter The filter, rank 4, of shape
 *     `[filterHeight, filterWidth, inDepth, outDepth]`.
 * @param strides The strides of the convolution: `[strideHeight,
 * strideWidth]`.
 * @param pad The type of padding algorithm used:
 *    - `same` and stride 1: output will be of same size as input,
 *       regardless of filter size.
 *    - `valid`: output will be smaller than input if filter is larger
 *       than 1x1.
 * @param dataFormat: An optional string from: "NHWC", "NCHW". Defaults to
 *     "NHWC". Specify the data format of the input and output data. With the
 *     default format "NHWC", the data is stored in the order of: [batch,
 *     height, width, channels].
 * @param dimRoundingMode A string from: 'ceil', 'round', 'floor'. If none is
 *     provided, it will default to truncate.
 */
function conv2DBackpropInput_(xShape, dy, filter, strides, pad, dataFormat = 'NHWC', dimRoundingMode) {
	assert(xShape.length === dy.rank, () => `Length of inShape ` +
		`(${xShape.length}) and rank of dy (${dy.rank}) must match`);
	let xShape4D = xShape;
	let dy4D = dy;
	let reshapedTo4D = false;
	if (dy.rank === 3) {
		reshapedTo4D = true;
		dy4D = reshape$1(dy, [1, dy.shape[0], dy.shape[1], dy.shape[2]]);
		xShape4D = [1, xShape[0], xShape[1], xShape[2]];
	}
	assert(xShape4D.length === 4, () => `Error in conv2dDerInput: inShape must be length 4, but got length ` +
		`${xShape4D.length}.`);
	assert(dy4D.rank === 4, () => `Error in conv2dDerInput: dy must be rank 4, but got ` +
		`rank ${dy4D.rank}`);
	assert(filter.rank === 4, () => `Error in conv2dDerInput: filter must be rank 4, but got ` +
		`rank ${filter.rank}`);
	const inDepth = dataFormat === 'NHWC' ? xShape4D[3] : xShape4D[1];
	const outDepth = dataFormat === 'NHWC' ? dy4D.shape[3] : dy4D.shape[1];
	assert(inDepth === filter.shape[2], () => `Error in conv2dDerInput: depth of input (${inDepth}) must ` +
		`match input depth for filter ${filter.shape[2]}.`);
	assert(outDepth === filter.shape[3], () => `Error in conv2dDerInput: depth of output (${outDepth}) must ` +
		`match output depth for filter ${filter.shape[3]}.`);
	checkPadOnDimRoundingMode('conv2dDerInput', pad, dimRoundingMode);
	const inputs = {dy: dy4D, filter};
	const attrs = {strides, pad, dataFormat, dimRoundingMode, inputShape: xShape4D};
	// tslint:disable-next-line: no-unnecessary-type-assertion
	const res = ENGINE.runKernel(Conv2DBackpropInput, inputs, attrs);
	if (reshapedTo4D) {
		return reshape$1(res, [res.shape[1], res.shape[2], res.shape[3]]);
	}
	return res;
}

const conv2DBackpropInput = /* @__PURE__ */ op({conv2DBackpropInput_});

/**
 * Computes the transposed 2D convolution of an image, also known as a
 * deconvolution.
 *
 * @param x The input image, of rank 4 or rank 3, of shape
 *   `[batch, height, width, inDepth]`. If rank 3, batch of 1 is assumed.
 * @param filter The filter, rank 4, of shape
 *     `[filterHeight, filterWidth, outDepth, inDepth]`.
 *     `inDepth` must match `inDepth` in `x`.
 * @param outputShape Output shape, of rank 4 or rank 3:
 *     `[batch, height, width, outDepth]`. If rank 3, batch of 1 is assumed.
 * @param strides The strides of the original convolution:
 *     `[strideHeight, strideWidth]`.
 * @param pad  The type of padding algorithm used in the non-transpose version
 *    of the op.
 * @param dimRoundingMode A string from: 'ceil', 'round', 'floor'. If none is
 *     provided, it will default to truncate.
 *
 * @doc {heading: 'Operations', subheading: 'Convolution'}
 */
function conv2dTranspose_(x, filter, outputShape, strides, pad, dimRoundingMode) {
	const $x = convertToTensor(x, 'x', 'conv2dTranspose');
	const $filter = convertToTensor(filter, 'filter', 'conv2dTranspose');
	return conv2DBackpropInput(outputShape, $x, $filter, strides, pad, 'NHWC', dimRoundingMode);
}

const conv2dTranspose = /* @__PURE__ */ op({conv2dTranspose_});

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Computes a 3D convolution over the input x.
 *
 * @param x The input tensor, of rank 5 or rank 4, of shape
 *     `[batch, depth, height, width, channels]`. If rank 4,
 * batch of 1 is assumed.
 * @param filter The filter, rank 5, of shape
 *     `[filterDepth, filterHeight, filterWidth, inChannels, outChannels]`.
 *      inChannels must match between input and filter.
 * @param strides The strides of the convolution: `[strideDepth, strideHeight,
 * strideWidth]`.
 * @param pad The type of padding algorithm.
 *    - `same` and stride 1: output will be of same size as input,
 *       regardless of filter size.
 *    - `valid`: output will be smaller than input if filter is larger
 *       than 1x1.
 *   - For more info, see this guide:
 *     [https://www.tensorflow.org/api_docs/python/tf/nn/convolution](
 *          https://www.tensorflow.org/api_docs/python/tf/nn/convolution)
 * @param dataFormat: An optional string from: "NDHWC", "NCDHW". Defaults to
 *     "NDHWC". Specify the data format of the input and output data. With the
 *     default format "NDHWC", the data is stored in the order of: [batch,
 *     depth, height, width, channels]. Only "NDHWC" is currently supported.
 * @param dilations The dilation rates: `[dilationDepth, dilationHeight,
 *     dilationWidth]` in which we sample input values across the height
 *     and width dimensions in atrous convolution. Defaults to `[1, 1, 1]`.
 *     If `dilations` is a single number, then
 *     `dilationDepth == dilationHeight == dilationWidth`. If it is greater
 *     than 1, then all values of `strides` must be 1.
 *
 * @doc {heading: 'Operations', subheading: 'Convolution'}
 */
function conv3d_(x, filter, strides, pad, dataFormat = 'NDHWC', dilations = [1, 1, 1]) {
	const $x = convertToTensor(x, 'x', 'conv3d');
	const $filter = convertToTensor(filter, 'filter', 'conv3d');
	let x5D = $x;
	let reshapedTo5D = false;
	if ($x.rank === 4) {
		reshapedTo5D = true;
		x5D = reshape$1($x, [1, $x.shape[0], $x.shape[1], $x.shape[2], $x.shape[3]]);
	}
	assert(x5D.rank === 5, () => `Error in conv3d: input must be rank 5, but got rank ${x5D.rank}.`);
	assert($filter.rank === 5, () => `Error in conv3d: filter must be rank 5, but got rank ` +
		`${$filter.rank}.`);
	assert(x5D.shape[4] === $filter.shape[3], () => `Error in conv3d: depth of input (${x5D.shape[4]}) must match ` +
		`input depth for filter ${$filter.shape[3]}.`);
	assert(eitherStridesOrDilationsAreOne(strides, dilations), () => 'Error in conv3D: Either strides or dilations must be 1. ' +
		`Got strides ${strides} and dilations '${dilations}'`);
	assert(dataFormat === 'NDHWC', () => `Error in conv3d: got dataFormat of ${dataFormat} but only NDHWC is currently supported.`);
	assert(stridesOrDilationsArePositive(dilations), () => 'Error in conv3D: Dilated rates should be larger than 0.');
	assert(stridesOrDilationsArePositive(strides), () => 'Error in conv3D: Strides should be larger than 0.');
	const inputs = {x: x5D, filter: $filter};
	const attrs = {strides, pad, dataFormat, dilations};
	// tslint:disable-next-line: no-unnecessary-type-assertion
	const res = ENGINE.runKernel(Conv3D, inputs, attrs);
	if (reshapedTo5D) {
		return reshape$1(res, [res.shape[1], res.shape[2], res.shape[3], res.shape[4]]);
	}
	return res;
}

const conv3d = /* @__PURE__ */ op({conv3d_});

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Computes the derivative of the input of a 3D convolution.
 *
 * @param xShape The shape of the input: [batch, depth, height, width,
 * in_channels]. If length of 4, batch of 1 is assumed.
 * @param dy The derivative of the output, of rank 5 or rank 4 of shape
 *   `[batch, outDepth, outHeight, outWidth, in_channels]`.
 * If rank 4, batch of 1 is assumed.
 * @param filter The filter, rank 5, of shape
 *     `[filterDepth, filterHeight, filterWidth, inDepth, outDepth]`.
 * @param strides The strides of the convolution: `[strideDepth, strideHeight,
 * strideWidth]`.
 * @param pad The type of padding algorithm used:
 *    - `same` and stride 1: output will be of same size as input,
 *       regardless of filter size.
 *    - `valid`: output will be smaller than input if filter is larger
 *       than 1x1.
 */
function conv3DBackpropInput_(xShape, dy, filter, strides, pad) {
	assert(xShape.length === dy.rank, () => `Length of inShape ` +
		`(${xShape.length}) and rank of dy (${dy.rank}) must match`);
	let xShape5D = xShape;
	let dy5D = dy;
	let reshapedTo5D = false;
	if (dy.rank === 4) {
		reshapedTo5D = true;
		dy5D = reshape$1(dy, [1, dy.shape[0], dy.shape[1], dy.shape[2], dy.shape[3]]);
		xShape5D = [1, xShape[0], xShape[1], xShape[2], xShape[3]];
	}
	const inDepth = xShape5D[4];
	const outDepth = dy5D.shape[4];
	assert(xShape5D.length === 5, () => `Error in conv3dDerInput: inShape must be length 5, but got length ` +
		`${xShape5D.length}.`);
	assert(dy5D.rank === 5, () => `Error in conv3dDerInput: dy must be rank 5, but got ` +
		`rank ${dy5D.rank}`);
	assert(filter.rank === 5, () => `Error in conv3dDerInput: filter must be rank 5, but got ` +
		`rank ${filter.rank}`);
	assert(inDepth === filter.shape[3], () => `Error in conv3dDerInput: depth of input (${inDepth}) must ` +
		`match input depth for filter ${filter.shape[3]}.`);
	assert(outDepth === filter.shape[4], () => `Error in conv3dDerInput: depth of output (${outDepth}) must ` +
		`match output depth for filter ${filter.shape[4]}.`);
	const inputs = {dy: dy5D, filter};
	const attrs = {pad, strides, inputShape: xShape5D};
	// tslint:disable-next-line: no-unnecessary-type-assertion
	const res = ENGINE.runKernel(Conv3DBackpropInputV2, inputs, attrs);
	if (reshapedTo5D) {
		return reshape$1(res, [res.shape[1], res.shape[2], res.shape[3], res.shape[4]]);
	}
	return res;
}

const conv3DBackpropInput = /* @__PURE__ */ op({conv3DBackpropInput_});

/**
 * Computes the transposed 3D convolution of a volume, also known as a
 * deconvolution.
 *
 * @param x The input image, of rank 5 or rank 4, of shape
 *   `[batch, depth, height, width, inDepth]`. If rank 4, batch of 1 is assumed.
 * @param filter The filter, rank 4, of shape
 *     `[depth, filterHeight, filterWidth, outDepth, inDepth]`.
 *     `inDepth` must match `inDepth` in `x`.
 * @param outputShape Output shape, of rank 5 or rank 4:
 *     `[batch, depth, height, width, outDepth]`. If rank 3, batch of 1 is
 *    assumed.
 * @param strides The strides of the original convolution:
 *     `[strideDepth, strideHeight, strideWidth]`.
 * @param pad  The type of padding algorithm used in the non-transpose version
 *    of the op.
 *
 * @doc {heading: 'Operations', subheading: 'Convolution'}
 */
function conv3dTranspose_(x, filter, outputShape, strides, pad) {
	const $x = convertToTensor(x, 'x', 'conv3dTranspose');
	const $filter = convertToTensor(filter, 'filter', 'conv3dTranspose');
	return conv3DBackpropInput(outputShape, $x, $filter, strides, pad);
}

const conv3dTranspose = /* @__PURE__ */ op({conv3dTranspose_});

/**
 * @license
 * Copyright 2018 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Computes cos of the input `tf.Tensor` element-wise: `cos(x)`
 *
 * ```js
 * const x = tf.tensor1d([0, Math.PI / 2, Math.PI * 3 / 4]);
 *
 * x.cos().print();  // or tf.cos(x)
 * ```
 * @param x The input tensor. Must be float32 type.
 *
 * @doc {heading: 'Operations', subheading: 'Basic math'}
 */
function cos_(x) {
	const $x = convertToTensor(x, 'x', 'cos', 'float32');
	const inputs = {x: $x};
	return ENGINE.runKernel(Cos, inputs);
}

const cos = /* @__PURE__ */ op({cos_});

/**
 * @license
 * Copyright 2018 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Computes hyperbolic cos of the input `tf.Tensor` element-wise: `cosh(x)`
 *
 * ```js
 * const x = tf.tensor1d([0, 1, -1, .7]);
 *
 * x.cosh().print();  // or tf.cosh(x)
 * ```
 * @param x The input tensor. Must be float32 type.
 *
 * @doc {heading: 'Operations', subheading: 'Basic math'}
 */
function cosh_(x) {
	const $x = convertToTensor(x, 'x', 'cosh', 'float32');
	const inputs = {x: $x};
	return ENGINE.runKernel(Cosh, inputs);
}

const cosh = /* @__PURE__ */ op({cosh_});

/**
 * @license
 * Copyright 2022 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the 'License');
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an 'AS IS' BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Computes the cumulative product of a `tf.Tensor` along `axis`.
 *
 * ```js
 * const x = tf.tensor([1, 2, 3, 4]);
 * x.cumprod().print();
 * ```
 * ```js
 * const x = tf.tensor([[1, 2], [3, 4]]);
 * x.cumprod().print();
 * ```
 *
 * @param x The input tensor to cumulatively multiply.
 * @param axis The axis along which to multiply. Optional. Defaults to 0.
 * @param exclusive Whether to perform exclusive cumulative product. Optional.
 *     Defaults to false. If set to true then the product of each tensor entry
 *     does not include its own value, but only the values previous to it
 *     along the specified axis.
 * @param reverse Whether to multiply in the opposite direction. Optional.
 *     Defaults to false.
 *
 * @doc {heading: 'Operations', subheading: 'Scan'}
 */
function cumprod_(x, axis = 0, exclusive = false, reverse = false) {
	const $x = convertToTensor(x, 'x', 'cumprod');
	const inputs = {x: $x};
	const attrs = {axis, exclusive, reverse};
	return ENGINE.runKernel(Cumprod, inputs, attrs);
}

const cumprod = /* @__PURE__ */ op({cumprod_});

/**
 * @license
 * Copyright 2018 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Computes the cumulative sum of a `tf.Tensor` along `axis`.
 *
 * ```js
 * const x = tf.tensor([1, 2, 3, 4]);
 * x.cumsum().print();
 * ```
 * ```js
 * const x = tf.tensor([[1, 2], [3, 4]]);
 * x.cumsum().print();
 * ```
 *
 * @param x The input tensor to be summed.
 * @param axis The axis along which to sum. Optional. Defaults to 0.
 * @param exclusive Whether to perform exclusive cumulative sum. Optional.
 *     Defaults to false. If set to true then the sum of each tensor entry
 *     does not include its own value, but only the values previous to it
 *     along the specified axis.
 * @param reverse Whether to sum in the opposite direction. Optional.
 *     Defaults to false.
 *
 * @doc {heading: 'Operations', subheading: 'Scan'}
 */
function cumsum_(x, axis = 0, exclusive = false, reverse = false) {
	const $x = convertToTensor(x, 'x', 'cumsum');
	const inputs = {x: $x};
	const attrs = {axis, exclusive, reverse};
	return ENGINE.runKernel(Cumsum, inputs, attrs);
}

const cumsum = /* @__PURE__ */ op({cumsum_});

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Outputs a vector with length `size` and the same dtype as `weights`.
 *
 * If `weights` are empty, then index `i` stores the number of times the value
 * `i` is counted in `x`. If `weights` are non-empty, then index `i` stores the
 * sum of the value in `weights` at each index where the corresponding value in
 * `x` is `i`.
 *
 * Values in `x` outside of the range [0, size) are ignored.
 *
 * @param x The input int tensor, rank 1 or rank 2.
 * @param weights The weights tensor, must have the same shape as x, or a
 *     length-0 Tensor, in which case it acts as all weights equal to 1.
 * @param size Non-negative integer.
 * @param binaryOutput Optional. Whether the kernel should count the appearance
 *     or number of occurrences. Defaults to False.
 *
 * @doc {heading: 'Operations', subheading: 'Reduction'}
 */
function denseBincount_(x, weights, size, binaryOutput = false) {
	const $x = convertToTensor(x, 'x', 'denseBincount');
	const $weights = convertToTensor(weights, 'weights', 'denseBincount');
	assert($x.dtype === 'int32', () => `Error in denseBincount: input ` +
		`dtype must be int32, but got ${$x.dtype}`);
	assert($x.rank <= 2, () => `Error in denseBincount: input must be at most rank 2, but got ` +
		`rank ${$x.rank}.`);
	assert(size >= 0, () => `size must be non-negative, but got ${size}.`);
	assert($weights.size === $x.size || $weights.size === 0, () => `Error in denseBincount: weights must have the same shape as x or ` +
		`0-length, but got x shape: ${$x.shape}, weights shape: ` +
		`${$weights.shape}.`);
	const inputs = {x: $x, weights: $weights};
	const attrs = {size, binaryOutput};
	return ENGINE.runKernel(DenseBincount, inputs, attrs);
}

const denseBincount = /* @__PURE__ */ op({denseBincount_});

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Rearranges data from depth into blocks of spatial data. More specifically,
 * this op outputs a copy of the input tensor where values from the `depth`
 * dimension are moved in spatial blocks to the `height` and `width` dimensions.
 * The attr `blockSize` indicates the input block size and how the data is
 * moved.
 *
 *  - Chunks of data of size `blockSize * blockSize` from depth are rearranged
 * into non-overlapping blocks of size `blockSize x blockSize`
 *
 *  - The width the output tensor is `inputWidth * blockSize`, whereas the
 * height is `inputHeight * blockSize`
 *
 *  - The Y, X coordinates within each block of the output image are determined
 * by the high order component of the input channel index
 *
 *  - The depth of the input tensor must be divisible by `blockSize *
 * blockSize`
 *
 * The `dataFormat` attr specifies the layout of the input and output tensors
 * with the following options: "NHWC": [ `batch, height, width, channels` ]
 * "NCHW": [ `batch, channels, height, width` ]
 *
 * ```js
 * const x = tf.tensor4d([1, 2, 3, 4], [1, 1, 1, 4]);
 * const blockSize = 2;
 * const dataFormat = "NHWC";
 *
 * tf.depthToSpace(x, blockSize, dataFormat).print();
 * ```
 *
 * @param x The input tensor of rank 4
 * @param blockSIze  An `int` that is `>= 2`. The size of the spatial block
 * @param dataFormat An optional string from: "NHWC", "NCHW". Defaults to "NHWC"
 *
 * @doc {heading: 'Tensors', subheading: 'Transformations'}
 */
function depthToSpace_(x, blockSize, dataFormat = 'NHWC') {
	const $x = convertToTensor(x, 'x', 'depthToSpace', 'float32');
	const inputHeight = (dataFormat === 'NHWC') ? $x.shape[1] : $x.shape[2];
	const inputWidth = (dataFormat === 'NHWC') ? $x.shape[2] : $x.shape[3];
	const inputDepth = (dataFormat === 'NHWC') ? $x.shape[3] : $x.shape[1];
	assert(blockSize > 1, () => `blockSize should be > 1 for depthToSpace, but was: ${blockSize}`);
	assert(inputHeight * blockSize >= 0, () => `Negative dimension size caused by overflow when multiplying
    ${inputHeight} and ${blockSize}  for depthToSpace with input shape
    ${$x.shape}`);
	assert(inputWidth * blockSize >= 0, () => `Negative dimension size caused by overflow when multiplying
    ${inputWidth} and ${blockSize} for depthToSpace with input shape
        ${$x.shape}`);
	assert((inputDepth % (blockSize * blockSize) === 0), () => `Dimension size must be evenly divisible by ${blockSize * blockSize} but is ${inputDepth} for depthToSpace with input shape ${$x.shape}`);
	const inputs = {x: $x};
	const attrs = {blockSize, dataFormat};
	return ENGINE.runKernel(DepthToSpace, inputs, attrs);
}

const depthToSpace = /* @__PURE__ */ op({depthToSpace_});

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Depthwise 2D convolution.
 *
 * Given a 4D `input` array and a `filter` array of shape
 * `[filterHeight, filterWidth, inChannels, channelMultiplier]` containing
 * `inChannels` convolutional filters of depth 1, this op applies a
 * different filter to each input channel (expanding from 1 channel to
 * `channelMultiplier` channels for each), then concatenates the results
 * together. The output has `inChannels * channelMultiplier` channels.
 *
 * See
 * [https://www.tensorflow.org/api_docs/python/tf/nn/depthwise_conv2d](
 *     https://www.tensorflow.org/api_docs/python/tf/nn/depthwise_conv2d)
 * for more details.
 *
 * @param x The input tensor, of rank 4 or rank 3, of shape
 *     `[batch, height, width, inChannels]`. If rank 3, batch of 1 is
 * assumed.
 * @param filter The filter tensor, rank 4, of shape
 *     `[filterHeight, filterWidth, inChannels, channelMultiplier]`.
 * @param strides The strides of the convolution: `[strideHeight,
 * strideWidth]`. If strides is a single number, then `strideHeight ==
 * strideWidth`.
 * @param pad The type of padding algorithm.
 *   - `same` and stride 1: output will be of same size as input,
 *       regardless of filter size.
 *   - `valid`: output will be smaller than input if filter is larger
 *       than 1x1.
 *   - For more info, see this guide:
 *     [https://www.tensorflow.org/api_docs/python/tf/nn/convolution](
 *          https://www.tensorflow.org/api_docs/python/tf/nn/convolution)
 * @param dilations The dilation rates: `[dilationHeight, dilationWidth]`
 *     in which we sample input values across the height and width dimensions
 *     in atrous convolution. Defaults to `[1, 1]`. If `rate` is a single
 *     number, then `dilationHeight == dilationWidth`. If it is greater than
 *     1, then all values of `strides` must be 1.
 * @param dataFormat: An optional string from: "NHWC", "NCHW". Defaults to
 *     "NHWC". Specify the data format of the input and output data. With the
 *     default format "NHWC", the data is stored in the order of: [batch,
 *     height, width, channels]. Only "NHWC" is currently supported.
 * @param dimRoundingMode A string from: 'ceil', 'round', 'floor'. If none is
 *     provided, it will default to truncate.
 *
 * @doc {heading: 'Operations', subheading: 'Convolution'}
 */
function depthwiseConv2d_(x, filter, strides, pad, dataFormat = 'NHWC', dilations = [1, 1], dimRoundingMode) {
	const $x = convertToTensor(x, 'x', 'depthwiseConv2d', 'float32');
	const $filter = convertToTensor(filter, 'filter', 'depthwiseConv2d', 'float32');
	let x4D = $x;
	let reshapedTo4D = false;
	if ($x.rank === 3) {
		reshapedTo4D = true;
		x4D = reshape$1($x, [1, $x.shape[0], $x.shape[1], $x.shape[2]]);
	}
	assert(x4D.rank === 4, () => `Error in depthwiseConv2d: input must be rank 4, but got ` +
		`rank ${x4D.rank}.`);
	assert($filter.rank === 4, () => `Error in depthwiseConv2d: filter must be rank 4, but got rank ` +
		`${$filter.rank}.`);
	const inChannels = dataFormat === 'NHWC' ? x4D.shape[3] : x4D.shape[1];
	assert(inChannels === $filter.shape[2], () => `Error in depthwiseConv2d: number of input channels ` +
		`(${inChannels}) must match the inChannels dimension in ` +
		`filter ${$filter.shape[2]}.`);
	checkPadOnDimRoundingMode('depthwiseConv2d', pad, dimRoundingMode);
	const inputs = {x: x4D, filter: $filter};
	const attrs = {strides, pad, dataFormat, dilations, dimRoundingMode};
	// tslint:disable-next-line: no-unnecessary-type-assertion
	const res = ENGINE.runKernel(DepthwiseConv2dNative, inputs, attrs);
	if (reshapedTo4D) {
		return reshape$1(res, [res.shape[1], res.shape[2], res.shape[3]]);
	}
	return res;
}

const depthwiseConv2d$1 = /* @__PURE__ */ op({depthwiseConv2d_});

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Returns a diagonal tensor with given diagonal values.
 *
 * Given a diagonal, this operation returns a tensor with the diagonal and
 * everything else padded with zeros.
 *
 * Assume the input has dimensions `[D1,..., Dk]`, then the output is a tensor
 * of rank 2k with dimensions `[D1,..., Dk, D1,..., Dk]`
 *
 * ```js
 * const x = tf.tensor1d([1, 2, 3, 4]);
 *
 * tf.diag(x).print()
 * ```
 * ```js
 * const x = tf.tensor2d([1, 2, 3, 4, 5, 6, 7, 8], [4, 2])
 *
 * tf.diag(x).print()
 * ```
 * @param x The input tensor.
 *
 * @doc {heading: 'Tensors', subheading: 'Creation'}
 */
function diag_(x) {
	const $x = convertToTensor(x, 'x', 'diag');
	const inputs = {x: $x};
	return ENGINE.runKernel(Diag, inputs);
}

const diag = /* @__PURE__ */ op({diag_});

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Computes the grayscale dilation over the input `x`.
 *
 * @param x The input tensor, rank 3 or rank 4 of shape
 *     `[batch, height, width, depth]`. If rank 3, batch of 1 is assumed.
 * @param filter The filter tensor, rank 3, of shape
 *     `[filterHeight, filterWidth, depth]`.
 * @param strides The strides of the sliding window for each dimension of the
 *     input tensor: `[strideHeight, strideWidth]`.
 *     If `strides` is a single number,
 *     then `strideHeight == strideWidth`.
 * @param pad The type of padding algorithm.
 *    - `same` and stride 1: output will be of same size as input,
 *       regardless of filter size.
 *    - `valid`: output will be smaller than input if filter is larger
 *       than 1*1x1.
 *    - For more info, see this guide:
 *     [https://www.tensorflow.org/api_docs/python/tf/nn/convolution](
 *          https://www.tensorflow.org/api_docs/python/tf/nn/convolution)
 * @param dataFormat Specify the data format of the input and output data.
 *      Defaults to 'NHWC'. Only 'NHWC' is currently supported. With the
 *      default format "NHWC", the data is stored in the order of: [batch,
 *      height, width, channels].
 * @param dilations The dilation rates: `[dilationHeight, dilationWidth]`
 *     in which we sample input values across the height and width dimensions
 *     for atrous morphological dilation. Defaults to `[1, 1]`. If `dilations`
 *     is a single number, then `dilationHeight == dilationWidth`. If it is
 *     greater than 1, then all values of `strides` must be 1.
 *
 * @doc {heading: 'Operations', subheading: 'Convolution'}
 */
function dilation2d_(x, filter, strides, pad, dilations = [1, 1], dataFormat = 'NHWC') {
	const $x = convertToTensor(x, 'x', 'dilation2d');
	const $filter = convertToTensor(filter, 'filter', 'dilation2d');
	assert($x.rank === 3 || $x.rank === 4, () => `Error in dilation2d: input must be rank 3 or 4, but got rank ` +
		`${$x.rank}.`);
	assert($filter.rank === 3, () => `Error in dilation2d: filter must be rank 3, but got rank ` +
		`${$filter.rank}.`);
	assert(dataFormat === 'NHWC', () => `Error in dilation2d: Only NHWC is currently supported, ` +
		`but got dataFormat of ${dataFormat}`);
	let x4D = $x;
	let reshapedTo4D = false;
	if ($x.rank === 3) {
		x4D = reshape$1($x, [1, $x.shape[0], $x.shape[1], $x.shape[2]]);
		reshapedTo4D = true;
	}
	assert(x4D.shape[3] === $filter.shape[2], () => `Error in dilation2d:  input and filter must have the same depth: ${x4D.shape[3]} vs ${$filter.shape[2]}`);
	const inputs = {x: x4D, filter: $filter};
	const attrs = {strides, pad, dilations};
	// tslint:disable-next-line: no-unnecessary-type-assertion
	const res = ENGINE.runKernel(Dilation2D, inputs, attrs);
	if (reshapedTo4D) {
		return reshape$1(res, [res.shape[1], res.shape[2], res.shape[3]]);
	}
	return res;
}

const dilation2d = /* @__PURE__ */ op({dilation2d_});

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Returns the truth value of (a == b) element-wise. Supports broadcasting.
 *
 * ```js
 * const a = tf.tensor1d([1, 2, 3]);
 * const b = tf.tensor1d([2, 2, 2]);
 *
 * a.equal(b).print();
 * ```
 *
 * @param a The first input tensor.
 * @param b The second input tensor. Must have the same dtype as `a`.
 *
 * @doc {heading: 'Operations', subheading: 'Logical'}
 */
function equal_(a, b) {
	let $a = convertToTensor(a, 'a', 'equal', 'string_or_numeric');
	let $b = convertToTensor(b, 'b', 'equal', 'string_or_numeric');
	[$a, $b] = makeTypesMatch($a, $b);
	assertAndGetBroadcastShape($a.shape, $b.shape);
	const inputs = {a: $a, b: $b};
	return ENGINE.runKernel(Equal, inputs);
}

const equal$1 = /* @__PURE__ */ op({equal_});

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Returns the elements, either `a` or `b` depending on the `condition`.
 *
 * If the condition is true, select from `a`, otherwise select from `b`.
 *
 * ```js
 * const cond = tf.tensor1d([false, false, true], 'bool');
 * const a = tf.tensor1d([1 , 2, 3]);
 * const b = tf.tensor1d([-1, -2, -3]);
 *
 * a.where(cond, b).print();
 * ```
 *
 * @param condition The input condition. Must be of dtype bool.
 * @param a If `condition` is rank 1, `a` may have a higher rank but
 *     its first dimension must match the size of `condition`.
 * @param b A tensor with the same dtype as `a` and with shape that is
 *     compatible with `a`.
 * @return A tensor with same dtype as `a` and `b`, and shape that is
 *     broadcastable from `a` and `b`.
 *
 * @doc {heading: 'Operations', subheading: 'Logical'}
 */
function where_(condition, a, b) {
	const $a = convertToTensor(a, 'a', 'where');
	const $b = convertToTensor(b, 'b', 'where');
	const $condition = convertToTensor(condition, 'condition', 'where', 'bool');
	// TODO: move this logic to forward function when the broadcastTo op is
	// implemented in WASM.
	// Find the broadcastable shape for $condition, $a, and $b.
	const broadcastShape = assertAndGetBroadcastShape(assertAndGetBroadcastShape($condition.shape, $a.shape), $b.shape);
	const $broadcastedCondition = broadcastTo($condition, broadcastShape);
	const $broadcastedA = broadcastTo($a, broadcastShape);
	const $broadcastedB = broadcastTo($b, broadcastShape);
	const inputs = {
		condition: $broadcastedCondition,
		t: $broadcastedA,
		e: $broadcastedB
	};
	return ENGINE.runKernel(Select, inputs);
}

const where = /* @__PURE__ */ op({where_});

/**
 * @license
 * Copyright 2018 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Creates a `tf.Tensor` with all elements set to 0 with the same shape as the
 * given tensor.
 *
 * ```js
 * const x = tf.tensor([1, 2]);
 * tf.zerosLike(x).print();
 * ```
 *
 * @param x The tensor of required shape.
 *
 * @doc {heading: 'Tensors', subheading: 'Creation'}
 */
function zerosLike_(x) {
	const $x = convertToTensor(x, 'x', 'zerosLike');
	const inputs = {x: $x};
	return ENGINE.runKernel(ZerosLike, inputs);
}

const zerosLike$1 = /* @__PURE__ */ op({zerosLike_});

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Divides two `tf.Tensor`s element-wise, A / B. Supports broadcasting. Return 0
 * if denominator is 0.
 *
 *
 * ```js
 * const a = tf.tensor1d([1, 4, 9, 16]);
 * const b = tf.tensor1d([1, 2, 3, 4]);
 * const c = tf.tensor1d([0, 0, 0, 0]);
 *
 * a.divNoNan(b).print();  // or tf.divNoNan(a, b)
 * a.divNoNan(c).print();  // or tf.divNoNan(a, c)
 * ```
 *
 * ```js
 * // Broadcast div a with b.
 * const a = tf.tensor1d([2, 4, 6, 8]);
 * const b = tf.scalar(2);
 * const c = tf.scalar(0);
 *
 * a.divNoNan(b).print();  // or tf.divNoNan(a, b)
 * a.divNoNan(c).print();  // or tf.divNoNan(a, c)
 * ```
 *
 * @param a The first tensor as the numerator.
 * @param b The second tensor as the denominator. Must have the same dtype as
 * `a`.
 *
 * @doc {heading: 'Operations', subheading: 'Arithmetic'}
 */
function divNoNan_(a, b) {
	// TODO: Make this into its own kernel.
	let $a = convertToTensor(a, 'a', 'div');
	let $b = convertToTensor(b, 'b', 'div');
	[$a, $b] = makeTypesMatch($a, $b);
	const divResult = div$1($a, $b);
	const zeros = zerosLike$1(divResult);
	const bEqualsZero = equal$1($b, zeros);
	return where(bEqualsZero, zeros, divResult);
}

const divNoNan = /* @__PURE__ */ op({divNoNan_});

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Computes the dot product of two matrices and/or vectors, `t1` and `t2`.
 *
 * ```js
 * const a = tf.tensor1d([1, 2]);
 * const b = tf.tensor2d([[1, 2], [3, 4]]);
 * const c = tf.tensor2d([[1, 2, 3], [4, 5, 6]]);
 *
 * a.dot(b).print();  // or tf.dot(a, b)
 * b.dot(a).print();
 * b.dot(c).print();
 * ```
 * @param t1 The first tensor in the dot operation.
 * @param t2 The second tensor in the dot operation.
 *
 * @doc {heading: 'Operations', subheading: 'Matrices'}
 */
function dot_(t1, t2) {
	const $t1 = convertToTensor(t1, 't1', 'dot');
	const $t2 = convertToTensor(t2, 't2', 'dot');
	assert(($t1.rank === 1 || $t1.rank === 2) && ($t2.rank === 1 || $t2.rank === 2), () => `Error in dot: inputs must all be rank 1 or 2, but got ranks ` +
		`${$t1.rank} and ${$t2.rank}.`);
	const t1Inner = ($t1.rank === 1 ? $t1.size : $t1.shape[1]);
	const t2Inner = ($t2.rank === 1 ? $t2.size : $t2.shape[0]);
	assert(t1Inner === t2Inner, () => `Error in dot: inner dimensions of inputs must match, but got ` +
		`${t1Inner} and ${t2Inner}.`);
	if ($t1.rank === 1 && $t2.rank === 1) {
		const t12D = reshape$1($t1, [1, -1]);
		const t22D = reshape$1($t2, [-1, 1]);
		const t1t2 = matMul$1(t12D, t22D);
		return reshape$1(t1t2, []);
	} else if ($t1.rank === 1 && $t2.rank === 2) {
		const t12D = reshape$1($t1, [1, -1]);
		const t22D = reshape$1($t2, [$t2.shape[0], $t2.shape[1]]);
		const t1t2 = matMul$1(t12D, t22D);
		return reshape$1(t1t2, [t1t2.size]);
	} else if ($t1.rank === 2 && $t2.rank === 1) {
		const t22D = reshape$1($t2, [-1, 1]);
		const t1t2 = matMul$1($t1, t22D);
		return reshape$1(t1t2, [t1t2.size]);
	} else {
		const t22D = reshape$1($t2, [$t2.shape[0], $t2.shape[1]]);
		const t1t2 = matMul$1($t1, t22D);
		return t1t2;
	}
}

const dot = /* @__PURE__ */ op({dot_});

/**
 * @license
 * Copyright 2021 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Tensor contraction over specified indices and outer product.
 *
 * `einsum` allows defining Tensors by defining their element-wise computation.
 * This computation is based on
 * [Einstein summation](https://en.wikipedia.org/wiki/Einstein_notation).
 *
 * Some special cases include:
 *
 * Matrix multiplication:
 * ```js
 * const x = tf.tensor2d([[1, 2, 3], [4, 5, 6]]);
 * const y = tf.tensor2d([[0, 1], [2, 3], [4, 5]]);
 * x.print();
 * y.print();
 * tf.einsum('ij,jk->ik', x, y).print();
 * ```
 *
 * Dot product:
 * ```js
 * const x = tf.tensor1d([1, 2, 3]);
 * const y = tf.tensor1d([0, 1, 2]);
 * x.print();
 * y.print();
 * tf.einsum('i,i->', x, y).print();
 * ```
 *
 * Batch dot product:
 * ```js
 * const x = tf.tensor2d([[1, 2, 3], [4, 5, 6]]);
 * const y = tf.tensor2d([[0, 1, 2], [3, 4, 5]]);
 * x.print();
 * y.print();
 * tf.einsum('bi,bi->b', x, y).print();
 * ```
 *
 * Outer prouduct:
 * ```js
 * const x = tf.tensor1d([1, 3, 5]);
 * const y = tf.tensor1d([2, 4, 6]);
 * x.print();
 * y.print();
 * tf.einsum('i,j->ij', x, y).print();
 * ```
 *
 * Matrix transpose:
 * ```js
 * const x = tf.tensor2d([[1, 2], [3, 4]]);
 * x.print();
 * tf.einsum('ij->ji', x).print();
 * ```
 *
 * Batch matrix transpose:
 * ```js
 * const x = tf.tensor3d([[[1, 2], [3, 4]], [[-1, -2], [-3, -4]]]);
 * x.print();
 * tf.einsum('bij->bji', x).print();
 * ```
 *
 * Limitations:
 *
 * This implementation of einsum has the following limitations:
 *
 * - Does not support >2 input tensors.
 * - Does not support duplicate axes for any given input tensor. E.g., equation
 *   'ii->' is not supported.
 * - The `...` notation is not supported.
 *
 * @param equation a string describing the contraction, in the same format as
 * [numpy.einsum](https://numpy.org/doc/stable/reference/generated/numpy.einsum.html).
 * @param tensors the input(s) to contract (each one a Tensor), whose shapes
 *     should be consistent with equation.
 * @returns The output tensor.
 *
 * @doc {heading: 'Tensors', subheading: 'Matrices'}
 */
function einsum_(equation, ...tensors) {
	const $tensors = tensors.map((t, i) => convertToTensor(t, `tensors${i}`, 'einsum'));
	const attrs = {equation};
	return ENGINE.runKernel(Einsum, $tensors, attrs);
}

const einsum = /* @__PURE__ */ op({einsum_});

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Computes exponential linear element-wise: `x > 0 ? x : (e ^ x) - 1`.
 *
 * ```js
 * const x = tf.tensor1d([-1, 1, -3, 2]);
 *
 * x.elu().print();  // or tf.elu(x)
 * ```
 * @param x The input tensor.
 *
 * @doc {heading: 'Operations', subheading: 'Basic math'}
 */
function elu_(x) {
	const $x = convertToTensor(x, 'x', 'elu', 'float32');
	const inputs = {x: $x};
	return ENGINE.runKernel(Elu, inputs);
}

const elu$1 = /* @__PURE__ */ op({elu_});

/**
 * @license
 * Copyright 2023 Google LLC.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Checks the input tensor mathes the given shape.
 *
 * Given an input tensor, returns a new tensor with the same values as the
 * input tensor with shape `shape`.
 *
 * The method supports the null value in tensor. It will still check the shapes,
 * and null is a placeholder.
 *
 *
 * ```js
 * const x = tf.tensor1d([1, 2, 3, 4]);
 * const y = tf.tensor1d([1, null, 3, 4]);
 * const z = tf.tensor2d([1, 2, 3, 4], [2,2]);
 * tf.ensureShape(x, [4]).print();
 * tf.ensureShape(y, [4]).print();
 * tf.ensureShape(z, [null, 2]).print();
 * ```
 *
 * @param x The input tensor to be ensured.
 * @param shape A TensorShape representing the shape of this tensor, an array
 *     or null.
 *
 * @doc {heading: 'Tensors', subheading: 'Transformations'}
 */
function ensureShape_(x, shape) {
	const $x = convertToTensor(x, 'x', 'ensureShape', 'string_or_numeric');
	if (!arraysEqualWithNull($x.shape, shape)) {
		throw new Error(`EnsureShape: Shape of tensor ${$x.shape} is not compatible with expected shape ${shape}`);
	}
	return x;
}

const ensureShape = /* @__PURE__ */ op({ensureShape_});

/**
 * @license
 * Copyright 2018 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Computes Gauss error function of the input `tf.Tensor` element-wise:
 * `erf(x)`
 *
 * ```js
 * const x = tf.tensor1d([0, .1, -.1, .7]);
 *
 * x.erf().print(); // or tf.erf(x);
 * ```
 * @param x The input tensor.
 *
 * @doc {heading: 'Operations', subheading: 'Basic math'}
 */
function erf_(x) {
	let $x = convertToTensor(x, 'x', 'erf');
	assert($x.dtype === 'int32' || $x.dtype === 'float32', () => 'Input dtype must be `int32` or `float32`.');
	if ($x.dtype === 'int32') {
		$x = cast$1($x, 'float32');
	}
	const inputs = {x: $x};
	return ENGINE.runKernel(Erf, inputs);
}

const erf = /* @__PURE__ */ op({erf_});

/**
 * @license
 * Copyright 2017 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Returns true if the axis specifies the inner most dimensions of the
 * array.
 */
function axesAreInnerMostDims(axes, rank) {
	for (let i = 0; i < axes.length; ++i) {
		if (axes[axes.length - i - 1] !== rank - 1 - i) {
			return false;
		}
	}
	return true;
}

function combineLocations(outputLoc, reduceLoc, axes) {
	const rank = outputLoc.length + reduceLoc.length;
	const loc = [];
	let outIdx = 0;
	let reduceIdx = 0;
	for (let dim = 0; dim < rank; dim++) {
		if (axes.indexOf(dim) === -1) {
			loc.push(outputLoc[outIdx++]);
		} else {
			loc.push(reduceLoc[reduceIdx++]);
		}
	}
	return loc;
}

function computeOutAndReduceShapes(aShape, axes) {
	const outShape = [];
	const rank = aShape.length;
	for (let dim = 0; dim < rank; dim++) {
		if (axes.indexOf(dim) === -1) {
			outShape.push(aShape[dim]);
		}
	}
	const reduceShape = axes.map(dim => aShape[dim]);
	return [outShape, reduceShape];
}

function expandShapeToKeepDim(shape, axes) {
	const reduceSubShape = axes.map(x => 1);
	return combineLocations(shape, reduceSubShape, axes);
}

function assertAxesAreInnerMostDims(msg, axes, rank) {
	assert(axesAreInnerMostDims(axes, rank), () => `${msg} supports only inner-most axes for now. ` +
		`Got axes ${axes} and rank-${rank} input.`);
}

/**
 * Returns the axes permutation to be used with `tf.transpose`, if such
 * permutation is necessary. Otherwise it returns null. This method is used by
 * operations that operate only on inner-most axes.
 */
function getAxesPermutation(axes, rank) {
	if (axesAreInnerMostDims(axes, rank)) {
		return null;
	}
	const result = [];
	for (let i = 0; i < rank; ++i) {
		if (axes.indexOf(i) === -1) {
			result.push(i);
		}
	}
	axes.forEach(axis => result.push(axis));
	return result;
}

/** Returns the axes permutation that undoes the original permutation. */
function getUndoAxesPermutation(axes) {
	return axes.map((axis, i) => [i, axis])
			   .sort((a, b) => a[1] - b[1])
			   .map(x => x[0]);
}

function getInnerMostAxes(numAxes, rank) {
	const res = [];
	for (let i = rank - numAxes; i < rank; ++i) {
		res.push(i);
	}
	return res;
}

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Computes the maximum of elements across dimensions of a `tf.Tensor`.
 *
 * Reduces the input along the dimensions given in `axes`. Unless `keepDims`
 * is true, the rank of the `tf.Tensor` is reduced by 1 for each entry in
 * `axes`. If `keepDims` is true, the reduced dimensions are retained with
 * length 1. If `axes` has no entries, all dimensions are reduced, and a
 * `tf.Tensor` with a single element is returned.
 *
 * ```js
 * const x = tf.tensor1d([1, 2, 3]);
 *
 * x.max().print();  // or tf.max(x)
 * ```
 *
 * ```js
 * const x = tf.tensor2d([1, 2, 3, 4], [2, 2]);
 *
 * const axis = 1;
 * x.max(axis).print();  // or tf.max(x, axis)
 * ```
 *
 * @param x The input tensor.
 * @param axis The dimension(s) to reduce. By default it reduces
 *     all dimensions.
 * @param keepDims If true, retains reduced dimensions with size 1.
 *
 * @doc {heading: 'Operations', subheading: 'Reduction'}
 */
function max_(x, axis = null, keepDims = false) {
	const $x = convertToTensor(x, 'x', 'max');
	const inputs = {x: $x};
	const attrs = {reductionIndices: axis, keepDims};
	return ENGINE.runKernel(Max, inputs, attrs);
}

const max = /* @__PURE__ */ op({max_});

/**
 * @license
 * Copyright 2020 Google Inc. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Computes the minimum value from the input.
 *
 * Reduces the input along the dimensions given in `axes`. Unless `keepDims`
 * is true, the rank of the array is reduced by 1 for each entry in `axes`.
 * If `keepDims` is true, the reduced dimensions are retained with length 1.
 * If `axes` has no entries, all dimensions are reduced, and an array with a
 * single element is returned.
 *
 * ```js
 * const x = tf.tensor1d([1, 2, 3]);
 *
 * x.min().print();  // or tf.min(x)
 * ```
 *
 * ```js
 * const x = tf.tensor2d([1, 2, 3, 4], [2, 2]);
 *
 * const axis = 1;
 * x.min(axis).print();  // or tf.min(x, axis)
 * ```
 *
 * @param x The input Tensor.
 * @param axis The dimension(s) to reduce. By default it reduces
 *     all dimensions.
 * @param keepDims If true, retains reduced dimensions with size 1.
 *
 * @doc {heading: 'Operations', subheading: 'Reduction'}
 */
function min_(x, axis = null, keepDims = false) {
	const $x = convertToTensor(x, 'x', 'min');
	const inputs = {x: $x};
	const attrs = {axis, keepDims};
	// tslint:disable-next-line: no-unnecessary-type-assertion
	return ENGINE.runKernel(Min, inputs, attrs);
}

const min = /* @__PURE__ */ op({min_});

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Computes the power of one `tf.Tensor` to another. Supports broadcasting.
 *
 * Given a `tf.Tensor` x and a `tf.Tensor` y, this operation computes x^y for
 * corresponding elements in x and y. The result's dtype will be the upcasted
 * type of the `base` and `exp` dtypes.
 *
 * ```js
 * const a = tf.tensor([[2, 3], [4, 5]])
 * const b = tf.tensor([[1, 2], [3, 0]]).toInt();
 *
 * a.pow(b).print();  // or tf.pow(a, b)
 * ```
 *
 * ```js
 * const a = tf.tensor([[1, 2], [3, 4]])
 * const b = tf.tensor(2).toInt();
 *
 * a.pow(b).print();  // or tf.pow(a, b)
 * ```
 * We also expose `powStrict` which has the same signature as this op and
 * asserts that `base` and `exp` are the same shape (does not broadcast).
 *
 * @param base The base `tf.Tensor` to pow element-wise.
 * @param exp The exponent `tf.Tensor` to pow element-wise.
 *
 * @doc {heading: 'Operations', subheading: 'Arithmetic'}
 */
function pow_(base, exp) {
	let $base = convertToTensor(base, 'base', 'pow');
	let $exp = convertToTensor(exp, 'exp', 'pow');
	[$base, $exp] = makeTypesMatch($base, $exp);
	const inputs = {a: $base, b: $exp};
	return ENGINE.runKernel(Pow, inputs);
}

const pow = /* @__PURE__ */ op({pow_});

/**
 * @license
 * Copyright 2018 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Creates rank-0 `tf.Tensor` (scalar) with the provided value and dtype.
 *
 * The same functionality can be achieved with `tf.tensor`, but in general
 * we recommend using `tf.scalar` as it makes the code more readable.
 *
 * ```js
 * tf.scalar(3.14).print();
 * ```
 *
 * @param value The value of the scalar.
 * @param dtype The data type.
 *
 * @doc {heading: 'Tensors', subheading: 'Creation'}
 */
function scalar(value, dtype) {
	if (((isTypedArray(value) && dtype !== 'string') || Array.isArray(value)) &&
		dtype !== 'complex64') {
		throw new Error('Error creating a new Scalar: value must be a primitive ' +
			'(number|boolean|string)');
	}
	if (dtype === 'string' && isTypedArray(value) &&
		!(value instanceof Uint8Array)) {
		throw new Error('When making a scalar from encoded string, ' +
			'the value must be `Uint8Array`.');
	}
	const shape = [];
	const inferredShape = [];
	return makeTensor(value, shape, inferredShape, dtype);
}

/**
 * @license
 * Copyright 2018 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Computes square root of the input `tf.Tensor` element-wise: `y = sqrt(x)`
 *
 * ```js
 * const x = tf.tensor1d([1, 2, 4, -1]);
 *
 * x.sqrt().print();  // or tf.sqrt(x)
 * ```
 * @param x The input tensor.
 *
 * @doc {heading: 'Operations', subheading: 'Basic math'}
 */
function sqrt_(x) {
	const $x = convertToTensor(x, 'x', 'sqrt', 'float32');
	const inputs = {x: $x};
	return ENGINE.runKernel(Sqrt, inputs);
}

const sqrt$1 = /* @__PURE__ */ op({sqrt_});

/**
 * @license
 * Copyright 2019 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Computes square of `x` element-wise: `x ^ 2`
 *
 * ```js
 * const x = tf.tensor1d([1, 2, Math.sqrt(2), -1]);
 *
 * x.square().print();  // or tf.square(x)
 * ```
 * @param x The input Tensor.
 *
 * @doc {heading: 'Operations', subheading: 'Basic math'}
 */
function square_(x) {
	const $x = convertToTensor(x, 'x', 'square');
	const attrs = {};
	return ENGINE.runKernel('Square', {x: $x}, attrs);
}

const square = /* @__PURE__ */ op({square_});

/**
 * @license
 * Copyright 2018 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Computes the sum of elements across dimensions of a `tf.Tensor`.
 *
 * Reduces the input along the dimensions given in `axes`. Unless `keepDims`
 * is true, the rank of the `tf.Tensor` is reduced by 1 for each entry in
 * `axes`. If `keepDims` is true, the reduced dimensions are retained with
 * length 1. If axes has no entries, all dimensions are reduced, and a
 * `tf.Tensor` with a single element is returned.
 *
 * ```js
 * const x = tf.tensor1d([1, 2, 3]);
 *
 * x.sum().print();  // or tf.sum(x)
 * ```
 *
 * ```js
 * const x = tf.tensor2d([1, 2, 3, 4], [2, 2]);
 *
 * const axis = 1;
 * x.sum(axis).print();  // or tf.sum(x, axis)
 * ```
 *
 * @param x The input tensor to compute the sum over. If the dtype is `bool`
 *   it will be converted to `int32` and the output dtype will be `int32`.
 * @param axis The dimension(s) to reduce. By default it reduces
 *     all dimensions.
 * @param keepDims If true, retains reduced dimensions with size 1.
 *
 * @doc {heading: 'Operations', subheading: 'Reduction'}
 */
function sum_(x, axis = null, keepDims = false) {
	let $x = convertToTensor(x, 'x', 'sum');
	if ($x.dtype === 'bool') {
		$x = cast$1($x, 'int32');
	}
	const inputs = {x: $x};
	const attrs = {axis, keepDims};
	return ENGINE.runKernel(Sum, inputs, attrs);
}

const sum$1 = /* @__PURE__ */ op({sum_});

/**
 * @license
 * Copyright 2018 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Computes the norm of scalar, vectors, and matrices.
 * This function can compute several different vector norms (the 1-norm, the
 * Euclidean or 2-norm, the inf-norm, and in general the p-norm for p > 0)
 * and matrix norms (Frobenius, 1-norm, and inf-norm).
 *
 * ```js
 * const x = tf.tensor1d([1, 2, 3, 4]);
 *
 * x.norm().print();  // or tf.norm(x)
 * ```
 *
 * @param x The input array.
 * @param ord Optional. Order of the norm. Supported norm types are
 * following:
 *
 *  | ord        | norm for matrices         | norm for vectors
 *  |------------|---------------------------|---------------------
 *  |'euclidean' |Frobenius norm             |2-norm
 *  |'fro'       |Frobenius norm	           |
 *  |Infinity    |max(sum(abs(x), axis=1))   |max(abs(x))
 *  |-Infinity   |min(sum(abs(x), axis=1))   |min(abs(x))
 *  |1           |max(sum(abs(x), axis=0))   |sum(abs(x))
 *  |2           |                           |sum(abs(x)^2)^(1/2)
 *
 * @param axis Optional. If axis is null (the default), the input is
 * considered a vector and a single vector norm is computed over the entire
 * set of values in the Tensor, i.e. norm(x, ord) is equivalent
 * to norm(x.reshape([-1]), ord). If axis is an integer, the input
 * is considered a batch of vectors, and axis determines the axis in x
 * over which to compute vector norms. If axis is a 2-tuple of integer it is
 * considered a batch of matrices and axis determines the axes in NDArray
 * over which to compute a matrix norm.
 * @param keepDims Optional. If true, the norm has the same dimensionality
 * as the input.
 *
 * @doc {heading: 'Operations', subheading: 'Matrices'}
 */
function norm_(x, ord = 'euclidean', axis = null, keepDims = false) {
	x = convertToTensor(x, 'x', 'norm');
	const norm = normImpl(x, ord, axis);
	let keepDimsShape = norm.shape;
	if (keepDims) {
		const axes = parseAxisParam(axis, x.shape);
		keepDimsShape = expandShapeToKeepDim(norm.shape, axes);
	}
	return reshape$1(norm, keepDimsShape);
}

function normImpl(x, p, axis = null) {
	if (x.rank === 0) {
		return abs$1(x);
	}
	// consider vector when no axis is specified
	if (x.rank !== 1 && axis === null) {
		return normImpl(reshape$1(x, [-1]), p, axis);
	}
	// vector
	if (x.rank === 1 || typeof axis === 'number' ||
		Array.isArray(axis) && axis.length === 1) {
		if (p === 1) {
			return sum$1(abs$1(x), axis);
		}
		if (p === Infinity) {
			return max(abs$1(x), axis);
		}
		if (p === -Infinity) {
			return min(abs$1(x), axis);
		}
		if (p === 'euclidean' || p === 2) {
			// norm(x, 2) = sum(abs(xi) ^ 2) ^ 1/2
			return sqrt$1(sum$1(pow(abs$1(x), scalar(2, 'int32')), axis));
		}
		throw new Error(`Error in norm: invalid ord value: ${p}`);
	}
	// matrix (assumption axis[0] < axis[1])
	if (Array.isArray(axis) && axis.length === 2) {
		if (p === 1) {
			return max(sum$1(abs$1(x), axis[0]), axis[1] - 1);
		}
		if (p === Infinity) {
			return max(sum$1(abs$1(x), axis[1]), axis[0]);
		}
		if (p === -Infinity) {
			return min(sum$1(abs$1(x), axis[1]), axis[0]);
		}
		if (p === 'fro' || p === 'euclidean') {
			// norm(x) = sqrt(sum(pow(x, 2)))
			return sqrt$1(sum$1(square(x), axis));
		}
		throw new Error(`Error in norm: invalid ord value: ${p}`);
	}
	throw new Error(`Error in norm: invalid axis: ${axis}`);
}

const norm = /* @__PURE__ */ op({norm_});

/**
 * @license
 * Copyright 2022 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Computes the Euclidean norm of scalar, vectors, and matrices.
 *
 * ```js
 * const x = tf.tensor1d([1, 2, 3, 4]);
 *
 * x.euclideanNorm().print();  // or tf.euclideanNorm(x)
 * ```
 *
 * @param x The input array.
 * @param axis Optional. If axis is null (the default), the input is
 * considered a vector and a single vector norm is computed over the entire
 * set of values in the Tensor, i.e. euclideanNorm(x) is equivalent
 * to euclideanNorm(x.reshape([-1])). If axis is an integer, the input
 * is considered a batch of vectors, and axis determines the axis in x
 * over which to compute vector norms. If axis is a 2-tuple of integer it is
 * considered a batch of matrices and axis determines the axes in NDArray
 * over which to compute a matrix norm.
 * @param keepDims Optional. If true, the norm has the same dimensionality
 * as the input.
 *
 * @doc {heading: 'Operations', subheading: 'Matrices'}
 */
function euclideanNorm_(x, axis = null, keepDims = false) {
	return norm(x, 'euclidean', axis, keepDims);
}

const euclideanNorm = /* @__PURE__ */ op({euclideanNorm_});

/**
 * @license
 * Copyright 2018 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Computes exponential of the input `tf.Tensor` element-wise. `e ^ x`
 *
 * ```js
 * const x = tf.tensor1d([1, 2, -3]);
 *
 * x.exp().print();  // or tf.exp(x)
 * ```
 * @param x The input tensor.
 *
 * @doc {heading: 'Operations', subheading: 'Basic math'}
 */
function exp_(x) {
	const $x = convertToTensor(x, 'x', 'exp');
	const inputs = {x: $x};
	return ENGINE.runKernel(Exp, inputs);
}

const exp$1 = /* @__PURE__ */ op({exp_});

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Returns a `tf.Tensor` that has expanded rank, by inserting a dimension
 * into the tensor's shape.
 *
 * ```js
 * const x = tf.tensor1d([1, 2, 3, 4]);
 * const axis = 1;
 * x.expandDims(axis).print();
 * ```
 *
 * @param x The input tensor whose dimensions are to be expanded.
 * @param axis The dimension index at which to insert shape of `1`. Defaults
 *     to 0 (the first dimension).
 *
 * @doc {heading: 'Tensors', subheading: 'Transformations'}
 */
function expandDims_(x, axis = 0) {
	const $x = convertToTensor(x, 'x', 'expandDims', 'string_or_numeric');
	assert(axis <= $x.rank, () => 'Axis must be <= rank of the tensor');
	const inputs = {input: $x};
	const attrs = {dim: axis};
	return ENGINE.runKernel(ExpandDims, inputs, attrs);
}

const expandDims = /* @__PURE__ */ op({expandDims_});

/**
 * @license
 * Copyright 2018 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Computes exponential of the input `tf.Tensor` minus one element-wise.
 * `e ^ x - 1`
 *
 * ```js
 * const x = tf.tensor1d([1, 2, -3]);
 *
 * x.expm1().print();  // or tf.expm1(x)
 * ```
 * @param x The input tensor.
 *
 * @doc {heading: 'Operations', subheading: 'Basic math'}
 */
function expm1_(x) {
	const $x = convertToTensor(x, 'x', 'expm1');
	const inputs = {x: $x};
	return ENGINE.runKernel(Expm1, inputs);
}

const expm1 = /* @__PURE__ */ op({expm1_});

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Construct a tensor by repeating it the number of times given by reps.
 *
 * This operation creates a new tensor by replicating `input` `reps`
 * times. The output tensor's `i`th dimension has `input.shape[i] *
 * reps[i]` elements, and the values of `input` are replicated
 * `reps[i]` times along the `i`th dimension. For example, tiling
 * `[a, b, c, d]` by `[2]` produces `[a, b, c, d, a, b, c, d]`.
 *
 * ```js
 * const a = tf.tensor1d([1, 2]);
 *
 * a.tile([2]).print();    // or tf.tile(a, [2])
 * ```
 *
 * ```js
 * const a = tf.tensor2d([1, 2, 3, 4], [2, 2]);
 *
 * a.tile([1, 2]).print();  // or tf.tile(a, [1,2])
 * ```
 * @param x The tensor to tile.
 * @param reps Determines the number of replications per dimension.
 *
 * @doc {heading: 'Tensors', subheading: 'Slicing and Joining'}
 */
function tile_(x, reps) {
	const $x = convertToTensor(x, 'x', 'tile', 'string_or_numeric');
	assert($x.rank === reps.length, () => `Error in transpose: rank of input ${$x.rank} ` +
		`must match length of reps ${reps}.`);
	const inputs = {x: $x};
	const attrs = {reps};
	return ENGINE.runKernel(Tile, inputs, attrs);
}

const tile = /* @__PURE__ */ op({tile_});

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Create an identity matrix.
 *
 * @param numRows Number of rows.
 * @param numColumns Number of columns. Defaults to `numRows`.
 * @param batchShape If provided, will add the batch shape to the beginning
 *   of the shape of the returned `tf.Tensor` by repeating the identity
 *   matrix.
 * @param dtype Data type.
 * @returns Identity matrix of the specified size and data type, possibly
 *   with batch repetition if `batchShape` is specified.
 *
 * @doc {heading: 'Tensors', subheading: 'Creation'}
 */
function eye_(numRows, numColumns, batchShape, dtype = 'float32') {
	if (numColumns == null) {
		numColumns = numRows;
	}
	const buff = buffer([numRows, numColumns], dtype);
	const n = numRows <= numColumns ? numRows : numColumns;
	for (let i = 0; i < n; ++i) {
		buff.set(1, i, i);
	}
	const out = reshape$1(buff.toTensor(), [numRows, numColumns]);
	if (batchShape == null) {
		return out;
	} else {
		if (batchShape.length === 1) {
			return tile(expandDims(out, 0), [batchShape[0], 1, 1]);
		} else if (batchShape.length === 2) {
			// tslint:disable-next-line:no-unnecessary-type-assertion
			return tile(expandDims(expandDims(out, 0), 0), [batchShape[0], batchShape[1], 1, 1]);
		} else if (batchShape.length === 3) {
			// tslint:disable-next-line:no-unnecessary-type-assertion
			return tile(expandDims(expandDims(expandDims(out, 0), 0), 0), [
				batchShape[0], batchShape[1], batchShape[2], 1, 1
			]);
		} else {
			throw new Error(`eye() currently supports only 1D and 2D ` +
				// tslint:disable-next-line:no-any
				`batchShapes, but received ${batchShape.length}D.`);
		}
	}
}

const eye = /* @__PURE__ */ op({eye_});

/**
 * @license
 * Copyright 2018 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Computes floor of input `tf.Tensor` element-wise: `floor(x)`.
 *
 * ```js
 * const x = tf.tensor1d([.6, 1.1, -3.3]);
 *
 * x.floor().print();  // or tf.floor(x)
 * ```
 * @param x The input tensor.
 *
 * @doc {heading: 'Operations', subheading: 'Basic math'}
 */
function floor_(x) {
	const $x = convertToTensor(x, 'x', 'floor', 'float32');
	const inputs = {x: $x};
	return ENGINE.runKernel(Floor, inputs);
}

const floor$1 = /* @__PURE__ */ op({floor_});

/**
 * @license
 * Copyright 2018 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Gather slices from tensor `x`'s axis `axis` according to `indices`.
 *
 * ```js
 * const x = tf.tensor1d([1, 2, 3, 4]);
 * const indices = tf.tensor1d([1, 3, 3], 'int32');
 *
 * x.gather(indices).print();
 * ```
 *
 * ```js
 * const x = tf.tensor2d([1, 2, 3, 4], [2, 2]);
 * const indices = tf.tensor1d([1, 1, 0], 'int32');
 *
 * x.gather(indices).print();
 * ```
 * @param x The input tensor whose slices are to be gathered.
 * @param indices The indices of the values to extract.
 * @param axis The axis over which to select values. Defaults to 0.
 * @param batchDims Optional. The number of batch dimensions. It must be less
 *     than or equal to rank(indices). Defaults to 0.
 *     The output tensor will have shape of
 *     `x.shape[:axis] + indices.shape[batchDims:] + x.shape[axis + 1:]`
 *
 * @doc {heading: 'Tensors', subheading: 'Slicing and Joining'}
 */
function gather_(x, indices, axis = 0, batchDims = 0) {
	const $x = convertToTensor(x, 'x', 'gather');
	const $indices = convertToTensor(indices, 'indices', 'gather', 'int32');
	const inputs = {x: $x, indices: $indices};
	const attrs = {axis, batchDims};
	return ENGINE.runKernel(GatherV2, inputs, attrs);
}

const gather = /* @__PURE__ */ op({gather_});

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Returns the truth value of (a > b) element-wise. Supports broadcasting.
 *
 * ```js
 * const a = tf.tensor1d([1, 2, 3]);
 * const b = tf.tensor1d([2, 2, 2]);
 *
 * a.greater(b).print();
 * ```
 *
 * @param a The first input tensor.
 * @param b The second input tensor. Must have the same dtype as `a`.
 *
 * @doc {heading: 'Operations', subheading: 'Logical'}
 */
function greater_(a, b) {
	let $a = convertToTensor(a, 'a', 'greater', 'string_or_numeric');
	let $b = convertToTensor(b, 'b', 'greater', 'string_or_numeric');
	[$a, $b] = makeTypesMatch($a, $b);
	assertAndGetBroadcastShape($a.shape, $b.shape);
	const inputs = {a: $a, b: $b};
	return ENGINE.runKernel(Greater, inputs);
}

const greater$1 = /* @__PURE__ */ op({greater_});

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Returns the truth value of (a >= b) element-wise. Supports broadcasting.
 *
 * ```js
 * const a = tf.tensor1d([1, 2, 3]);
 * const b = tf.tensor1d([2, 2, 2]);
 *
 * a.greaterEqual(b).print();
 * ```
 *
 * @param a The first input tensor.
 * @param b The second input tensor. Must have the same dtype as `a`.
 *
 * @doc {heading: 'Operations', subheading: 'Logical'}
 */
function greaterEqual_(a, b) {
	let $a = convertToTensor(a, 'a', 'greaterEqual', 'string_or_numeric');
	let $b = convertToTensor(b, 'b', 'greaterEqual', 'string_or_numeric');
	[$a, $b] = makeTypesMatch($a, $b);
	assertAndGetBroadcastShape($a.shape, $b.shape);
	const inputs = {a: $a, b: $b};
	return ENGINE.runKernel(GreaterEqual, inputs);
}

const greaterEqual$1 = /* @__PURE__ */ op({greaterEqual_});

/**
 * @license
 * Copyright 2018 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Returns which elements of x are finite.
 *
 * ```js
 * const x = tf.tensor1d([NaN, Infinity, -Infinity, 0, 1]);
 *
 * x.isFinite().print();  // or tf.isNaN(x)
 * ```
 * @param x The input Tensor.
 *
 * @doc {heading: 'Operations', subheading: 'Basic math'}
 */
function isFinite_(x) {
	const $x = convertToTensor(x, 'x', 'isFinite');
	const inputs = {x: $x};
	return ENGINE.runKernel(IsFinite, inputs);
}

const isFinite$1 = /* @__PURE__ */ op({isFinite_});

/**
 * @license
 * Copyright 2018 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Returns which elements of x are Infinity or -Infinity.
 *
 * ```js
 * const x = tf.tensor1d([NaN, Infinity, -Infinity, 0, 1]);
 *
 * x.isInf().print();  // or tf.isNaN(x)
 * ```
 * @param x The input Tensor.
 *
 * @doc {heading: 'Operations', subheading: 'Basic math'}
 */
function isInf_(x) {
	const $x = convertToTensor(x, 'x', 'isInf');
	const inputs = {x: $x};
	return ENGINE.runKernel(IsInf, inputs);
}

const isInf = /* @__PURE__ */ op({isInf_});

/**
 * @license
 * Copyright 2018 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Returns which elements of x are NaN.
 *
 * ```js
 * const x = tf.tensor1d([NaN, Infinity, -Infinity, 0, 1]);
 *
 * x.isNaN().print();  // or tf.isNaN(x)
 * ```
 * @param x The input Tensor.
 *
 * @doc {heading: 'Operations', subheading: 'Basic math'}
 */
function isNaN_(x) {
	const $x = convertToTensor(x, 'x', 'isNaN');
	const inputs = {x: $x};
	return ENGINE.runKernel(IsNan, inputs);
}

const isNaN$1 = /* @__PURE__ */ op({isNaN_});

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Computes leaky rectified linear element-wise.
 *
 * See
 * [http://web.stanford.edu/~awni/papers/relu_hybrid_icml2013_final.pdf](
 *     http://web.stanford.edu/~awni/papers/relu_hybrid_icml2013_final.pdf)
 *
 * ```js
 * const x = tf.tensor1d([-1, 2, -3, 4]);
 *
 * x.leakyRelu(0.1).print();  // or tf.leakyRelu(x, 0.1)
 * ```
 * @param x The input tensor.
 * @param alpha The scaling factor for negative values, defaults to 0.2.
 *
 * @doc {heading: 'Operations', subheading: 'Basic math'}
 */
function leakyRelu_(x, alpha = 0.2) {
	const $x = convertToTensor(x, 'x', 'leakyRelu');
	const inputs = {x: $x};
	const attrs = {alpha};
	return ENGINE.runKernel(LeakyRelu, inputs, attrs);
}

const leakyRelu$1 = /* @__PURE__ */ op({leakyRelu_});

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Returns the truth value of (a < b) element-wise. Supports broadcasting.
 *
 * ```js
 * const a = tf.tensor1d([1, 2, 3]);
 * const b = tf.tensor1d([2, 2, 2]);
 *
 * a.less(b).print();
 * ```
 * @param a The first input tensor.
 * @param b The second input tensor. Must have the same dtype as `a`.
 *
 * @doc {heading: 'Operations', subheading: 'Logical'}
 */
function less_(a, b) {
	let $a = convertToTensor(a, 'a', 'less', 'string_or_numeric');
	let $b = convertToTensor(b, 'b', 'less', 'string_or_numeric');
	[$a, $b] = makeTypesMatch($a, $b);
	assertAndGetBroadcastShape($a.shape, $b.shape);
	const inputs = {a: $a, b: $b};
	return ENGINE.runKernel(Less, inputs);
}

const less = /* @__PURE__ */ op({less_});

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Returns the truth value of (a <= b) element-wise. Supports broadcasting.
 *
 * ```js
 * const a = tf.tensor1d([1, 2, 3]);
 * const b = tf.tensor1d([2, 2, 2]);
 *
 * a.lessEqual(b).print();
 * ```
 *
 * @param a The first input tensor.
 * @param b The second input tensor. Must have the same dtype as `a`.
 *
 * @doc {heading: 'Operations', subheading: 'Logical'}
 */
function lessEqual_(a, b) {
	let $a = convertToTensor(a, 'a', 'lessEqual', 'string_or_numeric');
	let $b = convertToTensor(b, 'b', 'lessEqual', 'string_or_numeric');
	[$a, $b] = makeTypesMatch($a, $b);
	assertAndGetBroadcastShape($a.shape, $b.shape);
	const inputs = {a: $a, b: $b};
	return ENGINE.runKernel(LessEqual, inputs);
}

const lessEqual$1 = /* @__PURE__ */ op({lessEqual_});

/**
 * @license
 * Copyright 2018 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Return an evenly spaced sequence of numbers over the given interval.
 *
 * ```js
 * tf.linspace(0, 9, 10).print();
 * ```
 * @param start The start value of the sequence.
 * @param stop The end value of the sequence.
 * @param num The number of values to generate.
 *
 * @doc {heading: 'Tensors', subheading: 'Creation'}
 */
function linspace(start, stop, num) {
	if (num <= 0) {
		throw new Error('The number of values should be positive.');
	}
	const attrs = {start, stop, num};
	return ENGINE.runKernel(LinSpace, {}, attrs);
}

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Normalizes the activation of a local neighborhood across or within
 * channels.
 *
 * @param x The input tensor. The 4-D input tensor is treated as a 3-D array
 *     of 1D vectors (along the last dimension), and each vector is
 *     normalized independently.
 * @param depthRadius The number of adjacent channels in the 1D normalization
 *     window.
 * @param bias A constant bias term for the basis.
 * @param alpha A scale factor, usually positive.
 * @param beta An exponent.
 *
 * @doc {heading: 'Operations', subheading: 'Normalization'}
 */
function localResponseNormalization_(x, depthRadius = 5, bias = 1, alpha = 1, beta = 0.5) {
	const $x = convertToTensor(x, 'x', 'localResponseNormalization');
	assert($x.rank === 4 || $x.rank === 3, () => `Error in localResponseNormalization: x must be rank 3 or 4 but got
               rank ${$x.rank}.`);
	assert(isInt(depthRadius), () => `Error in localResponseNormalization: depthRadius must be an ` +
		`integer but got depthRadius ${depthRadius}.`);
	let x4D = $x;
	let reshapedTo4D = false;
	if ($x.rank === 3) {
		reshapedTo4D = true;
		x4D = reshape$1($x, [1, $x.shape[0], $x.shape[1], $x.shape[2]]);
	}
	const inputs = {x: x4D};
	const attrs = {depthRadius, bias, alpha, beta};
	// tslint:disable-next-line: no-unnecessary-type-assertion
	const res = ENGINE.runKernel(LRN, inputs, attrs);
	if (reshapedTo4D) {
		return reshape$1(res, [res.shape[1], res.shape[2], res.shape[3]]);
	} else {
		return res;
	}
}

const localResponseNormalization = /* @__PURE__ */ op({localResponseNormalization_});

/**
 * @license
 * Copyright 2018 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Computes natural logarithm of the input `tf.Tensor` element-wise: `ln(x)`
 *
 * ```js
 * const x = tf.tensor1d([1, 2, Math.E]);
 *
 * x.log().print();  // or tf.log(x)
 * ```
 * @param x The input tensor.
 *
 * @doc {heading: 'Operations', subheading: 'Basic math'}
 */
function log_(x) {
	const $x = convertToTensor(x, 'x', 'log', 'float32');
	const inputs = {x: $x};
	return ENGINE.runKernel(Log, inputs);
}

const log$1 = /* @__PURE__ */ op({log_});

/**
 * @license
 * Copyright 2018 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Computes natural logarithm of the input `tf.Tensor` plus one
 * element-wise: `ln(1 + x)`
 *
 * ```js
 * const x = tf.tensor1d([1, 2, Math.E - 1]);
 *
 * x.log1p().print();  // or tf.log1p(x)
 * ```
 * @param x The input tensor.
 *
 * @doc {heading: 'Operations', subheading: 'Basic math'}
 */
function log1p_(x) {
	const $x = convertToTensor(x, 'x', 'log1p');
	const inputs = {x: $x};
	return ENGINE.runKernel(Log1p, inputs);
}

const log1p$1 = /* @__PURE__ */ op({log1p_});

/**
 * @license
 * Copyright 2018 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Provided `f(x)`, returns another function `g(x, dy?)`, which gives the
 * gradient of `f(x)` with respect to `x`.
 *
 * If `dy` is provided, the gradient of `f(x).mul(dy).sum()` with respect to
 * `x` is computed instead. `f(x)` must take a single tensor `x` and return a
 * single tensor `y`. If `f()` takes multiple inputs, use `tf.grads` instead.
 *
 * ```js
 * // f(x) = x ^ 2
 * const f = x => x.square();
 * // f'(x) = 2x
 * const g = tf.grad(f);
 *
 * const x = tf.tensor1d([2, 3]);
 * g(x).print();
 * ```
 *
 * ```js
 * // f(x) = x ^ 3
 * const f = x => x.pow(tf.scalar(3, 'int32'));
 * // f'(x) = 3x ^ 2
 * const g = tf.grad(f);
 * // f''(x) = 6x
 * const gg = tf.grad(g);
 *
 * const x = tf.tensor1d([2, 3]);
 * gg(x).print();
 * ```
 *
 * @param f The function f(x), to compute gradient for.
 *
 * @doc {heading: 'Training', subheading: 'Gradients'}
 */
function grad(f) {
	assert(isFunction(f), () => 'The f passed in grad(f) must be a function');
	return (x, dy) => {
		// x can be of any dtype, thus null as the last argument.
		const $x = convertToTensor(x, 'x', 'tf.grad', 'string_or_numeric');
		const $dy = (dy != null) ? convertToTensor(dy, 'dy', 'tf.grad') : null;
		return ENGINE.tidy(() => {
			const {value, grads} = ENGINE.gradients(() => f($x), [$x], $dy);
			if ($dy != null) {
				assertShapesMatch(value.shape, $dy.shape, 'The shape of dy passed in grad(f)(x, dy) must match the shape ' +
					'returned by f(x)');
			}
			checkGrads(grads);
			return grads[0];
		});
	};
}

/**
 * Provided `f(x1, x2,...)`, returns another function `g([x1, x2,...], dy?)`,
 * which gives an array of gradients of `f()` with respect to each input
 * [`x1`,`x2`,...].
 *
 * If `dy` is passed when calling `g()`, the gradient of
 * `f(x1,...).mul(dy).sum()` with respect to each input is computed instead.
 * The provided `f` must take one or more tensors and return a single tensor
 * `y`. If `f()` takes a single input, we recommend using `tf.grad` instead.
 *
 * ```js
 * // f(a, b) = a * b
 * const f = (a, b) => a.mul(b);
 * // df / da = b, df / db = a
 * const g = tf.grads(f);
 *
 * const a = tf.tensor1d([2, 3]);
 * const b = tf.tensor1d([-2, -3]);
 * const [da, db] = g([a, b]);
 * console.log('da');
 * da.print();
 * console.log('db');
 * db.print();
 * ```
 *
 * @param f The function `f(x1, x2,...)` to compute gradients for.
 *
 * @doc {heading: 'Training', subheading: 'Gradients'}
 */
function grads(f) {
	assert(isFunction(f), () => 'The f passed in grads(f) must be a function');
	return (args, dy) => {
		assert(Array.isArray(args), () => 'The args passed in grads(f)(args) must be an array ' +
			'of `Tensor`s or `TensorLike`s');
		// args can be of any dtype, thus null as the last argument.
		const $args = convertToTensorArray(args, 'args', 'tf.grads', 'string_or_numeric');
		const $dy = (dy != null) ? convertToTensor(dy, 'dy', 'tf.grads') : null;
		return ENGINE.tidy(() => {
			const {value, grads} = ENGINE.gradients(() => f(...$args), $args, $dy);
			if ($dy != null) {
				assertShapesMatch(value.shape, $dy.shape, 'The shape of dy passed in grads(f)([x1,...], dy) must ' +
					'match the shape returned by f([x1,...])');
			}
			checkGrads(grads);
			return grads;
		});
	};
}

/**
 * Like `tf.grad`, but also returns the value of `f()`. Useful when `f()`
 * returns a metric you want to show.
 *
 * The result is a rich object with the following properties:
 * - grad: The gradient of `f(x)` w.r.t. `x` (result of `tf.grad`).
 * - value: The value returned by `f(x)`.
 *
 * ```js
 * // f(x) = x ^ 2
 * const f = x => x.square();
 * // f'(x) = 2x
 * const g = tf.valueAndGrad(f);
 *
 * const x = tf.tensor1d([2, 3]);
 * const {value, grad} = g(x);
 *
 * console.log('value');
 * value.print();
 * console.log('grad');
 * grad.print();
 * ```
 *
 * @doc {heading: 'Training', subheading: 'Gradients'}
 */
function valueAndGrad(f) {
	assert(isFunction(f), () => 'The f passed in valueAndGrad(f) must be a function');
	return (x, dy) => {
		assert(x instanceof Tensor, () => 'The x passed in valueAndGrad(f)(x) must be a tensor');
		assert(dy == null || dy instanceof Tensor, () => 'The dy passed in valueAndGrad(f)(x, dy) must be a tensor');
		const {grads, value} = ENGINE.gradients(() => f(x), [x], dy);
		checkGrads(grads);
		return {grad: grads[0], value};
	};
}

/**
 * Like `tf.grads`, but returns also the value of `f()`. Useful when `f()`
 * returns a metric you want to show.
 *
 * The result is a rich object with the following properties:
 * - grads: The gradients of `f()` w.r.t. each input (result of `tf.grads`).
 * - value: The value returned by `f(x)`.
 *
 * ```js
 * // f(a, b) = a * b
 * const f = (a, b) => a.mul(b);
 * // df/da = b, df/db = a
 * const g = tf.valueAndGrads(f);
 *
 * const a = tf.tensor1d([2, 3]);
 * const b = tf.tensor1d([-2, -3]);
 * const {value, grads} = g([a, b]);
 *
 * const [da, db] = grads;
 *
 * console.log('value');
 * value.print();
 *
 * console.log('da');
 * da.print();
 * console.log('db');
 * db.print();
 * ```
 *
 * @doc {heading: 'Training', subheading: 'Gradients'}
 */
function valueAndGrads(f) {
	assert(isFunction(f), () => 'The f passed in valueAndGrads(f) must be a function');
	return (args, dy) => {
		assert(Array.isArray(args) && args.every(arg => arg instanceof Tensor), () => 'The args passed in valueAndGrads(f)(args) must be array of ' +
			'tensors');
		assert(dy == null || dy instanceof Tensor, () => 'The dy passed in valueAndGrads(f)(args, dy) must be a tensor');
		const res = ENGINE.gradients(() => f(...args), args, dy);
		if (dy != null) {
			assertShapesMatch(res.value.shape, dy.shape, 'The shape of dy passed in valueAndGrads(f)([x1,...], dy) must ' +
				'match the shape returned by f([x1,...])');
		}
		checkGrads(res.grads);
		return res;
	};
}

/**
 * Computes and returns the gradient of f(x) with respect to the list of
 * trainable variables provided by `varList`. If no list is provided, it
 * defaults to all trainable variables.
 *
 * ```js
 * const a = tf.variable(tf.tensor1d([3, 4]));
 * const b = tf.variable(tf.tensor1d([5, 6]));
 * const x = tf.tensor1d([1, 2]);
 *
 * // f(a, b) = a * x ^ 2 + b * x
 * const f = () => a.mul(x.square()).add(b.mul(x)).sum();
 * // df/da = x ^ 2, df/db = x
 * const {value, grads} = tf.variableGrads(f);
 *
 * Object.keys(grads).forEach(varName => grads[varName].print());
 * ```
 *
 * @param f The function to execute. f() should return a scalar.
 * @param varList The list of variables to compute the gradients with respect
 *     to. Defaults to all trainable variables.
 * @returns An object with the following keys and values:
 *   - `value`: The value of the function `f`.
 *   - `grads`: A map from the names of the variables to the gradients.
 *     If the `varList` argument is provided explicitly and contains a subset of
 *     non-trainable variables, this map in the return value will contain keys
 *     that map the names of the non-trainable variables to `null`.
 *
 * @doc {heading: 'Training', subheading: 'Gradients'}
 */
function variableGrads(f, varList) {
	assert(isFunction(f), () => 'The f passed in variableGrads(f) must be a function');
	assert(varList == null ||
		Array.isArray(varList) && varList.every(v => v instanceof Variable), () => 'The varList passed in variableGrads(f, varList) must be an array ' +
		'of variables');
	const specifiedVarList = varList != null;
	if (!specifiedVarList) {
		// Get all of the trainable variables.
		varList = [];
		for (const varName in ENGINE.registeredVariables) {
			varList.push(ENGINE.registeredVariables[varName]);
		}
	}
	const specifiedNonTrainable = specifiedVarList ? varList.filter(variable => !variable.trainable) : null;
	// Prune non-trainable variables.
	const originalVarCount = varList.length;
	varList = varList.filter(variable => variable.trainable);
	assert(varList.length > 0, () => `variableGrads() expects at least one of the input variables to ` +
		`be trainable, but none of the ${originalVarCount} variables is ` +
		`trainable.`);
	const allowNoGradients = true;
	const {value, grads} = ENGINE.gradients(f, varList, null, allowNoGradients);
	assert(grads.some(g => g != null), () => 'Cannot find a connection between any variable and the result of ' +
		'the loss function y=f(x). Please make sure the operations that ' +
		'use variables are inside the function f passed to minimize().');
	assert(value.rank === 0, () => `The f passed in variableGrads(f) must return a scalar, but it ` +
		`returned a rank-${value.rank} tensor`);
	const namedGrads = {};
	varList.forEach((v, i) => {
		if (grads[i] != null) {
			namedGrads[v.name] = grads[i];
		}
	});
	if (specifiedNonTrainable != null) {
		// If varList is explicitly provided and contains non-trainable values,
		// add them to the returned gradients with `null` values.
		specifiedNonTrainable.forEach(v => namedGrads[v.name] = null);
	}
	return {value, grads: namedGrads};
}

/**
 * Overrides the gradient computation of a function `f`.
 *
 * Takes a function
 * `f(...inputs, save) => {value: Tensor, gradFunc: (dy, saved) => Tensor[]}`
 * and returns another function `g(...inputs)` which takes the same inputs as
 * `f`. When called, `g` returns `f().value`. In backward mode, custom gradients
 * with respect to each input of `f` are computed using `f().gradFunc`.
 *
 * The `save` function passed to `f` should be used for saving tensors needed
 * in the gradient. And the `saved` passed to the `gradFunc` is a
 * `NamedTensorMap`, which contains those saved tensors.
 *
 * ```js
 * const customOp = tf.customGrad((x, save) => {
 *   // Save x to make sure it's available later for the gradient.
 *   save([x]);
 *   // Override gradient of our custom x ^ 2 op to be dy * abs(x);
 *   return {
 *     value: x.square(),
 *     // Note `saved.x` which points to the `x` we saved earlier.
 *     gradFunc: (dy, saved) => [dy.mul(saved[0].abs())]
 *   };
 * });
 *
 * const x = tf.tensor1d([-1, -2, 3]);
 * const dx = tf.grad(x => customOp(x));
 *
 * console.log(`f(x):`);
 * customOp(x).print();
 * console.log(`f'(x):`);
 * dx(x).print();
 * ```
 *
 * @param f The function to evaluate in forward mode, which should return
 *     `{value: Tensor, gradFunc: (dy, saved) => Tensor[]}`, where `gradFunc`
 *     returns the custom gradients of `f` with respect to its inputs.
 *
 * @doc {heading: 'Training', subheading: 'Gradients'}
 */
function customGrad(f) {
	return ENGINE.customGrad(f);
}

function checkGrads(grads) {
	const numNullGradients = grads.filter(g => g == null).length;
	if (numNullGradients > 0) {
		throw new Error(`Cannot compute gradient of y=f(x) with respect to x. Make sure that
    the f you passed encloses all operations that lead from x to y.`);
	}
}

/**
 * @license
 * Copyright 2018 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Computes softplus of the input `tf.Tensor` element-wise: `log(exp(x) + 1)`
 *
 * ```js
 * const x = tf.tensor1d([0, 1, -1, .7]);
 *
 * x.softplus().print();  // or tf.softplus(x)
 * ```
 * @param x The input tensor.
 *
 * @doc {heading: 'Operations', subheading: 'Basic math'}
 */
function softplus_(x) {
	const $x = convertToTensor(x, 'x', 'softplus');
	const inputs = {x: $x};
	return ENGINE.runKernel(Softplus, inputs);
}

const softplus = /* @__PURE__ */ op({softplus_});

/**
 * @license
 * Copyright 2018 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Computes log sigmoid of the input `tf.Tensor` element-wise:
 * `logSigmoid(x)`. For numerical stability, we use `-tf.softplus(-x)`.
 *
 * ```js
 * const x = tf.tensor1d([0, 1, -1, .7]);
 *
 * x.logSigmoid().print();  // or tf.logSigmoid(x)
 * ```
 * @param x The input tensor.
 *
 * @doc {heading: 'Operations', subheading: 'Basic math'}
 */
function logSigmoid_(x) {
	const $x = convertToTensor(x, 'x', 'logSigmoid');
	// Use a custom gradient to maintain previous implementation.
	// There is no LogSigmoid kernel in TF so we can't use engine.runKernel
	// directly
	const customOp = customGrad((x) => {
		// TODO(yassogba) we can remove the chained softplus call here only
		// after backends have modualrized softplus at which point we can call
		// engine runKernel(..., Sotfplus, ...) directly.
		const value = neg$1(softplus(neg$1(x)));
		const gradFunc = (dy) => {
			const derX = mul(dy, sigmoid$1(neg$1(x)));
			return derX;
		};
		return {value, gradFunc};
	});
	return customOp($x);
}

const logSigmoid = /* @__PURE__ */ op({logSigmoid_});

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Subtracts two `tf.Tensor`s element-wise, A - B. Supports broadcasting.
 *
 * ```js
 * const a = tf.tensor1d([10, 20, 30, 40]);
 * const b = tf.tensor1d([1, 2, 3, 4]);
 *
 * a.sub(b).print();  // or tf.sub(a, b)
 * ```
 *
 * ```js
 * // Broadcast subtract a with b.
 * const a = tf.tensor1d([10, 20, 30, 40]);
 * const b = tf.scalar(5);
 *
 * a.sub(b).print();  // or tf.sub(a, b)
 * ```
 * @param a The first `tf.Tensor` to subtract from.
 * @param b The second `tf.Tensor` to be subtracted. Must have the same dtype as
 * `a`.
 *
 * @doc {heading: 'Operations', subheading: 'Arithmetic'}
 */
function sub_(a, b) {
	let $a = convertToTensor(a, 'a', 'sub');
	let $b = convertToTensor(b, 'b', 'sub');
	[$a, $b] = makeTypesMatch($a, $b);
	const inputs = {a: $a, b: $b};
	return ENGINE.runKernel(Sub, inputs);
}

const sub$1 = /* @__PURE__ */ op({sub_});

/**
 * @license
 * Copyright 2020 Google Inc. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Computes the log softmax.
 *
 * ```js
 * const a = tf.tensor1d([1, 2, 3]);
 *
 * a.logSoftmax().print();  // or tf.logSoftmax(a)
 * ```
 *
 * ```js
 * const a = tf.tensor2d([2, 4, 6, 1, 2, 3], [2, 3]);
 *
 * a.logSoftmax().print();  // or tf.logSoftmax(a)
 * ```
 *
 * @param logits The logits array.
 * @param axis The dimension softmax would be performed on. Defaults to `-1`
 *     which indicates the last dimension.
 *
 * @doc {heading: 'Operations', subheading: 'Normalization'}
 */
function logSoftmax_(logits, axis = -1) {
	const $logits = convertToTensor(logits, 'logits', 'logSoftmax');
	if (axis === -1) {
		axis = $logits.rank - 1;
	}
	if (axis !== $logits.rank - 1) {
		throw Error('Log Softmax along a non-last dimension is not yet supported. ' +
			`Logits was rank ${$logits.rank} and axis was ${axis}`);
	}
	// const forward: ForwardFunc<Tensor> = (backend, save) => {
	//   const keepDims = true;
	//   const xMax = max(logits, axis, true);
	//   const shifted = sub(logits, xMax);
	//   const value =
	//       sub(cast(shifted, 'float32'), log(sum(exp(shifted), axis,
	//       keepDims)));
	//   save([value]);
	//   return value;
	// };
	// Use a custom gradient for numerical stability.
	const customOp = customGrad((logits, save) => {
		const keepDims = true;
		const xMax = max(logits, axis, true);
		const shifted = sub$1(logits, xMax);
		const value = sub$1(cast$1(shifted, 'float32'), log$1(sum$1(exp$1(shifted), axis, keepDims)));
		save([value]);
		const gradFunc = (dy, saved) => {
			const [value] = saved;
			const keepDims = true;
			const softmax = exp$1(value);
			return sub$1(dy, mul(sum$1(dy, axis, keepDims), softmax));
		};
		return {value, gradFunc};
	});
	return customOp($logits);
	// TODO Use Engine.runKernel when CPU/WebGL/WASM backends implement this.
	// const inputs: LogSoftmaxInputs = {logits: $logits};
	// const attrs: LogSoftmaxAttrs = {axis};
	// return ENGINE.runKernel(
	//            LogSoftmax, inputs as unknown as NamedTensorMap,
	//            attrs as unknown as NamedAttrMap);
}

const logSoftmax = /* @__PURE__ */ op({logSoftmax_});

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Computes the log(sum(exp(elements across the reduction dimensions))).
 *
 * Reduces the input along the dimensions given in `axis`. Unless `keepDims`
 * is true, the rank of the array is reduced by 1 for each entry in `axis`.
 * If `keepDims` is true, the reduced dimensions are retained with length 1.
 * If `axis` has no entries, all dimensions are reduced, and an array with a
 * single element is returned.
 *
 * ```js
 * const x = tf.tensor1d([1, 2, 3]);
 *
 * x.logSumExp().print();  // or tf.logSumExp(x)
 * ```
 *
 * ```js
 * const x = tf.tensor2d([1, 2, 3, 4], [2, 2]);
 *
 * const axis = 1;
 * x.logSumExp(axis).print();  // or tf.logSumExp(a, axis)
 * ```
 * @param x The input tensor.
 * @param axis The dimension(s) to reduce. If null (the default),
 *     reduces all dimensions.
 * @param keepDims If true, retains reduced dimensions with length
 *     of 1. Defaults to false.
 *
 * @doc {heading: 'Operations', subheading: 'Reduction'}
 */
function logSumExp_(x, axis = null, keepDims = false) {
	const $x = convertToTensor(x, 'x', 'logSumExp');
	const axes = parseAxisParam(axis, $x.shape);
	const xMax = max($x, axes, true /* keepDims */);
	const a = sub$1($x, xMax);
	const b = exp$1(a);
	const c = sum$1(b, axes);
	const d = log$1(c);
	const res = add$1(reshape$1(xMax, d.shape), d);
	if (keepDims) {
		const newShape = expandShapeToKeepDim(res.shape, axes);
		return reshape$1(res, newShape);
	}
	return res;
}

const logSumExp = /* @__PURE__ */ op({logSumExp_});

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Returns the truth value of `a AND b` element-wise. Supports broadcasting.
 *
 * ```js
 * const a = tf.tensor1d([false, false, true, true], 'bool');
 * const b = tf.tensor1d([false, true, false, true], 'bool');
 *
 * a.logicalAnd(b).print();
 * ```
 *
 * @param a The first input tensor. Must be of dtype bool.
 * @param b The second input tensor. Must be of dtype bool.
 *
 * @doc {heading: 'Operations', subheading: 'Logical'}
 */
function logicalAnd_(a, b) {
	const $a = convertToTensor(a, 'a', 'logicalAnd', 'bool');
	const $b = convertToTensor(b, 'b', 'logicalAnd', 'bool');
	assertAndGetBroadcastShape($a.shape, $b.shape);
	const inputs = {a: $a, b: $b};
	return ENGINE.runKernel(LogicalAnd, inputs);
}

const logicalAnd$1 = /* @__PURE__ */ op({logicalAnd_});

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Returns the truth value of `NOT x` element-wise.
 *
 * ```js
 * const a = tf.tensor1d([false, true], 'bool');
 *
 * a.logicalNot().print();
 * ```
 *
 * @param x The input tensor. Must be of dtype 'bool'.
 *
 * @doc {heading: 'Operations', subheading: 'Logical'}
 */
function logicalNot_(x) {
	const $x = convertToTensor(x, 'x', 'logicalNot', 'bool');
	const inputs = {x: $x};
	return ENGINE.runKernel(LogicalNot, inputs);
}

const logicalNot = /* @__PURE__ */ op({logicalNot_});

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Returns the truth value of `a OR b` element-wise. Supports broadcasting.
 *
 * ```js
 * const a = tf.tensor1d([false, false, true, true], 'bool');
 * const b = tf.tensor1d([false, true, false, true], 'bool');
 *
 * a.logicalOr(b).print();
 * ```
 * @param a The first input tensor. Must be of dtype bool.
 * @param b The second input tensor. Must be of dtype bool.
 *
 * @doc {heading: 'Operations', subheading: 'Logical'}
 */
function logicalOr_(a, b) {
	const $a = convertToTensor(a, 'a', 'logicalOr', 'bool');
	const $b = convertToTensor(b, 'b', 'logicalOr', 'bool');
	assertAndGetBroadcastShape($a.shape, $b.shape);
	const inputs = {a: $a, b: $b};
	return ENGINE.runKernel(LogicalOr, inputs);
}

const logicalOr = /* @__PURE__ */ op({logicalOr_});

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Returns the truth value of `a XOR b` element-wise. Supports broadcasting.
 *
 * ```js
 * const a = tf.tensor1d([false, false, true, true], 'bool');
 * const b = tf.tensor1d([false, true, false, true], 'bool');
 *
 * a.logicalXor(b).print();
 * ```
 *
 * @param a The first input tensor. Must be of dtype bool.
 * @param b The second input tensor. Must be of dtype bool.
 *
 * @doc {heading: 'Operations', subheading: 'Logical'}
 */
function logicalXor_(a, b) {
	const $a = convertToTensor(a, 'a', 'logicalXor', 'bool');
	const $b = convertToTensor(b, 'b', 'logicalXor', 'bool');
	assertAndGetBroadcastShape($a.shape, $b.shape);
	// x ^ y = (x | y) & ~(x & y)
	return logicalAnd$1(logicalOr(a, b), logicalNot(logicalAnd$1(a, b)));
}

const logicalXor = /* @__PURE__ */ op({logicalXor_});

/**
 * @license
 * Copyright 2022 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
const INT32_MAX$1 = 2147483648;

/**
 * Searches for where a value would go in a sorted sequence.
 *
 * This is not a method for checking containment (like javascript in).
 *
 * The typical use case for this operation is "binning", "bucketing", or
 * "discretizing". The values are assigned to bucket-indices based on the edges
 * listed in 'sortedSequence'. This operation returns the bucket-index for each
 * value.
 *
 * The side argument controls which index is returned if a value lands exactly
 * on an edge.
 *
 * The axis is not settable for this operation. It always operates on the
 * innermost dimension (axis=-1). The operation will accept any number of outer
 * dimensions.
 *
 * Note: This operation assumes that 'sortedSequence' is sorted along the
 * innermost axis, maybe using 'sort(..., axis=-1)'. If the sequence is not
 * sorted no error is raised and the content of the returned tensor is not well
 * defined.
 *
 * ```js
 * const edges = tf.tensor1d([-1, 3.3, 9.1, 10.0]);
 * let values = tf.tensor1d([0.0, 4.1, 12.0]);
 * const result1 = tf.searchSorted(edges, values, 'left');
 * result1.print(); // [1, 2, 4]
 *
 * const seq = tf.tensor1d([0, 3, 9, 10, 10]);
 * values = tf.tensor1d([0, 4, 10]);
 * const result2 = tf.searchSorted(seq, values, 'left');
 * result2.print(); // [0, 2, 3]
 * const result3 = tf.searchSorted(seq, values, 'right');
 * result3.print(); // [1, 2, 5]
 *
 * const sortedSequence = tf.tensor2d([[0., 3., 8., 9., 10.],
 *                                     [1., 2., 3., 4., 5.]]);
 * values = tf.tensor2d([[9.8, 2.1, 4.3],
 *                       [0.1, 6.6, 4.5, ]]);
 * const result4 = tf.searchSorted(sortedSequence, values, 'left');
 * result4.print(); // [[4, 1, 2], [0, 5, 4]]
 * ```
 * @param sortedSequence: N-D. Sorted sequence.
 * @param values: N-D. Search values.
 * @param side: 'left'|'right'. Defaults to 'left'. 'left' corresponds to lower
 *     bound and 'right' to upper bound.
 * @return An N-D int32 tensor the size of values containing the result of
 *     applying either lower bound or upper bound (depending on side) to each
 *     value. The result is not a global index to the entire Tensor, but the
 *     index in the last dimension.
 * @doc {heading: 'Operations', subheading: 'Evaluation'}
 */
function searchSorted_(sortedSequence, values, side = 'left') {
	const $sortedSequence = convertToTensor(sortedSequence, 'sortedSequence', 'searchSorted');
	const $values = convertToTensor(values, 'values', 'searchSorted');
	const sequenceSize = $sortedSequence.shape[$sortedSequence.shape.length - 1];
	const valuesSize = $values.shape[$values.shape.length - 1];
	const $sortedSequence2D = reshape$1($sortedSequence, [-1, sequenceSize]);
	const $values2D = reshape$1($values, [-1, valuesSize]);
	if ($sortedSequence2D.rank < 2) {
		throw new Error(`Sorted input argument must be at least 2-dimensional`);
	}
	if ($sortedSequence2D.shape[0] !== $values2D.shape[0]) {
		throw new Error(`Leading dimension of 'sortedSequence' and 'values' must match.`);
	}
	if (sizeFromShape($values2D.shape) >= INT32_MAX$1) {
		throw new Error(`values tensor size must less than ${INT32_MAX$1}`);
	}
	if ($sortedSequence2D.shape[1] >= INT32_MAX$1) {
		throw new Error(`trailing dim_size must less than ${INT32_MAX$1} for int32 output type, was ${$sortedSequence2D.shape[1]}`);
	}
	const inputs = {
		sortedSequence: $sortedSequence2D,
		values: $values2D,
	};
	const attrs = {side};
	return ENGINE.runKernel(SearchSorted, inputs, attrs);
}

const searchSorted = /* @__PURE__ */ op({searchSorted_});

/**
 * @license
 * Copyright 2022 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Searches for where a value would go in a sorted sequence.
 *
 * This is not a method for checking containment (like javascript in).
 *
 * The typical use case for this operation is "binning", "bucketing", or
 * "discretizing". The values are assigned to bucket-indices based on the edges
 * listed in 'sortedSequence'. This operation returns the bucket-index for each
 * value.
 *
 * The index returned corresponds to the first edge greater than or equal to the
 * value.
 *
 * The axis is not settable for this operation. It always operates on the
 * innermost dimension (axis=-1). The operation will accept any number of outer
 * dimensions.
 *
 * Note: This operation assumes that 'lowerBound' is sorted along the
 * innermost axis, maybe using 'sort(..., axis=-1)'. If the sequence is not
 * sorted no error is raised and the content of the returned tensor is not well
 * defined.
 *
 * ```js
 * const edges = tf.tensor1d([-1, 3.3, 9.1, 10.0]);
 * let values = tf.tensor1d([0.0, 4.1, 12.0]);
 * const result1 = tf.lowerBound(edges, values);
 * result1.print(); // [1, 2, 4]
 *
 * const seq = tf.tensor1d([0, 3, 9, 10, 10]);
 * values = tf.tensor1d([0, 4, 10]);
 * const result2 = tf.lowerBound(seq, values);
 * result2.print(); // [0, 2, 3]
 *
 * const sortedSequence = tf.tensor2d([[0., 3., 8., 9., 10.],
 *                                     [1., 2., 3., 4., 5.]]);
 * values = tf.tensor2d([[9.8, 2.1, 4.3],
 *                       [0.1, 6.6, 4.5, ]]);
 * const result3 = tf.lowerBound(sortedSequence, values);
 * result3.print(); // [[4, 1, 2], [0, 5, 4]]
 * ```
 * @param sortedSequence: N-D. Sorted sequence.
 * @param values: N-D. Search values.
 * @return An N-D int32 tensor the size of values containing the result of
 *     applying lower bound to each value. The result is not a global index to
 *     the entire Tensor, but the index in the last dimension.
 * @doc {heading: 'Operations', subheading: 'Evaluation'}
 */
function lowerBound(sortedSequence, values) {
	return searchSorted(sortedSequence, values, 'left');
}

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Computes the 2D max pooling of an image.
 *
 * @param x The input tensor, of rank 4 or rank 3 of shape
 *     `[batch, height, width, inChannels]`. If rank 3, batch of 1 is assumed.
 * @param filterSize The filter size: `[filterHeight, filterWidth]`. If
 *     `filterSize` is a single number, then `filterHeight == filterWidth`.
 * @param strides The strides of the pooling: `[strideHeight, strideWidth]`. If
 *     `strides` is a single number, then `strideHeight == strideWidth`.
 * @param dilations The dilation rates: `[dilationHeight, dilationWidth]`
 *     in which we sample input values across the height and width dimensions
 *     in dilated pooling. Defaults to `[1, 1]`. If `dilations` is a single
 *     number, then `dilationHeight == dilationWidth`. If it is greater than
 *     1, then all values of `strides` must be 1.
 * @param pad The type of padding algorithm.
 *    - `same` and stride 1: output will be of same size as input,
 *       regardless of filter size.
 *    - `valid`: output will be smaller than input if filter is larger
 *       than 1x1.
 *    - For more info, see this guide:
 *     [https://www.tensorflow.org/api_docs/python/tf/nn/convolution](
 *          https://www.tensorflow.org/api_docs/python/tf/nn/convolution)
 * @param dimRoundingMode A string from: 'ceil', 'round', 'floor'. If none is
 *     provided, it will default to truncate.
 */
function maxPool_(x, filterSize, strides, pad, dimRoundingMode) {
	const $x = convertToTensor(x, 'x', 'maxPool');
	const dilations = 1;
	let x4D = $x;
	let reshapedTo4D = false;
	if ($x.rank === 3) {
		reshapedTo4D = true;
		x4D = reshape$1($x, [1, $x.shape[0], $x.shape[1], $x.shape[2]]);
	}
	assert(x4D.rank === 4, () => `Error in maxPool: input must be rank 4 but got rank ${x4D.rank}.`);
	assert(eitherStridesOrDilationsAreOne(strides, dilations), () => 'Error in maxPool: Either strides or dilations must be 1. ' +
		`Got strides ${strides} and dilations '${dilations}'`);
	checkPadOnDimRoundingMode('maxPool', pad, dimRoundingMode);
	const inputs = {x: x4D};
	const attrs = {filterSize, strides, pad, dimRoundingMode};
	// tslint:disable-next-line: no-unnecessary-type-assertion
	const res = ENGINE.runKernel(MaxPool, inputs, attrs);
	if (reshapedTo4D) {
		return reshape$1(res, [res.shape[1], res.shape[2], res.shape[3]]);
	}
	return res;
}

const maxPool = /* @__PURE__ */ op({maxPool_});

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Computes the 3D max pooling.
 *
 * ```js
 * const x = tf.tensor5d([1, 2, 3, 4, 5, 6, 7, 8], [1, 2, 2, 2, 1]);
 * const result = tf.maxPool3d(x, 2, 1, 'valid');
 * result.print();
 * ```
 *
 * @param x The input tensor, of rank 5 or rank 4 of shape
 *     `[batch, depth, height, width, inChannels]`.
 * @param filterSize The filter size:
 *     `[filterDepth, filterHeight, filterWidth]`.
 *     If `filterSize` is a single number,
 *     then `filterDepth == filterHeight == filterWidth`.
 * @param strides The strides of the pooling:
 *     `[strideDepth, strideHeight, strideWidth]`.
 *     If `strides` is a single number,
 *     then `strideDepth == strideHeight == strideWidth`.
 * @param pad The type of padding algorithm.
 *    - `same` and stride 1: output will be of same size as input,
 *       regardless of filter size.
 *    - `valid`: output will be smaller than input if filter is larger
 *       than 1*1x1.
 *    - For more info, see this guide:
 *     [https://www.tensorflow.org/api_docs/python/tf/nn/convolution](
 *          https://www.tensorflow.org/api_docs/python/tf/nn/convolution)
 * @param dimRoundingMode A string from: 'ceil', 'round', 'floor'. If none is
 *     provided, it will default to truncate.
 * @param dataFormat An optional string from: "NDHWC", "NCDHW". Defaults to
 *     "NDHWC". Specify the data format of the input and output data. With the
 *     default format "NDHWC", the data is stored in the order of: [batch,
 *     depth, height, width, channels]. Only "NDHWC" is currently supported.
 * @doc {heading: 'Operations', subheading: 'Convolution'}
 */
function maxPool3d_(x, filterSize = [1, 1, 1], strides, pad, dimRoundingMode, dataFormat = 'NDHWC') {
	const $x = convertToTensor(x, 'x', 'maxPool3d');
	let x5D = $x;
	let reshapedTo5D = false;
	if ($x.rank === 4) {
		reshapedTo5D = true;
		x5D = reshape$1($x, [1, $x.shape[0], $x.shape[1], $x.shape[2], $x.shape[3]]);
	}
	assert(x5D.rank === 5, () => `Error in maxPool3d: x must be rank 5 but got rank ${x5D.rank}.`);
	assert(dataFormat === 'NDHWC', () => `Error in maxPool3d: Only NDHWC is currently supported, ` +
		`but got dataFormat of ${dataFormat}`);
	checkPadOnDimRoundingMode('maxPool3d', pad, dimRoundingMode);
	const inputs = {x: x5D};
	const attrs = {filterSize, strides, pad, dimRoundingMode, dataFormat};
	// tslint:disable-next-line: no-unnecessary-type-assertion
	const res = ENGINE.runKernel(MaxPool3D, inputs, attrs);
	if (reshapedTo5D) {
		return reshape$1(res, [res.shape[1], res.shape[2], res.shape[3], res.shape[4]]);
	}
	return res;
}

const maxPool3d = /* @__PURE__ */ op({maxPool3d_});

/**
 * @license
 * Copyright 2018 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Computes the 2D max pooling of an image with Argmax index.
 * The indices in argmax are flattened, so that a maximum value at position `[b,
 * y, x, c]` becomes flattened index: `(y * width + x) * channels + c` if
 * include_batch_in_index is False; `((b * height + y) * width + x) * channels
 * +c` if include_batch_in_index is True.
 *
 * The indices returned are always in `[0, height) x [0, width)` before
 * flattening.
 *
 * @param x The input tensor, of rank 4 or rank 3 of shape
 *     `[batch, height, width, inChannels]`. If rank 3, batch of 1 is assumed.
 * @param filterSize The filter size: `[filterHeight, filterWidth]`. If
 *     `filterSize` is a single number, then `filterHeight == filterWidth`.
 * @param strides The strides of the pooling: `[strideHeight, strideWidth]`. If
 *     `strides` is a single number, then `strideHeight == strideWidth`.
 * @param dataFormat An optional string from: "NDHWC", "NCDHW". Defaults to
 *     "NDHWC". Specify the data format of the input and output data. With the
 *     default format "NDHWC", the data is stored in the order of: [batch,
 *     depth, height, width, channels]. Only "NDHWC" is currently supported.
 * @param pad The type of padding algorithm.
 *    - `same` and stride 1: output will be of same size as input,
 *       regardless of filter size.
 *    - `valid`: output will be smaller than input if filter is larger
 *       than 1x1.
 *    - For more info, see this guide:
 *     [https://www.tensorflow.org/api_docs/python/tf/nn/convolution](
 *          https://www.tensorflow.org/api_docs/python/tf/nn/convolution)
 * @param includeBatchIndex Defaults to False. Whether to include batch
 *    dimension in flattened index of argmax.
 *
 * @doc {heading: 'Operations', subheading: 'Convolution'}
 */
function maxPoolWithArgmax_(x, filterSize, strides, pad, includeBatchInIndex = false) {
	const $x = convertToTensor(x, 'x', 'maxPoolWithArgmax');
	const inputs = {x: $x};
	const attrs = {filterSize, strides, pad, includeBatchInIndex};
	// tslint:disable-next-line: no-unnecessary-type-assertion
	const result = ENGINE.runKernel(MaxPoolWithArgmax, inputs, attrs);
	return {result: result[0], indexes: result[1]};
}

const maxPoolWithArgmax = /* @__PURE__ */ op({maxPoolWithArgmax_});

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Returns the max of a and b (`a > b ? a : b`) element-wise.
 * Supports broadcasting.
 *
 * We also expose `tf.maximumStrict` which has the same signature as this op and
 * asserts that `a` and `b` are the same shape (does not broadcast).
 *
 * ```js
 * const a = tf.tensor1d([1, 4, 3, 16]);
 * const b = tf.tensor1d([1, 2, 9, 4]);
 *
 * a.maximum(b).print();  // or tf.maximum(a, b)
 * ```
 *
 * ```js
 * // Broadcast maximum a with b.
 * const a = tf.tensor1d([2, 4, 6, 8]);
 * const b = tf.scalar(5);
 *
 * a.maximum(b).print();  // or tf.maximum(a, b)
 * ```
 *
 * @param a The first tensor.
 * @param b The second tensor. Must have the same type as `a`.
 *
 * @doc {heading: 'Operations', subheading: 'Arithmetic'}
 */
function maximum_(a, b) {
	let $a = convertToTensor(a, 'a', 'maximum');
	let $b = convertToTensor(b, 'b', 'maximum');
	[$a, $b] = makeTypesMatch($a, $b);
	if ($a.dtype === 'bool') {
		$a = cast$1($a, 'int32');
		$b = cast$1($b, 'int32');
	}
	assertAndGetBroadcastShape($a.shape, $b.shape);
	const inputs = {a: $a, b: $b};
	return ENGINE.runKernel(Maximum, inputs);
}

const maximum = /* @__PURE__ */ op({maximum_});

/**
 * @license
 * Copyright 2020 Google Inc. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Computes the mean of elements across dimensions of a `tf.Tensor`.
 *
 * Reduces `x` along the dimensions given in `axis`. Unless `keepDims` is
 * true, the rank of the `tf.Tensor` is reduced by 1 for each entry in `axis`.
 * If `keepDims` is true, the reduced dimensions are retained with length 1.
 * If `axis` has no entries, all dimensions are reduced, and a `tf.Tensor` with
 * a single element is returned.
 *
 * ```js
 * const x = tf.tensor1d([1, 2, 3]);
 *
 * x.mean().print();  // or tf.mean(a)
 * ```
 *
 * ```js
 * const x = tf.tensor2d([1, 2, 3, 4], [2, 2]);
 *
 * const axis = 1;
 * x.mean(axis).print();  // or tf.mean(x, axis)
 * ```
 *
 * @param x The input tensor.
 * @param axis The dimension(s) to reduce. By default it reduces
 *     all dimensions.
 * @param keepDims If true, retains reduced dimensions with size 1.
 *
 * @doc {heading: 'Operations', subheading: 'Reduction'}
 */
function mean_(x, axis = null, keepDims = false) {
	const $x = convertToTensor(x, 'x', 'mean');
	const inputs = {x: $x};
	const attrs = {axis, keepDims};
	return ENGINE.runKernel(Mean, inputs, attrs);
}

const mean$1 = /* @__PURE__ */ op({mean_});

/**
 * @license
 * Copyright 2018 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Creates a `tf.Tensor` with all elements set to 0.
 *
 * ```js
 * tf.zeros([2, 2]).print();
 * ```
 *
 * @param shape An array of integers defining the output tensor shape.
 * @param dtype The type of an element in the resulting tensor. Can
 *     be 'float32', 'int32' or 'bool'. Defaults to 'float'.
 *
 * @doc {heading: 'Tensors', subheading: 'Creation'}
 */
function zeros$1(shape, dtype = 'float32') {
	assertNonNegativeIntegerDimensions(shape);
	if (dtype === 'complex64') {
		const real = zeros$1(shape, 'float32');
		const imag = zeros$1(shape, 'float32');
		return complex$1(real, imag);
	}
	const values = makeZerosTypedArray(sizeFromShape(shape), dtype);
	return ENGINE.makeTensor(values, shape, dtype);
}

/**
 * @license
 * Copyright 2018 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Creates a `tf.Tensor` with all elements set to 1.
 *
 * ```js
 * tf.ones([2, 2]).print();
 * ```
 *
 * @param shape An array of integers defining the output tensor shape.
 * @param dtype The type of an element in the resulting tensor. Defaults to
 *     'float'.
 *
 * @doc {heading: 'Tensors', subheading: 'Creation'}
 */
function ones(shape, dtype = 'float32') {
	assertNonNegativeIntegerDimensions(shape);
	if (dtype === 'complex64') {
		const real = ones(shape, 'float32');
		const imag = zeros$1(shape, 'float32');
		return complex$1(real, imag);
	}
	const values = makeOnesTypedArray(sizeFromShape(shape), dtype);
	return ENGINE.makeTensor(values, shape, dtype);
}

/**
 * @license
 * Copyright 2021 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Broadcasts parameters for evaluation on an N-D grid.
 *
 * Given N one-dimensional coordinate arrays `*args`, returns a list `outputs`
 * of N-D coordinate arrays for evaluating expressions on an N-D grid.
 *
 * Notes:
 * `meshgrid` supports cartesian ('xy') and matrix ('ij') indexing conventions.
 * When the `indexing` argument is set to 'xy' (the default), the broadcasting
 * instructions for the first two dimensions are swapped.
 * Examples:
 * Calling `const [X, Y] = meshgrid(x, y)` with the tensors
 *
 * ```javascript
 * const x = [1, 2, 3];
 * const y = [4, 5, 6];
 * const [X, Y] = tf.meshgrid(x, y);
 * // X = [[1, 2, 3],
 * //      [1, 2, 3],
 * //      [1, 2, 3]]
 * // Y = [[4, 4, 4],
 * //      [5, 5, 5],
 * //      [6, 6, 6]]
 * ```
 *
 * @param x Tensor with rank geq 1.
 * @param y Tensor with rank geq 1.
 * @param indexing
 *
 * @doc {heading: 'Operations', subheading: 'Slicing and Joining'}
 */
function meshgrid(x, y, {indexing = 'xy'} = {}) {
	if (indexing !== 'xy' && indexing !== 'ij') {
		throw new TypeError(`${indexing} is not a valid third argument to meshgrid`);
	}
	if (x === undefined) {
		return [];
	}
	let $x = convertToTensor(x, 'x', 'meshgrid', x instanceof Tensor ? x.dtype : 'float32');
	if (y === undefined) {
		return [$x];
	}
	let $y = convertToTensor(y, 'y', 'meshgrid', y instanceof Tensor ? y.dtype : 'float32');
	const w = sizeFromShape($x.shape);
	const h = sizeFromShape($y.shape);
	if (indexing === 'xy') {
		$x = reshape$1($x, [1, -1]);
		$y = reshape$1($y, [-1, 1]);
		return [
			matMul$1(ones([h, 1], $x.dtype), $x),
			matMul$1($y, ones([1, w], $y.dtype)),
		];
	}
	$x = reshape$1($x, [-1, 1]);
	$y = reshape$1($y, [1, -1]);
	return [
		matMul$1($x, ones([1, h], $x.dtype)),
		matMul$1(ones([w, 1], $y.dtype), $y),
	];
}

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Returns the min of a and b (`a < b ? a : b`) element-wise.
 * Supports broadcasting.
 *
 * We also expose `minimumStrict` which has the same signature as this op and
 * asserts that `a` and `b` are the same shape (does not broadcast).
 *
 * ```js
 * const a = tf.tensor1d([1, 4, 3, 16]);
 * const b = tf.tensor1d([1, 2, 9, 4]);
 *
 * a.minimum(b).print();  // or tf.minimum(a, b)
 * ```
 *
 * ```js
 * // Broadcast minimum a with b.
 * const a = tf.tensor1d([2, 4, 6, 8]);
 * const b = tf.scalar(5);
 *
 * a.minimum(b).print();  // or tf.minimum(a, b)
 * ```
 *
 * @param a The first tensor.
 * @param b The second tensor. Must have the same type as `a`.
 *
 * @doc {heading: 'Operations', subheading: 'Arithmetic'}
 */
function minimum_(a, b) {
	let $a = convertToTensor(a, 'a', 'minimum');
	let $b = convertToTensor(b, 'b', 'minimum');
	[$a, $b] = makeTypesMatch($a, $b);
	if ($a.dtype === 'bool') {
		$a = cast$1($a, 'int32');
		$b = cast$1($b, 'int32');
	}
	assertAndGetBroadcastShape($a.shape, $b.shape);
	const inputs = {a: $a, b: $b};
	return ENGINE.runKernel(Minimum, inputs);
}

const minimum = /* @__PURE__ */ op({minimum_});

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */

/**
 * Pads a `tf.Tensor` using mirror padding.
 *
 * This operation implements the `REFLECT` and `SYMMETRIC` modes of pad.
 *
 * ```js
 * const x = tf.range(0, 9).reshape([1, 1, 3, 3]);
 * x.mirrorPad([[0, 0], [0, 0], [2, 2], [2, 2]], 'reflect').print();
 * ```
 * @param x The tensor to pad.
 * @param paddings An array of length `R` (the rank of the tensor), where
 * each element is a length-2 tuple of ints `[padBefore, padAfter]`,
 * specifying how much to pad along each dimension of the tensor.
 * In "reflect" mode, the padded regions do not include the borders,
 * while in "symmetric" mode the padded regions do include the borders.
 * For example, if the input is `[1, 2, 3]` and paddings is `[0, 2]`,
 * then the output is `[1, 2, 3, 2, 1]` in "reflect" mode, and
 * `[1, 2, 3, 3, 2]` in "symmetric" mode.
 * If `mode` is "reflect" then both `paddings[D, 0]` and `paddings[D, 1]`
 * must be no greater than `x.shape[D] - 1`. If mode is "symmetric"
 * then both `paddings[D, 0]` and `paddings[D, 1]` must be no greater than
 * `x.shape[D]`
 * @param mode String to specify padding mode. Can be `'reflect' | 'symmetric'`
 */
/** @doc {heading: 'Tensors', subheading: 'Transformations'} */
function mirrorPad_(x, paddings, mode) {
	assert(mode === 'reflect' || mode === 'symmetric', () => `Invalid mode. Mode must be either reflect or symmetric. ` +
		`Got ${mode}.`);
	const $x = convertToTensor(x, 'x', 'mirrorPad');
	if ($x.rank === 0) {
		throw new Error('mirrorPad(scalar) is not defined. ' +
			'Pass non-scalar to mirrorPad');
	}
	assert(paddings.length === $x.rank, () => `Padding doesn't match input. Must be ${$x.rank}. ` +
		`Got ${paddings.length}.`);
	const shapeOffset = mode === 'reflect' ? 1 : 0;
	for (let i = 0; i < $x.rank; i++) {
		assert(paddings[i].length === 2, () => `Invalid number of paddings. Must be length of 2 each.`);
		assert(paddings[i][0] >= 0 && paddings[i][0] <= $x.shape[i] - shapeOffset &&
			paddings[i][1] >= 0 && paddings[i][1] <= $x.shape[i] - shapeOffset, () => `Padding in dimension ${i} cannot be greater than or equal ` +
			`to ${$x.shape[i] - shapeOffset} or less than 0 for input of ` +
			`shape ${$x.shape}`);
	}
	const attrs = {paddings, mode};
	const inputs = {x: $x};
	return ENGINE.runKernel(MirrorPad, inputs, attrs);
}

const mirrorPad = /* @__PURE__ */ op({mirrorPad_});

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Returns the mod of a and b element-wise.
 * `floor(x / y) * y + mod(x, y) = x`
 * Supports broadcasting.
 *
 * We also expose `tf.modStrict` which has the same signature as this op and
 * asserts that `a` and `b` are the same shape (does not broadcast).
 *
 * ```js
 * const a = tf.tensor1d([1, 4, 3, 16]);
 * const b = tf.tensor1d([1, 2, 9, 4]);
 *
 * a.mod(b).print();  // or tf.mod(a, b)
 * ```
 *
 * ```js
 * // Broadcast a mod b.
 * const a = tf.tensor1d([2, 4, 6, 8]);
 * const b = tf.scalar(5);
 *
 * a.mod(b).print();  // or tf.mod(a, b)
 * ```
 *
 * @param a The first tensor.
 * @param b The second tensor. Must have the same type as `a`.
 *
 * @doc {heading: 'Operations', subheading: 'Arithmetic'}
 */
function mod_(a, b) {
	let $a = convertToTensor(a, 'a', 'mod');
	let $b = convertToTensor(b, 'b', 'mod');
	[$a, $b] = makeTypesMatch($a, $b);
	const inputs = {a: $a, b: $b};
	return ENGINE.runKernel(Mod, inputs);
}

const mod = /* @__PURE__ */ op({mod_});

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Calculates the mean and variance of `x`. The mean and variance are
 * calculated by aggregating the contents of `x` across `axes`. If `x` is
 * 1-D and `axes = [0]` this is just the mean and variance of a vector.
 *
 * @param x The input tensor.
 * @param axis The dimension(s) along with to compute mean and
 *     variance. By default it reduces all dimensions.
 * @param keepDims If true, the moments have the same dimensionality as the
 *     input.
 * @return An object with two keys: `mean` and `variance`.
 *
 * @doc {heading: 'Operations', subheading: 'Normalization'}
 */
function moments_(x, axis = null, keepDims = false) {
	x = convertToTensor(x, 'x', 'moments');
	const axes = parseAxisParam(axis, x.shape);
	const xMean = mean$1(x, axes, keepDims);
	let keepDimsShape = xMean.shape;
	if (!keepDims) {
		keepDimsShape = expandShapeToKeepDim(xMean.shape, axes);
	}
	const devSquared = square(sub$1(cast$1(x, 'float32'), reshape$1(xMean, keepDimsShape)));
	const variance = mean$1(devSquared, axes, keepDims);
	return {mean: xMean, variance};
}

const moments = /* @__PURE__ */ op({moments_});

/**
 * Computes the next states and outputs of a stack of LSTMCells.
 *
 * Each cell output is used as input to the next cell.
 *
 * Returns `[cellState, cellOutput]`.
 *
 * Derived from tf.contrib.rn.MultiRNNCell.
 *
 * @param lstmCells Array of LSTMCell functions.
 * @param data The input to the cell.
 * @param c Array of previous cell states.
 * @param h Array of previous cell outputs.
 *
 * @doc {heading: 'Operations', subheading: 'RNN'}
 */
function multiRNNCell_(lstmCells, data, c, h) {
	const $data = convertToTensor(data, 'data', 'multiRNNCell');
	const $c = convertToTensorArray(c, 'c', 'multiRNNCell');
	const $h = convertToTensorArray(h, 'h', 'multiRNNCell');
	let input = $data;
	const newStates = [];
	for (let i = 0; i < lstmCells.length; i++) {
		const output = lstmCells[i](input, $c[i], $h[i]);
		newStates.push(output[0]);
		newStates.push(output[1]);
		input = output[1];
	}
	const newC = [];
	const newH = [];
	for (let i = 0; i < newStates.length; i += 2) {
		newC.push(newStates[i]);
		newH.push(newStates[i + 1]);
	}
	return [newC, newH];
}

const multiRNNCell = /* @__PURE__ */ op({multiRNNCell_});

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Creates a `tf.Tensor` with values drawn from a multinomial distribution.
 *
 * ```js
 * const probs = tf.tensor([.75, .25]);
 * tf.multinomial(probs, 3).print();
 * ```
 *
 * @param logits 1D array with unnormalized log-probabilities, or
 *     2D array of shape `[batchSize, numOutcomes]`. See the `normalized`
 *     parameter.
 * @param numSamples Number of samples to draw for each row slice.
 * @param seed The seed number.
 * @param normalized Whether the provided `logits` are normalized true
 *     probabilities (sum to 1). Defaults to false.
 * @return 1D array of shape `[numSamples]`, or 2D array of shape
 *     `[batchSize, numSamples]`, depending on the rank of the input.
 *
 * @doc {heading: 'Tensors', subheading: 'Random'}
 */
function multinomial_(logits, numSamples, seed, normalized = false) {
	const $logits = convertToTensor(logits, 'logits', 'multinomial');
	const numOutcomes = $logits.size;
	const origRank = $logits.rank;
	if (numOutcomes < 2) {
		throw new Error(`Error in multinomial: you need at least 2 outcomes, but got ` +
			`${numOutcomes}.`);
	}
	if (origRank > 2) {
		throw new Error(`Rank of probabilities must be 1 or 2, but is ${origRank}`);
	}
	// TODO(lina128): Investigate correct seed behavior. The code seems not allow
	// setting see to 0.
	seed = seed || Math.random();
	// The kernel only accepts (and returns) rank 2 tensors.
	const logits2D = origRank === 1 ? reshape$1($logits, [1, -1]) : $logits;
	const inputs = {logits: logits2D};
	const attrs = {numSamples, seed, normalized};
	// tslint:disable-next-line: no-unnecessary-type-assertion
	const res = ENGINE.runKernel(Multinomial, inputs, attrs);
	// tslint:disable-next-line:no-unnecessary-type-assertion
	return origRank === 1 ? reshape$1(res, [res.size]) : res;
}

const multinomial = /* @__PURE__ */ op({multinomial_});

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Returns the truth value of (a != b) element-wise. Supports broadcasting.
 *
 * ```js
 * const a = tf.tensor1d([1, 2, 3]);
 * const b = tf.tensor1d([0, 2, 3]);
 *
 * a.notEqual(b).print();
 * ```
 * @param a The first input tensor.
 * @param b The second input tensor. Must have the same dtype as `a`.
 *
 * @doc {heading: 'Operations', subheading: 'Logical'}
 */
function notEqual_(a, b) {
	let $a = convertToTensor(a, 'a', 'notEqual', 'string_or_numeric');
	let $b = convertToTensor(b, 'b', 'notEqual', 'string_or_numeric');
	[$a, $b] = makeTypesMatch($a, $b);
	assertAndGetBroadcastShape($a.shape, $b.shape);
	const inputs = {a: $a, b: $b};
	return ENGINE.runKernel(NotEqual, inputs);
}

const notEqual = /* @__PURE__ */ op({notEqual_});

/**
 * @license
 * Copyright 2018 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Creates a `tf.Tensor` with all elements set to 1 with the same shape as the
 * given tensor.
 *
 * ```js
 * const x = tf.tensor([1, 2]);
 * tf.onesLike(x).print();
 * ```
 * @param x A tensor.
 *
 * @doc {heading: 'Tensors', subheading: 'Creation'}
 */
function onesLike_(x) {
	const $x = convertToTensor(x, 'x', 'onesLike');
	const inputs = {x: $x};
	return ENGINE.runKernel(OnesLike, inputs);
}

const onesLike$1 = /* @__PURE__ */ op({onesLike_});

/**
 * Computes the outer product of two vectors, `v1` and `v2`.
 *
 * ```js
 * const a = tf.tensor1d([1, 2, 3]);
 * const b = tf.tensor1d([3, 4, 5]);
 *
 * tf.outerProduct(a, b).print();
 * ```
 * @param v1 The first vector in the outer product operation.
 * @param v2 The second vector in the outer product operation.
 *
 * @doc {heading: 'Operations', subheading: 'Matrices'}
 */
function outerProduct_(v1, v2) {
	const $v1 = convertToTensor(v1, 'v1', 'outerProduct');
	const $v2 = convertToTensor(v2, 'v2', 'outerProduct');
	assert($v1.rank === 1 && $v2.rank === 1, () => `Error in outerProduct: inputs must be rank 1, but got ranks ` +
		`${$v1.rank} and ${$v2.rank}.`);
	const v12D = reshape$1($v1, [-1, 1]);
	const v22D = reshape$1($v2, [1, -1]);
	return matMul$1(v12D, v22D);
}

const outerProduct = /* @__PURE__ */ op({outerProduct_});

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Pads a `tf.Tensor` with a given value and paddings.
 *
 * This operation implements `CONSTANT` mode. For `REFLECT` and `SYMMETRIC`,
 * refer to `tf.mirrorPad`.
 *
 * Also available are stricter rank-specific methods with the same signature
 * as this method that assert that `paddings` is of given length.
 *   - `tf.pad1d`
 *   - `tf.pad2d`
 *   - `tf.pad3d`
 *   - `tf.pad4d`
 *
 * ```js
 * const x = tf.tensor1d([1, 2, 3, 4]);
 * x.pad([[1, 2]]).print();
 * ```
 * @param x The tensor to pad.
 * @param paddings An array of length `R` (the rank of the tensor), where
 * each element is a length-2 tuple of ints `[padBefore, padAfter]`,
 * specifying how much to pad along each dimension of the tensor.
 * @param constantValue The pad value to use. Defaults to 0.
 *
 * @doc {heading: 'Tensors', subheading: 'Transformations'}
 */
function pad_(x, paddings, constantValue = 0) {
	const $x = convertToTensor(x, 'x', 'pad');
	if ($x.rank === 0) {
		throw new Error('pad(scalar) is not defined. Pass non-scalar to pad');
	}
	const attrs = {paddings, constantValue};
	const inputs = {x: $x};
	return ENGINE.runKernel(PadV2, inputs, attrs);
}

const pad = /* @__PURE__ */ op({pad_});

/**
 * Pads a `tf.Tensor1D` with a given value and paddings. See `pad` for details.
 */
function pad1d_(x, paddings, constantValue = 0) {
	assert(paddings.length === 2, () => 'Invalid number of paddings. Must be length of 2.');
	return pad(x, [paddings], constantValue);
}

const pad1d = /* @__PURE__ */ op({pad1d_});

/**
 * Pads a `tf.Tensor2D` with a given value and paddings. See `pad` for details.
 */
function pad2d_(x, paddings, constantValue = 0) {
	assert(paddings.length === 2 && paddings[0].length === 2 &&
		paddings[1].length === 2, () => 'Invalid number of paddings. Must be length of 2 each.');
	return pad(x, paddings, constantValue);
}

const pad2d = /* @__PURE__ */ op({pad2d_});

/**
 * Pads a `tf.Tensor3D` with a given value and paddings. See `pad` for details.
 */
function pad3d_(x, paddings, constantValue = 0) {
	assert(paddings.length === 3 && paddings[0].length === 2 &&
		paddings[1].length === 2 && paddings[2].length === 2, () => 'Invalid number of paddings. Must be length of 2 each.');
	return pad(x, paddings, constantValue);
}

const pad3d = /* @__PURE__ */ op({pad3d_});

/**
 * Pads a `tf.Tensor4D` with a given value and paddings. See `pad` for details.
 */
function pad4d_(x, paddings, constantValue = 0) {
	assert(paddings.length === 4 && paddings[0].length === 2 &&
		paddings[1].length === 2 && paddings[2].length === 2 &&
		paddings[3].length === 2, () => 'Invalid number of paddings. Must be length of 2 each.');
	return pad(x, paddings, constantValue);
}

const pad4d = /* @__PURE__ */ op({pad4d_});

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * This operation divides "spatial" dimensions `[1, ..., M]` of the input into
 * a grid of blocks of shape `blockShape`, and interleaves these blocks with
 * the "batch" dimension (0) such that in the output, the spatial
 * dimensions `[1, ..., M]` correspond to the position within the grid,
 * and the batch dimension combines both the position within a spatial block
 * and the original batch position. Prior to division into blocks,
 * the spatial dimensions of the input are optionally zero padded
 * according to `paddings`. See below for a precise description.
 *
 * ```js
 * const x = tf.tensor4d([1, 2, 3, 4], [1, 2, 2, 1]);
 * const blockShape = [2, 2];
 * const paddings = [[0, 0], [0, 0]];
 *
 * x.spaceToBatchND(blockShape, paddings).print();
 * ```
 *
 * @param x A `tf.Tensor`. N-D with `x.shape` = `[batch] + spatialShape +
 * remainingShape`, where spatialShape has `M` dimensions.
 * @param blockShape A 1-D array. Must have shape `[M]`, all values must
 * be >= 1.
 * @param paddings A 2-D array. Must have shape `[M, 2]`, all values must be >=
 *     0. `paddings[i] = [padStart, padEnd]` specifies the amount to zero-pad
 * from input dimension `i + 1`, which corresponds to spatial dimension `i`. It
 * is required that
 * `(inputShape[i + 1] + padStart + padEnd) % blockShape[i] === 0`
 *
 * This operation is equivalent to the following steps:
 *
 * 1. Zero-pad the start and end of dimensions `[1, ..., M]` of the input
 * according to `paddings` to produce `padded` of shape paddedShape.
 *
 * 2. Reshape `padded` to `reshapedPadded` of shape:
 * `[batch] + [paddedShape[1] / blockShape[0], blockShape[0], ...,
 * paddedShape[M] / blockShape[M-1], blockShape[M-1]] + remainingShape`
 *
 * 3. Permute dimensions of `reshapedPadded` to produce `permutedReshapedPadded`
 * of shape: `blockShape + [batch] + [paddedShape[1] / blockShape[0], ...,
 * paddedShape[M] / blockShape[M-1]] + remainingShape`
 *
 * 4. Reshape `permutedReshapedPadded` to flatten `blockShape` into the
 * batch dimension, producing an output tensor of shape:
 * `[batch * prod(blockShape)] + [paddedShape[1] / blockShape[0], ...,
 * paddedShape[M] / blockShape[M-1]] + remainingShape`
 *
 * @doc {heading: 'Tensors', subheading: 'Transformations'}
 */
function spaceToBatchND_(x, blockShape, paddings) {
	const $x = convertToTensor(x, 'x', 'spaceToBatchND');
	assert($x.rank >= 1 + blockShape.length, () => `input rank ${$x.rank} should be > than [blockShape] ${blockShape.length}`);
	assert(paddings.length === blockShape.length, () => `paddings.shape[0] ${paddings.length} must be equal to [blockShape] ${blockShape.length}`);
	assert($x.shape.reduce((a, b, i) => {
		if (i > 0 && i <= blockShape.length) {
			return a &&
				((b + paddings[i - 1][0] + paddings[i - 1][1]) %
					blockShape[i - 1] ===
					0);
		}
		return a;
	}, true), () => `input spatial dimensions ${$x.shape.slice(1)} with paddings ${paddings.toString()} must be divisible by blockShapes ${blockShape.toString()}`);
	const inputs = {x: $x};
	const attrs = {blockShape, paddings};
	return ENGINE.runKernel(SpaceToBatchND, inputs, attrs);
}

const spaceToBatchND = /* @__PURE__ */ op({spaceToBatchND_});

/**
 * @license
 * Copyright 2018 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Performs an N-D pooling operation
 *
 * @param input The input tensor, of rank 4 or rank 3 of shape
 *     `[batch, height, width, inChannels]`. If rank 3, batch of 1 is assumed.
 * @param windowShape The filter size: `[filterHeight, filterWidth]`. If
 *     `filterSize` is a single number, then `filterHeight == filterWidth`.
 * @param poolingType The type of pooling, either 'max' or 'avg'.
 * @param pad The type of padding algorithm:
 *    - `same` and stride 1: output will be of same size as input,
 *       regardless of filter size.
 *    - `valid`: output will be smaller than input if filter is larger
 *       than 1x1.
 *    - For more info, see this guide:
 *     [https://www.tensorflow.org/api_guides/python/nn#Convolution](
 *         https://www.tensorflow.org/api_guides/python/nn#Convolution)
 * @param dilations The dilation rates: `[dilationHeight, dilationWidth]`
 *     in which we sample input values across the height and width dimensions
 *     in dilated pooling. Defaults to `[1, 1]`. If `dilationRate` is a single
 *     number, then `dilationHeight == dilationWidth`. If it is greater than
 *     1, then all values of `strides` must be 1.
 * @param strides The strides of the pooling: `[strideHeight, strideWidth]`. If
 *     `strides` is a single number, then `strideHeight == strideWidth`.
 * @param dimRoundingMode A string from: 'ceil', 'round', 'floor'. If none is
 *     provided, it will default to truncate.
 *
 * @doc {heading: 'Operations', subheading: 'Convolution'}
 */
function pool_(input, windowShape, poolingType, pad, dilations, strides, dimRoundingMode) {
	if (dilations == null) {
		dilations = [1, 1];
	}
	if (strides == null) {
		strides = 1;
	}
	if (pad === 0) {
		pad = 'valid';
	}
	const $x = convertToTensor(input, 'x', 'maxPool');
	let x4D = $x;
	let reshapedTo4D = false;
	if ($x.rank === 3) {
		reshapedTo4D = true;
		x4D = reshape$1($x, [1, $x.shape[0], $x.shape[1], $x.shape[2]]);
	}
	assert(eitherStridesOrDilationsAreOne(strides, dilations), () => 'Error in pool: Either strides or dilations must be 1. ' +
		`Got strides ${strides} and dilations '${dilations}'`);
	const convInfo = computePool2DInfo(x4D.shape, windowShape, strides, dilations, pad);
	const dilation = [convInfo.dilationHeight, convInfo.dilationWidth];
	// The following implementation does batchToSpace(pool(spaceToBatch(x)))
	// whenever dilation > 1 since the TF kernels do not support dilation > 1.
	// tslint:disable-next-line:max-line-length
	// https://github.com/tensorflow/tensorflow/blob/50f6bb67dc98c9b74630b6047aae7a4f8a40fd02/tensorflow/python/ops/nn_ops.py#L1037
	let basePadding;
	if (pad === 'same') {
		basePadding = withSpaceToBatchBasePaddings([convInfo.filterHeight, convInfo.filterWidth], dilation);
	} else {
		basePadding = [[0, 0], [0, 0]];
	}
	const isDilationOne = dilation[0] === 1 && dilation[1] === 1;
	const [adjustedPadding, adjustedCrops] = requiredSpaceToBatchPaddings([convInfo.inHeight, convInfo.inWidth], dilation, basePadding);
	const convertedPad = isDilationOne ? pad : 'valid';
	const convertedX = isDilationOne ? x4D : spaceToBatchND(x4D, dilation, adjustedPadding);
	const forwardOp = poolingType === 'avg' ?
		() => avgPool(convertedX, windowShape, strides, convertedPad, dimRoundingMode) :
		() => maxPool(convertedX, windowShape, strides, convertedPad, dimRoundingMode);
	const y = forwardOp();
	const res = isDilationOne ? y : batchToSpaceND(y, dilation, adjustedCrops);
	if (reshapedTo4D) {
		return reshape$1(res, [res.shape[1], res.shape[2], res.shape[3]]);
	}
	return res;
}

// Helper function to compute crops and paddings for pool with dilation > 1.
// tslint:disable-next-line:max-line-length
// https://github.com/tensorflow/tensorflow/blob/50f6bb67dc98c9b74630b6047aae7a4f8a40fd02/tensorflow/python/ops/array_ops.py#L2184
function requiredSpaceToBatchPaddings(inputShape, blockShape, basePadding) {
	const padStart = basePadding.map(b => b[0]);
	const origPadEnd = basePadding.map(b => b[1]);
	const fullInputShape = inputShape.concat(padStart, origPadEnd);
	const padEndExtra = blockShape.map((b, i) => (b - fullInputShape[i] % b) % b);
	const padEnd = origPadEnd.map((s, i) => s + padEndExtra[i]);
	const paddings = blockShape.map((_, i) => [padStart[i], padEnd[i]]);
	const crops = blockShape.map((_, i) => [0, padEndExtra[i]]);
	return [paddings, crops];
}

// Helper function to compute base paddings for pool with dilation > 1.
// tslint:disable-next-line:max-line-length
// https://github.com/tensorflow/tensorflow/blob/50f6bb67dc98c9b74630b6047aae7a4f8a40fd02/tensorflow/python/ops/nn_ops.py#L524
function withSpaceToBatchBasePaddings(filterShape, dilation) {
	// Spatial dimensions of the filters and the upsampled filters in which we
	// introduce (rate - 1) zeros between consecutive filter values.
	const dilatedFilterShape = filterShape.map((s, i) => {
		return s + (s - 1) * (dilation[i] - 1);
	});
	const padExtraShape = dilatedFilterShape.map(s => s - 1);
	// When padding is odd, we pad more at end, following the same
	// convention as conv2d.
	const padExtraStart = padExtraShape.map(s => Math.floor(s / 2));
	const padExtraEnd = padExtraShape.map((s, i) => s - padExtraStart[i]);
	return padExtraShape.map((_, i) => {
		return [padExtraStart[i], padExtraEnd[i]];
	});
}

const pool = /* @__PURE__ */ op({pool_});

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Computes leaky rectified linear element-wise with parametric alphas.
 *
 * `x < 0 ? alpha * x : f(x) = x`
 *
 * ```js
 * const x = tf.tensor1d([-1, 2, -3, 4]);
 * const alpha = tf.scalar(0.1);
 *
 * x.prelu(alpha).print();  // or tf.prelu(x, alpha)
 * ```
 * @param x The input tensor.
 * @param alpha Scaling factor for negative values.
 *
 * @doc {heading: 'Operations', subheading: 'Basic math'}
 */
function prelu_(x, alpha) {
	const $x = convertToTensor(x, 'x', 'prelu');
	const $alpha = convertToTensor(alpha, 'alpha', 'prelu');
	const inputs = {x: $x, alpha: $alpha};
	return ENGINE.runKernel(Prelu, inputs);
}

const prelu$1 = /* @__PURE__ */ op({prelu_});

/**
 * @license
 * Copyright 2020 Google Inc. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Prints information about the `tf.Tensor` including its data.
 *
 * ```js
 * const verbose = true;
 * tf.tensor2d([1, 2, 3, 4], [2, 2]).print(verbose);
 * ```
 * @param x The tensor to be printed.
 * @param verbose Whether to print verbose information about the ` Tensor`,
 * including dtype and size.
 *
 * @doc {heading: 'Tensors', subheading: 'Creation'}
 */
function print(x, verbose = false) {
	console.log(x.toString(verbose));
}

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Computes the product of elements across dimensions of a `tf.Tensor`.
 *
 * Reduces the input along the dimensions given in `axes`. Unless `keepDims`
 * is true, the rank of the `tf.Tensor` is reduced by 1 for each entry in
 * `axes`. If `keepDims` is true, the reduced dimensions are retained with
 * length 1. If `axes` has no entries, all dimensions are reduced, and a
 * `tf.Tensor` with a single element is returned.
 *
 * ```js
 * const x = tf.tensor1d([1, 2, 3]);
 *
 * x.prod().print();  // or tf.prod(x)
 * ```
 *
 * ```js
 * const x = tf.tensor2d([1, 2, 3, 4], [2, 2]);
 *
 * const axis = 1;
 * x.prod(axis).print();  // or tf.prod(x, axis)
 * ```
 *
 * @param x The input tensor to compute the product over. If the dtype is `bool`
 *   it will be converted to `int32` and the output dtype will be `int32`.
 * @param axis The dimension(s) to reduce. By default it reduces
 *     all dimensions.
 * @param keepDims If true, retains reduced dimensions with size 1.
 *
 * @doc {heading: 'Operations', subheading: 'Reduction'}
 */
function prod_(x, axis = null, keepDims = false) {
	let $x = convertToTensor(x, 'x', 'prod');
	if ($x.dtype === 'bool') {
		// bool is not an allowed type for the underlying kernel.
		$x = cast$1($x, 'int32');
	}
	const inputs = {x: $x};
	const attrs = {axis, keepDims};
	return ENGINE.runKernel(Prod, inputs, attrs);
}

const prod = /* @__PURE__ */ op({prod_});

/**
 * @license
 * Copyright 2022 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
function raggedGather_(paramsNestedSplits, paramsDenseValues, indices, outputRaggedRank) {
	const $paramsNestedSplits = paramsNestedSplits.map((t, i) => convertToTensor(t, `tensors${i}`, 'raggedGather', 'int32'));
	const $paramsDenseValues = convertToTensor(paramsDenseValues, 'paramsDenseValues', 'raggedGather');
	const $indices = convertToTensor(indices, 'indices', 'raggedGather', 'int32');
	const inputs = {
		paramsNestedSplits: $paramsNestedSplits,
		paramsDenseValues: $paramsDenseValues,
		indices: $indices,
	};
	const attrs = {outputRaggedRank};
	const result = ENGINE.runKernel(RaggedGather, inputs, attrs);
	return {
		outputNestedSplits: result.slice(0, result.length - 1),
		outputDenseValues: result[result.length - 1],
	};
}

const raggedGather = /* @__PURE__ */ op({raggedGather_});

/**
 * @license
 * Copyright 2022 Google LLC.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Returns a RaggedTensor result composed from rtDenseValues and rtNestedSplits,
 * such that result[i] = [starts[i], starts[i] + deltas[i], ..., limits[i]]).
 *
 * @param starts: A Tensor. Must be one of the following types:
 *     'float32', 'int32'. The starts of each range.
 * @param limits: A Tensor. Must have the same type as starts. The limits of
 *     each range.
 * @param deltas: A Tensor. Must have the same type as starts. The deltas of
 *     each range.
 * @return A map with the following properties:
 *     - rtNestedSplits: A Tensor of type 'int32'.
 *     - rtDenseValues: A Tensor. Has the same type as starts.
 */
function raggedRange_(starts, limits, deltas) {
	const $starts = convertToTensor(starts, 'starts', 'raggedRange');
	const $limits = convertToTensor(limits, 'limits', 'raggedRange', $starts.dtype);
	const $deltas = convertToTensor(deltas, 'deltas', 'raggedRange', $starts.dtype);
	const inputs = {
		starts: $starts,
		limits: $limits,
		deltas: $deltas,
	};
	const result = ENGINE.runKernel(RaggedRange, inputs);
	return {
		rtNestedSplits: result[0],
		rtDenseValues: result[1],
	};
}

const raggedRange = /* @__PURE__ */ op({raggedRange_});

/**
 * @license
 * Copyright 2022 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Create a dense tensor from a ragged tensor, possibly altering its shape.
 *
 * The raggedTensorToTensor op creates a dense tensor from am array of row
 * partition tensors, a value vector, and default values. If the shape is
 * unspecified, the minimal shape required to contain all the elements in the
 * ragged tensor (the natural shape) will be used. If some dimensions are left
 * unspecified, then the size of the natural shape is used in that dimension.
 *
 * The defaultValue will be broadcast to the output shape. After that, the
 * values from the ragged tensor overwrite the default values. Note that the
 * defaultValue must have less dimensions than the value.
 *
 * The row partition tensors are in the order of the dimensions. At present, the
 * types can be: "ROW_SPLITS": the row_splits tensor from the ragged tensor.
 *   "VALUE_ROWIDS": the value_rowids tensor from the ragged tensor.
 *   "FIRST_DIM_SIZE": if value_rowids is used for the first dimension, then it
 * is preceded by "FIRST_DIM_SIZE".
 * ```
 * @param shape: A Tensor. Must be one of the following types: 'int32'. The
 *     desired shape of the output tensor. If left unspecified (empty), the
 *     minimal shape required to contain all the elements in the ragged tensor
 *     (the natural shape) will be used. If some dimensions are left
 *     unspecified, then the size of the natural shape is used in that
 *     dimension.
 *
 *     Note that dense dimensions cannot be modified by the shape argument.
 *     Trying to change the size of a dense dimension will cause the op to fail.
 *     Examples: natural shape: [4, 5, 6] shape: -1 output shape: [4, 5, 6]
 *
 *     natural shape: [4, 5, 6] shape: [3, -1, 2] output shape: [3, 5, 2]
 *
 *     natural shape: [4, 5, 6] shape: [3, 7, 2] output shape: [3, 7, 2]
 * @param values: A Tensor. A 1D tensor representing the values of the ragged
 *     tensor.
 * @param defaultValue: A Tensor. Must have the same type as values. The
 *     defaultValue when the shape is larger than the ragged tensor. The
 *     defaultValue is broadcast until it is the shape of the output tensor,
 *     and then overwritten by values in the ragged tensor. The default value
 *     must be compatible with this broadcast operation, and must have fewer
 *     dimensions than the value tensor.
 * @param rowPartitionTensors: A list of at least 1 Tensor objects with the same
 *     type in: 'int32'.
 * @param rowPartitionTypes: A list of strings. The types of the row partition
 *     tensors. At present, these can be:
 *     "ROW_SPLITS": the row_splits tensor from the ragged tensor.
 *     "VALUE_ROWIDS": the value_rowids tensor from the ragged tensor.
 *     "FIRST_DIM_SIZE": if value_rowids is used for the first dimension, then
 *         it is preceded by "FIRST_DIM_SIZE". The tensors are in the order of
 *         the dimensions.
 * @return A Tensor. Has the same type as values.
 * @doc {heading: 'Operations', subheading: 'Ragged'}
 */
function raggedTensorToTensor_(shape, values, defaultValue, rowPartitionTensors, rowPartitionTypes) {
	const $shape = convertToTensor(shape, 'shape', 'raggedTensorToTensor', 'int32');
	const $values = convertToTensor(values, 'values', 'raggedTensorToTensor');
	const $defaultValue = convertToTensor(defaultValue, 'defaultValue', 'raggedTensorToTensor', $values.dtype);
	const $rowPartitionTensors = rowPartitionTensors.map((t, i) => convertToTensor(t, `tensors${i}`, 'raggedTensorToTensor', 'int32'));
	const inputs = {
		shape: $shape,
		values: $values,
		defaultValue: $defaultValue,
		rowPartitionTensors: $rowPartitionTensors
	};
	const attrs = {rowPartitionTypes};
	return ENGINE.runKernel(RaggedTensorToTensor, inputs, attrs);
}

const raggedTensorToTensor = /* @__PURE__ */ op({raggedTensorToTensor_});

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Creates a `tf.Tensor` with values sampled from a random number generator
 * function defined by the user.
 *
 * @param shape An array of integers defining the output tensor shape.
 * @param randFunction A random number generator function which is called
 * for each element in the output tensor.
 * @param dtype The data type of the output tensor. Defaults to 'float32'.
 *
 * @doc {heading: 'Tensors', subheading: 'Random'}
 */
function rand_(shape, randFunction, dtype) {
	assertNonNegativeIntegerDimensions(shape);
	const size = sizeFromShape(shape);
	let values = null;
	if (dtype == null || dtype === 'float32') {
		values = new Float32Array(size);
	} else if (dtype === 'int32') {
		values = new Int32Array(size);
	} else if (dtype === 'bool') {
		values = new Uint8Array(size);
	} else {
		throw new Error(`Unknown data type ${dtype}`);
	}
	for (let i = 0; i < size; i++) {
		values[i] = randFunction();
	}
	return ENGINE.makeTensor(values, shape, dtype);
}

const rand = /* @__PURE__ */ op({rand_});

// A library of seedable RNGs implemented in Javascript.
//
// Usage:
//
// var seedrandom = require('seedrandom');
// var random = seedrandom(1); // or any seed.
// var x = random();       // 0 <= x < 1.  Every bit is random.
// var x = random.quick(); // 0 <= x < 1.  32 bits of randomness.

// alea, a 53-bit multiply-with-carry generator by Johannes Baage.
// Period: ~2^116
// Reported to pass all BigCrush tests.
//var alea = require('./lib/alea');

// xor128, a pure xor-shift generator by George Marsaglia.
// Period: 2^128-1.
// Reported to fail: MatrixRank and LinearComp.
//var xor128 = require('./lib/xor128');

// xorwow, George Marsaglia's 160-bit xor-shift combined plus weyl.
// Period: 2^192-2^32
// Reported to fail: CollisionOver, SimpPoker, and LinearComp.
//var xorwow = require('./lib/xorwow');

// xorshift7, by Franois Panneton and Pierre L'ecuyer, takes
// a different approach: it adds robustness by allowing more shifts
// than Marsaglia's original three.  It is a 7-shift generator
// with 256 bits, that passes BigCrush with no systmatic failures.
// Period 2^256-1.
// No systematic BigCrush failures reported.
//var xorshift7 = require('./lib/xorshift7');

// xor4096, by Richard Brent, is a 4096-bit xor-shift with a
// very long period that also adds a Weyl generator. It also passes
// BigCrush with no systematic failures.  Its long period may
// be useful if you have many generators and need to avoid
// collisions.
// Period: 2^4128-2^32.
// No systematic BigCrush failures reported.
//var xor4096 = require('./lib/xor4096');

// Tyche-i, by Samuel Neves and Filipe Araujo, is a bit-shifting random
// number generator derived from ChaCha, a modern stream cipher.
// https://eden.dei.uc.pt/~sneves/pubs/2011-snfa2.pdf
// Period: ~2^127
// No systematic BigCrush failures reported.
//var tychei = require('./lib/tychei');

// The original ARC4-based prng included in this library.
// Period: ~2^1600
//var sr = require('./seedrandom');

// sr.alea = alea;
// sr.xor128 = xor128;
// sr.xorwow = xorwow;
// sr.xorshift7 = xorshift7;
// sr.xor4096 = xor4096;
// sr.tychei = tychei;

//module.exports = sr;

/**
 * @license
 * Copyright 2018 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
// https://en.wikipedia.org/wiki/Marsaglia_polar_method
class MPRandGauss {
	constructor(mean, stdDeviation, dtype, truncated, seed) {
		this.mean = mean;
		this.stdDev = stdDeviation;
		this.dtype = dtype;
		this.nextVal = NaN;
		this.truncated = truncated;
		if (this.truncated) {
			this.upper = this.mean + this.stdDev * 2;
			this.lower = this.mean - this.stdDev * 2;
		}
		const seedValue = seed ? seed : Math.random();
		this.random = undefined(seedValue.toString());
	}

	/** Returns next sample from a Gaussian distribution. */
	nextValue() {
		if (!isNaN(this.nextVal)) {
			const value = this.nextVal;
			this.nextVal = NaN;
			return value;
		}
		let resultX, resultY;
		let isValid = false;
		while (!isValid) {
			let v1, v2, s;
			do {
				v1 = 2 * this.random() - 1;
				v2 = 2 * this.random() - 1;
				s = v1 * v1 + v2 * v2;
			} while (s >= 1 || s === 0);
			const mul = Math.sqrt(-2 * Math.log(s) / s);
			resultX = this.mean + this.stdDev * v1 * mul;
			resultY = this.mean + this.stdDev * v2 * mul;
			if (!this.truncated || this.isValidTruncated(resultX)) {
				isValid = true;
			}
		}
		if (!this.truncated || this.isValidTruncated(resultY)) {
			this.nextVal = this.convertValue(resultY);
		}
		return this.convertValue(resultX);
	}

	/** Handles proper rounding for non-floating-point numbers. */
	convertValue(value) {
		if (this.dtype == null || this.dtype === 'float32') {
			return value;
		}
		return Math.round(value);
	}

	/** Returns true if less than 2-standard-deviations from the mean. */
	isValidTruncated(value) {
		return value <= this.upper && value >= this.lower;
	}
}

// Marsaglia, George, and Wai Wan Tsang. 2000. "A Simple Method for Generating
// Gamma Variables."
class RandGamma {
	constructor(alpha, beta, dtype, seed) {
		this.alpha = alpha;
		this.beta = 1 / beta; // convert rate to scale parameter
		this.dtype = dtype;
		const seedValue = seed ? seed : Math.random();
		this.randu = undefined(seedValue.toString());
		this.randn = new MPRandGauss(0, 1, dtype, false, this.randu());
		if (alpha < 1) {
			this.d = alpha + (2 / 3);
		} else {
			this.d = alpha - (1 / 3);
		}
		this.c = 1 / Math.sqrt(9 * this.d);
	}

	/** Returns next sample from a gamma distribution. */
	nextValue() {
		let x2, v0, v1, x, u, v;
		while (true) {
			do {
				x = this.randn.nextValue();
				v = 1 + (this.c * x);
			} while (v <= 0);
			v *= v * v;
			x2 = x * x;
			v0 = 1 - (0.331 * x2 * x2);
			v1 = (0.5 * x2) + (this.d * (1 - v + Math.log(v)));
			u = this.randu();
			if (u < v0 || Math.log(u) < v1) {
				break;
			}
		}
		v = (1 / this.beta) * this.d * v;
		if (this.alpha < 1) {
			v *= Math.pow(this.randu(), 1 / this.alpha);
		}
		return this.convertValue(v);
	}

	/** Handles proper rounding for non-floating-point numbers. */
	convertValue(value) {
		if (this.dtype === 'float32') {
			return value;
		}
		return Math.round(value);
	}
}

class UniformRandom {
	constructor(min = 0, max = 1, dtype, seed) {
		/** Handles proper rounding for non floating point numbers. */
		this.canReturnFloat = () => (this.dtype == null || this.dtype === 'float32');
		this.min = min;
		this.range = max - min;
		this.dtype = dtype;
		if (seed == null) {
			seed = Math.random();
		}
		if (typeof seed === 'number') {
			seed = seed.toString();
		}
		if (!this.canReturnFloat() && this.range <= 1) {
			throw new Error(`The difference between ${min} - ${max} <= 1 and dtype is not float`);
		}
		this.random = undefined(seed);
	}

	convertValue(value) {
		if (this.canReturnFloat()) {
			return value;
		}
		return Math.round(value);
	}

	nextValue() {
		return this.convertValue(this.min + this.range * this.random());
	}
}

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Creates a `tf.Tensor` with values sampled from a gamma distribution.
 *
 * ```js
 * tf.randomGamma([2, 2], 1).print();
 * ```
 *
 * @param shape An array of integers defining the output tensor shape.
 * @param alpha The shape parameter of the gamma distribution.
 * @param beta The inverse scale parameter of the gamma distribution. Defaults
 *     to 1.
 * @param dtype The data type of the output. Defaults to float32.
 * @param seed The seed for the random number generator.
 *
 * @doc {heading: 'Tensors', subheading: 'Random'}
 */
function randomGamma_(shape, alpha, beta = 1, dtype = 'float32', seed) {
	assertNonNegativeIntegerDimensions(shape);
	if (beta == null) {
		beta = 1;
	}
	if (dtype == null) {
		dtype = 'float32';
	}
	if (dtype !== 'float32' && dtype !== 'int32') {
		throw new Error(`Unsupported data type ${dtype}`);
	}
	const rgamma = new RandGamma(alpha, beta, dtype, seed);
	const res = buffer(shape, dtype);
	for (let i = 0; i < res.values.length; i++) {
		res.values[i] = rgamma.nextValue();
	}
	return res.toTensor();
}

const randomGamma = /* @__PURE__ */ op({randomGamma_});

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Creates a `tf.Tensor` with values sampled from a normal distribution.
 *
 * ```js
 * tf.randomNormal([2, 2]).print();
 * ```
 *
 * @param shape An array of integers defining the output tensor shape.
 * @param mean The mean of the normal distribution.
 * @param stdDev The standard deviation of the normal distribution.
 * @param dtype The data type of the output.
 * @param seed The seed for the random number generator.
 *
 * @doc {heading: 'Tensors', subheading: 'Random'}
 */
function randomNormal_(shape, mean = 0, stdDev = 1, dtype, seed) {
	assertNonNegativeIntegerDimensions(shape);
	if (dtype != null && dtype === 'bool') {
		throw new Error(`Unsupported data type ${dtype}`);
	}
	const randGauss = new MPRandGauss(mean, stdDev, dtype, false /* truncated */, seed);
	const res = buffer(shape, dtype);
	for (let i = 0; i < res.values.length; i++) {
		res.values[i] = randGauss.nextValue();
	}
	return res.toTensor();
}

const randomNormal = /* @__PURE__ */ op({randomNormal_});

/**
 * @license
 * Copyright 2022 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Creates a `tf.Tensor` with values sampled from a normal distribution.
 *
 * The generated values will have mean 0 and standard deviation 1.
 *
 * ```js
 * tf.randomStandardNormal([2, 2]).print();
 * ```
 *
 * @param shape An array of integers defining the output tensor shape.
 * @param dtype The data type of the output.
 * @param seed The seed for the random number generator.
 *
 * @doc {heading: 'Tensors', subheading: 'Random'}
 */
function randomStandardNormal_(shape, dtype, seed) {
	if (dtype != null && dtype === 'bool') {
		throw new Error(`Unsupported data type ${dtype}`);
	}
	return randomNormal(shape, 0, 1, dtype, seed);
}

const randomStandardNormal = /* @__PURE__ */ op({randomStandardNormal_});

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Creates a `tf.Tensor` with values sampled from a uniform distribution.
 *
 * The generated values follow a uniform distribution in the range [minval,
 * maxval). The lower bound minval is included in the range, while the upper
 * bound maxval is excluded.
 *
 * ```js
 * tf.randomUniform([2, 2]).print();
 * ```
 *
 * @param shape An array of integers defining the output tensor shape.
 * @param minval The lower bound on the range of random values to generate.
 *   Defaults to 0.
 * @param maxval The upper bound on the range of random values to generate.
 *   Defaults to 1.
 * @param dtype The data type of the output tensor. Defaults to 'float32'.
 * @param seed An optional int. Defaults to 0. If seed is set to be non-zero,
 *   the random number generator is seeded by the given seed. Otherwise, it is
 *   seeded by a random seed.
 *
 * @doc {heading: 'Tensors', subheading: 'Random'}
 */
function randomUniform_(shape, minval = 0, maxval = 1, dtype = 'float32', seed) {
	assertNonNegativeIntegerDimensions(shape);
	const res = buffer(shape, dtype);
	const random = new UniformRandom(minval, maxval, null, seed);
	for (let i = 0; i < res.values.length; i++) {
		res.values[i] = random.nextValue();
	}
	return res.toTensor();
}

const randomUniform = /* @__PURE__ */ op({randomUniform_});

/**
 * @license
 * Copyright 2023 Google LLC.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Creates a `tf.Tensor` with integers sampled from a uniform distribution.
 *
 * The generated values are uniform integers in the range [minval, maxval). The
 * lower bound minval is included in the range, while the upper bound maxval is
 * excluded.
 *
 * ```js
 * tf.randomUniformInt([2, 2], 0, 10).print();
 * ```
 *
 * @param shape An array of integers defining the output tensor shape.
 * @param minval Inclusive lower bound on the generated integers.
 * @param maxval Exclusive upper bound on the generated integers.
 * @param seed An optional int. Defaults to 0. If seed is set to be non-zero,
 *   the random number generator is seeded by the given seed. Otherwise, it is
 *   seeded by a random seed.
 *
 * @doc {heading: 'Tensors', subheading: 'Random'}
 */
function randomUniformInt_(shape, minval, maxval, seed) {
	// TODO(mattsoulanille): Handle optional seed2 input.
	return randomUniform(shape, minval, maxval, 'int32', seed);
}

const randomUniformInt = /* @__PURE__ */ op({randomUniformInt_});

/**
 * @license
 * Copyright 2018 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Creates a new `tf.Tensor1D` filled with the numbers in the range provided.
 *
 * The tensor is a half-open interval meaning it includes start, but
 * excludes stop. Decrementing ranges and negative step values are also
 * supported.
 *
 *
 * ```js
 * tf.range(0, 9, 2).print();
 * ```
 *
 * @param start An integer start value
 * @param stop An integer stop value
 * @param step An integer increment (will default to 1 or -1)
 * @param dtype The data type of the output tensor. Defaults to 'float32'.
 *
 * @doc {heading: 'Tensors', subheading: 'Creation'}
 */
function range(start, stop, step = 1, dtype = 'float32') {
	if (step === 0) {
		throw new Error('Cannot have a step of zero');
	}
	const attrs = {start, stop, step, dtype};
	return ENGINE.runKernel(Range, {} /* inputs */, attrs);
}

/**
 * @license
 * Copyright 2018 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Computes reciprocal of x element-wise: `1 / x`
 *
 * ```js
 * const x = tf.tensor1d([0, 1, 2]);
 *
 * x.reciprocal().print();  // or tf.reciprocal(x)
 * ```
 * @param x The input tensor.
 *
 * @doc {heading: 'Operations', subheading: 'Basic math'}
 */
function reciprocal_(x) {
	const $x = convertToTensor(x, 'x', 'reciprocal');
	const inputs = {x: $x};
	return ENGINE.runKernel(Reciprocal, inputs);
}

const reciprocal = /* @__PURE__ */ op({reciprocal_});

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Computes rectified linear element-wise: `max(x, 0)`.
 *
 * ```js
 * const x = tf.tensor1d([-1, 2, -3, 4]);
 *
 * x.relu().print();  // or tf.relu(x)
 * ```
 * @param x The input tensor. If the dtype is `bool`, the output dtype will be
 *     `int32`.
 *
 * @doc {heading: 'Operations', subheading: 'Basic math'}
 */
function relu_(x) {
	const $x = convertToTensor(x, 'x', 'relu');
	const inputs = {x: $x};
	return ENGINE.runKernel(Relu, inputs);
}

const relu$1 = /* @__PURE__ */ op({relu_});

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Computes rectified linear 6 element-wise: `min(max(x, 0), 6)`.
 *
 * ```js
 * const x = tf.tensor1d([-1, 2, -3, 8]);
 *
 * x.relu6().print();  // or tf.relu6(x)
 * ```
 * @param x The input tensor. If the dtype is `bool`, the output dtype will be
 *     `int32`.
 *
 * @doc {heading: 'Operations', subheading: 'Basic math'}
 */
function relu6_(x) {
	const $x = convertToTensor(x, 'x', 'relu6');
	const inputs = {x: $x};
	return ENGINE.runKernel(Relu6, inputs);
}

const relu6$1 = /* @__PURE__ */ op({relu6_});

/**
 * @license
 * Copyright 2018 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Reverses a `tf.Tensor` along a specified axis.
 *
 * Also available are stricter rank-specific methods that assert that `x` is
 * of the given rank:
 *   - `tf.reverse1d`
 *   - `tf.reverse2d`
 *   - `tf.reverse3d`
 *   - `tf.reverse4d`
 *
 * Except `tf.reverse1d` (which does not have axis param), all methods have
 * same signature as this method.
 *
 * ```js
 * const x = tf.tensor1d([1, 2, 3, 4]);
 *
 * x.reverse().print();
 * ```
 *
 * ```js
 * const x = tf.tensor2d([1, 2, 3, 4], [2, 2]);
 *
 * const axis = 1;
 * x.reverse(axis).print();
 * ```
 * @param x The input tensor to be reversed.
 * @param axis The set of dimensions to reverse. Must be in the
 *     range [-rank(x), rank(x)). Defaults to all axes.
 *
 * @doc {heading: 'Tensors', subheading: 'Slicing and Joining'}
 */
function reverse_(x, axis) {
	const $x = convertToTensor(x, 'x', 'reverse');
	const inputs = {x: $x};
	const attrs = {dims: axis};
	return ENGINE.runKernel(Reverse, inputs, attrs);
}

const reverse = /* @__PURE__ */ op({reverse_});

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Reverses a `tf.Tensor1D`.
 *
 * @param x The input tensor.
 */
function reverse1d_(x) {
	const $x = convertToTensor(x, 'x', 'reverse');
	assert($x.rank === 1, () => `Error in reverse1D: x must be rank 1 but got rank ${$x.rank}.`);
	return reverse($x, 0);
}

const reverse1d = /* @__PURE__ */ op({reverse1d_});

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Reverses a `tf.Tensor2D` along a specified axis.
 *
 * @param x The input tensor.
 * @param axis The set of dimensions to reverse. Must be in the
 *     range [-rank(x), rank(x)). Defaults to all axes.
 */
function reverse2d_(x, axis) {
	const $x = convertToTensor(x, 'x', 'reverse');
	assert($x.rank === 2, () => `Error in reverse2D: x must be rank 2 but got rank ${$x.rank}.`);
	return reverse($x, axis);
}

const reverse2d = /* @__PURE__ */ op({reverse2d_});

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Reverses a `tf.Tensor3D` along a specified axis.
 *
 * @param x The input tensor.
 * @param axis The set of dimensions to reverse. Must be in the
 *     range [-rank(x), rank(x)). Defaults to all axes.
 */
function reverse3d_(x, axis) {
	const $x = convertToTensor(x, 'x', 'reverse');
	assert($x.rank === 3, () => `Error in reverse3D: x must be rank 3 but got rank ${$x.rank}.`);
	return reverse($x, axis);
}

const reverse3d = /* @__PURE__ */ op({reverse3d_});

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Reverses a `tf.Tensor4D` along a specified axis.
 *
 * @param x The input tensor.
 * @param axis The set of dimensions to reverse. Must be in the
 *     range [-rank(x), rank(x)). Defaults to all axes.
 */
function reverse4d_(x, axis) {
	const $x = convertToTensor(x, 'x', 'reverse');
	assert($x.rank === 4, () => `Error in reverse4D: x must be rank 4 but got rank ${$x.rank}.`);
	return reverse($x, axis);
}

const reverse4d = /* @__PURE__ */ op({reverse4d_});

/**
 * @license
 * Copyright 2018 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Computes round of input `tf.Tensor` element-wise: `round(x)`.
 * It implements banker's rounding.
 *
 * ```js
 * const x = tf.tensor1d([.6, 1.1, -3.3]);
 *
 * x.round().print();  // or tf.round(x)
 * ```
 * @param x The input tensor.
 *
 * @doc {heading: 'Operations', subheading: 'Basic math'}
 */
function round_(x) {
	const $x = convertToTensor(x, 'x', 'round');
	const inputs = {x: $x};
	return ENGINE.runKernel(Round, inputs);
}

const round = /* @__PURE__ */ op({round_});

/**
 * @license
 * Copyright 2018 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Computes reciprocal of square root of the input `tf.Tensor` element-wise:
 * `y = 1 / sqrt(x)`
 *
 * ```js
 * const x = tf.tensor1d([1, 2, 4, -1]);
 *
 * x.rsqrt().print();  // or tf.rsqrt(x)
 * ```
 * @param x The input tensor.
 *
 * @doc {heading: 'Operations', subheading: 'Basic math'}
 */
function rsqrt_(x) {
	const $x = convertToTensor(x, 'x', 'rsqrt', 'float32');
	const inputs = {x: $x};
	return ENGINE.runKernel(Rsqrt, inputs);
}

const rsqrt = /* @__PURE__ */ op({rsqrt_});

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Computes scaled exponential linear element-wise.
 *
 * `x < 0 ? scale * alpha * (exp(x) - 1) : scale * x`
 *
 * ```js
 * const x = tf.tensor1d([-1, 2, -3, 4]);
 *
 * x.selu().print();  // or tf.selu(x)
 * ```
 * @param x The input tensor.
 *
 * @doc {heading: 'Operations', subheading: 'Basic math'}
 */
function selu_(x) {
	const $x = convertToTensor(x, 'x', 'selu');
	const inputs = {x: $x};
	return ENGINE.runKernel(Selu, inputs);
}

const selu = /* @__PURE__ */ op({selu_});

/**
 * 2-D convolution with separable filters.
 *
 * Performs a depthwise convolution that acts separately on channels followed
 * by a pointwise convolution that mixes channels. Note that this is
 * separability between dimensions [1, 2] and 3, not spatial separability
 * between dimensions 1 and 2.
 *
 * See
 * [https://www.tensorflow.org/api_docs/python/tf/nn/separable_conv2d](
 *     https://www.tensorflow.org/api_docs/python/tf/nn/separable_conv2d)
 * for more details.
 *
 * @param x The input tensor, of rank 4 or rank 3, of shape
 *     `[batch, height, width, inChannels]`. If rank 3, batch of 1 is
 * assumed.
 * @param depthwiseFilter The depthwise filter tensor, rank 4, of shape
 *     `[filterHeight, filterWidth, inChannels, channelMultiplier]`. This is
 *     the filter used in the first step.
 * @param pointwiseFilter The pointwise filter tensor, rank 4, of shape
 *     `[1, 1, inChannels * channelMultiplier, outChannels]`. This is
 *     the filter used in the second step.
 * @param strides The strides of the convolution: `[strideHeight,
 * strideWidth]`. If strides is a single number, then `strideHeight ==
 * strideWidth`.
 * @param pad The type of padding algorithm.
 *   - `same` and stride 1: output will be of same size as input,
 *       regardless of filter size.
 *   - `valid`: output will be smaller than input if filter is larger
 *       than 1x1.
 *   - For more info, see this guide:
 *     [https://www.tensorflow.org/api_docs/python/tf/nn/convolution](
 *          https://www.tensorflow.org/api_docs/python/tf/nn/convolution)
 * @param dilations The dilation rates: `[dilationHeight, dilationWidth]`
 *     in which we sample input values across the height and width dimensions
 *     in atrous convolution. Defaults to `[1, 1]`. If `rate` is a single
 *     number, then `dilationHeight == dilationWidth`. If it is greater than
 *     1, then all values of `strides` must be 1.
 * @param dataFormat: An optional string from: "NHWC", "NCHW". Defaults to
 *     "NHWC". Specify the data format of the input and output data. With the
 *     default format "NHWC", the data is stored in the order of: [batch,
 *     height, width, channels]. Only "NHWC" is currently supported.
 *
 * @doc {heading: 'Operations', subheading: 'Convolution'}
 */
function separableConv2d_(x, depthwiseFilter, pointwiseFilter, strides, pad, dilation = [1, 1], dataFormat = 'NHWC') {
	const $x = convertToTensor(x, 'x', 'separableConv2d');
	const $depthwiseFilter = convertToTensor(depthwiseFilter, 'depthwiseFilter', 'separableConv2d');
	const $pointwiseFilter = convertToTensor(pointwiseFilter, 'pointwiseFilter', 'separableConv2d');
	let x4D = $x;
	let reshapedTo4D = false;
	if ($x.rank === 3) {
		reshapedTo4D = true;
		x4D = reshape$1($x, [1, $x.shape[0], $x.shape[1], $x.shape[2]]);
	}
	if (dataFormat === 'NCHW') {
		throw new Error('separableConv2d currently does not support dataFormat NCHW; only ' +
			'NHWC is supported');
	}
	assert(x4D.rank === 4, () => `Error in separableConv2d: input must be rank 4, but got ` +
		`rank ${x4D.rank}.`);
	assert($depthwiseFilter.rank === 4, () => `Error in separableConv2d: depthwise filter must be rank 4, but ` +
		`got rank ${$depthwiseFilter.rank}.`);
	assert($pointwiseFilter.rank === 4, () => `Error in separableConv2d: pointwise filter must be rank 4, but ` +
		`got rank ${$depthwiseFilter.rank}.`);
	assert($pointwiseFilter.shape[0] === 1, () => `Error in separableConv2d: the first dimension of pointwise filter ` +
		` must be 1, but got ${$pointwiseFilter.shape[0]}.`);
	assert($pointwiseFilter.shape[1] === 1, () => `Error in separableConv2d: the second dimension of pointwise ` +
		`filter must be 1, but got ${$pointwiseFilter.shape[1]}.`);
	const inChannels = $depthwiseFilter.shape[2];
	const channelMultiplier = $depthwiseFilter.shape[3];
	assert($pointwiseFilter.shape[2] === inChannels * channelMultiplier, () => `Error in separableConv2d: the third dimension of pointwise filter ` +
		`must be ${inChannels * channelMultiplier}, ` +
		`but got ${$pointwiseFilter.shape[2]}.`);
	const depthwise = depthwiseConv2d$1(x4D, $depthwiseFilter, strides, pad, dataFormat, dilation);
	const pointwiseStride = 1;
	const res = conv2d$1(depthwise, $pointwiseFilter, pointwiseStride, 'valid', dataFormat);
	if (reshapedTo4D) {
		return reshape$1(res, [res.shape[1], res.shape[2], res.shape[3]]);
	}
	return res;
}

const separableConv2d = /* @__PURE__ */ op({separableConv2d_});

/**
 * @license
 * Copyright 2020 Google Inc. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Computes the difference between two lists of numbers.
 *
 * Given a Tensor `x` and a Tensor `y`, this operation returns a Tensor `out`
 * that represents all values that are in `x` but not in `y`. The returned
 * Tensor `out` is sorted in the same order that the numbers appear in `x`
 * (duplicates are preserved). This operation also returns a Tensor indices that
 * represents the position of each out element in `x`. In other words:
 *
 * `out[i] = x[idx[i]] for i in [0, 1, ..., out.length - 1]`
 *
 * ```js
 * const x = [1, 2, 3, 4, 5, 6];
 * const y = [1, 3, 5];
 *
 * const [out, indices] = await tf.setdiff1dAsync(x, y);
 * out.print(); // [2, 4, 6]
 * indices.print(); // [1, 3, 5]
 * ```
 *
 * @param x 1-D Tensor. Values to keep.
 * @param y 1-D Tensor. Must have the same type as x. Values to exclude in the
 *     output.
 * @returns Promise of Tensor tuple [out, indices].
 *  out: Tensor with the same type as x.
 *  indices: A Tensor of type int32.
 *
 * @doc {heading: 'Tensors', subheading: 'Transformations'}
 */
async function setdiff1dAsync_(x, y) {
	const $x = convertToTensor(x, 'x', 'setdiff1d');
	const $y = convertToTensor(y, 'y', 'setdiff1d');
	assert($x.dtype === $y.dtype, () => `x and y should have the same dtype, but got x (${$x.dtype}) and y (${$y.dtype}).`);
	assert($x.rank === 1, () => `x should be 1D tensor, but got x (${$x.shape}).`);
	assert($y.rank === 1, () => `y should be 1D tensor, but got y (${$y.shape}).`);
	const xVals = await $x.data();
	const yVals = await $y.data();
	const ySet = new Set(yVals);
	let outputSize = 0;
	for (let i = 0; i < xVals.length; i++) {
		if (!ySet.has(xVals[i])) {
			outputSize++;
		}
	}
	const buffer = new TensorBuffer([outputSize], $x.dtype);
	const indices = new TensorBuffer([outputSize], 'int32');
	for (let i = 0, p = 0; i < xVals.length; i++) {
		if (!ySet.has(xVals[i])) {
			buffer.values[p] = xVals[i];
			indices.values[p] = i;
			p++;
		}
	}
	return [buffer.toTensor(), indices.toTensor()];
}

const setdiff1dAsync = setdiff1dAsync_;

/**
 * @license
 * Copyright 2018 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Returns an element-wise indication of the sign of a number.
 *
 * ```js
 * const x = tf.tensor1d([.6, 1.1, -3.3, NaN, 0]);
 *
 * x.sign().print();  // or tf.sign(x)
 * ```
 * @param x The input Tensor.
 *
 * @doc {heading: 'Operations', subheading: 'Basic math'}
 */
function sign_(x) {
	const $x = convertToTensor(x, 'x', 'sign');
	const inputs = {x: $x};
	return ENGINE.runKernel(Sign, inputs);
}

const sign = /* @__PURE__ */ op({sign_});

/**
 * @license
 * Copyright 2018 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Computes sin of the input Tensor element-wise: `sin(x)`
 *
 * ```js
 * const x = tf.tensor1d([0, Math.PI / 2, Math.PI * 3 / 4]);
 *
 * x.sin().print();  // or tf.sin(x)
 * ```
 * @param x The input tensor.
 *
 * @doc {heading: 'Operations', subheading: 'Basic math'}
 */
function sin_(x) {
	const $x = convertToTensor(x, 'x', 'sin', 'float32');
	const inputs = {x: $x};
	return ENGINE.runKernel(Sin, inputs);
}

const sin = /* @__PURE__ */ op({sin_});

/**
 * @license
 * Copyright 2018 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Computes hyperbolic sin of the input `tf.Tensor` element-wise: `sinh(x)`
 *
 * ```js
 * const x = tf.tensor1d([0, 1, -1, .7]);
 *
 * x.sinh().print();  // or tf.sinh(x)
 * ```
 * @param x The input tensor.
 *
 * @doc {heading: 'Operations', subheading: 'Basic math'}
 */
function sinh_(x) {
	const $x = convertToTensor(x, 'x', 'sinh');
	const inputs = {x: $x};
	return ENGINE.runKernel(Sinh, inputs);
}

const sinh = /* @__PURE__ */ op({sinh_});

/**
 * @license
 * Copyright 2018 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Extracts a 1D slice from 1D array starting at coordinates `begin` and is
 * of length `size`. See `slice` for details.
 */
function slice1d_(x, begin, size) {
	const $x = convertToTensor(x, 'x', 'slice1d');
	assert($x.rank === 1, () => `slice1d expects a rank-1 tensor, but got a rank-${$x.rank} tensor`);
	return slice$1($x, [begin], [size]);
}

const slice1d = /* @__PURE__ */ op({slice1d_});

/**
 * @license
 * Copyright 2018 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Extracts a 2D slice from a 2D array starting at coordinates `begin` and
 * is of size `size`. See `slice` for details.
 */
function slice2d_(x, begin, size) {
	const $x = convertToTensor(x, 'x', 'slice2d');
	assert($x.rank === 2, () => `slice2d expects a rank-2 tensor, but got a rank-${$x.rank} tensor`);
	return slice$1($x, begin, size);
}

const slice2d = /* @__PURE__ */ op({slice2d_});

/**
 * @license
 * Copyright 2018 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Extracts a 3D slice from a 3D array starting at coordinates `begin` and
 * is of size `size`. See `slice` for details.
 */
function slice3d_(x, begin, size) {
	const $x = convertToTensor(x, 'x', 'slice3d');
	assert($x.rank === 3, () => `slice3d expects a rank-3 tensor, but got a rank-${$x.rank} tensor`);
	return slice$1($x, begin, size);
}

const slice3d = /* @__PURE__ */ op({slice3d_});

/**
 * @license
 * Copyright 2018 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Extracts a 4D slice from a 4D array starting at coordinates `begin` and
 * is of size `size`. See `slice` for details.
 */
function slice4d_(x, begin, size) {
	const $x = convertToTensor(x, 'x', 'slice4d');
	assert($x.rank === 4, () => `slice4d expects a rank-4 tensor, but got a rank-${$x.rank} tensor`);
	return slice$1($x, begin, size);
}

const slice4d = /* @__PURE__ */ op({slice4d_});

/**
 * @license
 * Copyright 2018 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Computes the softmax normalized vector given the logits.
 *
 * ```js
 * const a = tf.tensor1d([1, 2, 3]);
 *
 * a.softmax().print();  // or tf.softmax(a)
 * ```
 *
 * ```js
 * const a = tf.tensor2d([2, 4, 6, 1, 2, 3], [2, 3]);
 *
 * a.softmax().print();  // or tf.softmax(a)
 * ```
 *
 * @param logits The logits array.
 * @param dim The dimension softmax would be performed on. Defaults to `-1`
 *     which indicates the last dimension.
 *
 * @doc {heading: 'Operations', subheading: 'Normalization'}
 */
function softmax_(logits, dim = -1) {
	const $logits = convertToTensor(logits, 'logits', 'softmax', 'float32');
	if (dim === -1) {
		dim = $logits.rank - 1;
	}
	if (dim !== $logits.rank - 1) {
		throw Error('Softmax along a non-last dimension is not yet supported. ' +
			`Logits was rank ${$logits.rank} and dim was ${dim}`);
	}
	const inputs = {logits: $logits};
	const attrs = {dim};
	return ENGINE.runKernel(Softmax, inputs, attrs);
}

const softmax = /* @__PURE__ */ op({softmax_});

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Fast Fourier transform.
 *
 * Computes the 1-dimensional discrete Fourier transform over the inner-most
 * dimension of input.
 *
 * ```js
 * const real = tf.tensor1d([1, 2, 3]);
 * const imag = tf.tensor1d([1, 2, 3]);
 * const x = tf.complex(real, imag);
 *
 * x.fft().print();  // tf.spectral.fft(x).print();
 * ```
 * @param input The complex input to compute an fft over.
 *
 * @doc {heading: 'Operations', subheading: 'Spectral', namespace: 'spectral'}
 */
function fft_(input) {
	assert(input.dtype === 'complex64', () => `The dtype for tf.spectral.fft() must be complex64 ` +
		`but got ${input.dtype}.`);
	const inputs = {input};
	return ENGINE.runKernel(FFT, inputs);
}

const fft = /* @__PURE__ */ op({fft_});

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Inverse fast Fourier transform.
 *
 * Computes the inverse 1-dimensional discrete Fourier transform over the
 * inner-most dimension of input.
 *
 * ```js
 * const real = tf.tensor1d([1, 2, 3]);
 * const imag = tf.tensor1d([1, 2, 3]);
 * const x = tf.complex(real, imag);
 *
 * x.ifft().print();  // tf.spectral.ifft(x).print();
 * ```
 * @param input The complex input to compute an ifft over.
 *
 * @doc {heading: 'Operations', subheading: 'Spectral', namespace: 'spectral'}
 */
function ifft_(input) {
	assert(input.dtype === 'complex64', () => `The dtype for tf.spectral.ifft() must be complex64 ` +
		`but got ${input.dtype}.`);
	const inputs = {input};
	return ENGINE.runKernel(IFFT, inputs);
}

const ifft = /* @__PURE__ */ op({ifft_});

/**
 * @license
 * Copyright 2018 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Inversed real value input fast Fourier transform.
 *
 * Computes the 1-dimensional inversed discrete Fourier transform over the
 * inner-most dimension of the real input.
 *
 * ```js
 * const real = tf.tensor1d([1, 2, 3]);
 * const imag = tf.tensor1d([0, 0, 0]);
 * const x = tf.complex(real, imag);
 *
 * x.irfft().print();
 * ```
 * @param input The real value input to compute an irfft over.
 *
 * @doc {heading: 'Operations', subheading: 'Spectral', namespace: 'spectral'}
 */
function irfft_(input) {
	const innerDimensionSize = input.shape[input.shape.length - 1];
	const batch = input.size / innerDimensionSize;
	let ret;
	if (innerDimensionSize <= 2) {
		const complexInput = reshape$1(input, [batch, innerDimensionSize]);
		ret = ifft(complexInput);
	} else {
		// The length of unique components of the DFT of a real-valued signal
		// is 2 * (input_len - 1)
		const outputShape = [batch, 2 * (innerDimensionSize - 1)];
		const realInput = reshape$1(real$1(input), [batch, innerDimensionSize]);
		const imagInput = reshape$1(imag$1(input), [batch, innerDimensionSize]);
		const realConjugate = reverse(slice$1(realInput, [0, 1], [batch, innerDimensionSize - 2]), 1);
		const imagConjugate = mul(reverse(slice$1(imagInput, [0, 1], [batch, innerDimensionSize - 2]), 1), scalar(-1));
		const r = concat([realInput, realConjugate], 1);
		const i = concat([imagInput, imagConjugate], 1);
		const complexInput = reshape$1(complex$1(r, i), [outputShape[0], outputShape[1]]);
		ret = ifft(complexInput);
	}
	ret = real$1(ret);
	// reshape the result if the input is 3D tensor.
	if (input.rank === 3 && input.shape[0] !== 0) {
		const temp = ret;
		const batch = input.shape[0];
		ret = reshape$1(ret, [batch, ret.shape[0] / batch, ret.shape[1]]);
		temp.dispose();
	}
	return ret;
}

const irfft = /* @__PURE__ */ op({irfft_});

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Splits a `tf.Tensor` into sub tensors.
 *
 * If `numOrSizeSplits` is a number, splits `x` along dimension `axis`
 * into `numOrSizeSplits` smaller tensors.
 * Requires that `numOrSizeSplits` evenly divides `x.shape[axis]`.
 *
 * If `numOrSizeSplits` is a number array, splits `x` into
 * `numOrSizeSplits.length` pieces. The shape of the `i`-th piece has the
 * same size as `x` except along dimension `axis` where the size is
 * `numOrSizeSplits[i]`.
 *
 * ```js
 * const x = tf.tensor2d([1, 2, 3, 4, 5, 6, 7, 8], [2, 4]);
 * const [a, b] = tf.split(x, 2, 1);
 * a.print();
 * b.print();
 *
 * const [c, d, e] = tf.split(x, [1, 2, 1], 1);
 * c.print();
 * d.print();
 * e.print();
 * ```
 *
 * @param x The input tensor to split.
 * @param numOrSizeSplits Either an integer indicating the number of
 * splits along the axis or an array of integers containing the sizes of
 * each output tensor along the axis. If a number then it must evenly divide
 * `x.shape[axis]`; otherwise the sum of sizes must match `x.shape[axis]`.
 * Can contain one -1 indicating that dimension is to be inferred.
 * @param axis The dimension along which to split. Defaults to 0 (the first
 * dim).
 *
 * @doc {heading: 'Tensors', subheading: 'Slicing and Joining'}
 */
function split_(x, numOrSizeSplits, axis = 0) {
	const $x = convertToTensor(x, 'x', 'split');
	const inputs = {x: $x};
	const attr = {numOrSizeSplits, axis};
	return ENGINE.runKernel(SplitV, inputs, attr);
}

const split$1 = /* @__PURE__ */ op({split_});

/**
 * @license
 * Copyright 2018 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Real value input fast Fourier transform.
 *
 * Computes the 1-dimensional discrete Fourier transform over the
 * inner-most dimension of the real input.
 *
 * ```js
 * const real = tf.tensor1d([1, 2, 3]);
 *
 * real.rfft().print();
 * ```
 * @param input The real value input to compute an rfft over.
 *
 * @doc {heading: 'Operations', subheading: 'Spectral', namespace: 'spectral'}
 */
function rfft_(input, fftLength) {
	assert(input.dtype === 'float32', () => `The dtype for rfft() must be real value but got ${input.dtype}`);
	let innerDimensionSize = input.shape[input.shape.length - 1];
	const batch = input.size / innerDimensionSize;
	let adjustedInput;
	if (fftLength != null && fftLength < innerDimensionSize) {
		// Need to crop
		const begin = input.shape.map(v => 0);
		const size = input.shape.map(v => v);
		size[input.shape.length - 1] = fftLength;
		adjustedInput = slice$1(input, begin, size);
		innerDimensionSize = fftLength;
	} else if (fftLength != null && fftLength > innerDimensionSize) {
		// Need to pad with zeros
		const zerosShape = input.shape.map(v => v);
		zerosShape[input.shape.length - 1] = fftLength - innerDimensionSize;
		adjustedInput = concat([input, zeros$1(zerosShape)], input.shape.length - 1);
		innerDimensionSize = fftLength;
	} else {
		adjustedInput = input;
	}
	// Complement the input with zero imaginary numbers.
	const zerosInput = zerosLike$1(adjustedInput);
	const complexInput = reshape$1(complex$1(adjustedInput, zerosInput), [batch, innerDimensionSize]);
	const ret = fft(complexInput);
	// Exclude complex conjugations. These conjugations are put symmetrically.
	const half = Math.floor(innerDimensionSize / 2) + 1;
	const realValues = real$1(ret);
	const imagValues = imag$1(ret);
	const realComplexConjugate = split$1(realValues, [half, innerDimensionSize - half], realValues.shape.length - 1);
	const imagComplexConjugate = split$1(imagValues, [half, innerDimensionSize - half], imagValues.shape.length - 1);
	const outputShape = adjustedInput.shape.slice();
	outputShape[adjustedInput.shape.length - 1] = half;
	return reshape$1(complex$1(realComplexConjugate[0], imagComplexConjugate[0]), outputShape);
}

const rfft = /* @__PURE__ */ op({rfft_});

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Returns (a - b) * (a - b) element-wise.
 * Supports broadcasting.
 *
 * ```js
 * const a = tf.tensor1d([1, 4, 3, 16]);
 * const b = tf.tensor1d([1, 2, 9, 4]);
 *
 * a.squaredDifference(b).print();  // or tf.squaredDifference(a, b)
 * ```
 *
 * ```js
 * // Broadcast squared difference  a with b.
 * const a = tf.tensor1d([2, 4, 6, 8]);
 * const b = tf.scalar(5);
 *
 * a.squaredDifference(b).print();  // or tf.squaredDifference(a, b)
 * ```
 *
 * @param a The first tensor.
 * @param b The second tensor. Must have the same type as `a`.
 *
 * @doc {heading: 'Operations', subheading: 'Arithmetic'}
 */
function squaredDifference_(a, b) {
	let $a = convertToTensor(a, 'a', 'squaredDifference');
	let $b = convertToTensor(b, 'b', 'squaredDifference');
	[$a, $b] = makeTypesMatch($a, $b);
	assertAndGetBroadcastShape($a.shape, $b.shape);
	const inputs = {a: $a, b: $b};
	const attrs = {};
	return ENGINE.runKernel(SquaredDifference, inputs, attrs);
}

const squaredDifference = /* @__PURE__ */ op({squaredDifference_});

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Removes dimensions of size 1 from the shape of a `tf.Tensor`.
 *
 * ```js
 * const x = tf.tensor([1, 2, 3, 4], [1, 1, 4]);
 * x.squeeze().print();
 * ```
 *
 * @param x The input tensor to be squeezed.
 * @param axis An optional list of numbers. If specified, only
 *     squeezes the dimensions listed. The dimension index starts at 0. It
 * is an error to squeeze a dimension that is not 1.
 *
 * @doc {heading: 'Tensors', subheading: 'Transformations'}
 */
function squeeze_(x, axis) {
	const $x = convertToTensor(x, 'x', 'squeeze', 'string_or_numeric');
	return reshape$1($x, squeezeShape($x.shape, axis).newShape);
}

const squeeze = /* @__PURE__ */ op({squeeze_});

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Stacks a list of rank-`R` `tf.Tensor`s into one rank-`(R+1)` `tf.Tensor`.
 *
 * ```js
 * const a = tf.tensor1d([1, 2]);
 * const b = tf.tensor1d([3, 4]);
 * const c = tf.tensor1d([5, 6]);
 * tf.stack([a, b, c]).print();
 * ```
 *
 * @param tensors A list of tensor objects with the same shape and dtype.
 * @param axis The axis to stack along. Defaults to 0 (the first dim).
 *
 * @doc {heading: 'Tensors', subheading: 'Slicing and Joining'}
 */
function stack_(tensors, axis = 0) {
	const $tensors = convertToTensorArray(tensors, 'tensors', 'stack', 'string_or_numeric');
	assert($tensors.length >= 1, () => 'Pass at least one tensor to tf.stack');
	if ($tensors.length > 0) {
		assert(axis <= $tensors[0].rank, () => 'Axis must be <= rank of the tensor');
	}
	const inputs = $tensors;
	const attrs = {axis};
	return ENGINE.runKernel(Pack, inputs, attrs);
}

const stack = /* @__PURE__ */ op({stack_});

/**
 * @license
 * Copyright 2018 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Computes step of the input `tf.Tensor` element-wise: `x > 0 ? 1 : alpha`
 *
 * ```js
 * const x = tf.tensor1d([0, 2, -1, -3]);
 *
 * x.step(.5).print();  // or tf.step(x, .5)
 * ```
 * @param x The input tensor.
 * @param alpha The gradient when input is negative. Defaults to 0.
 *
 * @doc {heading: 'Operations', subheading: 'Basic math'}
 */
function step_(x, alpha = 0.0) {
	const $x = convertToTensor(x, 'x', 'step');
	const inputs = {x: $x};
	const attrs = {alpha};
	return ENGINE.runKernel(Step, inputs, attrs);
}

const step$1 = /* @__PURE__ */ op({step_});

/**
 * @license
 * Copyright 2018 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Extracts a strided slice of a tensor.
 *
 * Roughly speaking, this op extracts a slice of size (end-begin)/stride from
 * the given input tensor (x). Starting at the location specified by begin the
 * slice continues by adding stride to the index until all dimensions are not
 * less than end. Note that a stride can be negative, which causes a reverse
 * slice.
 *
 * ```js
 * const t = tf.tensor3d([1, 1, 1 ,2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5, 5, 6, 6, 6],
 *    [3, 2, 3]);
 * t.stridedSlice([1, 0, 0], [2, 1, 3], [1, 1, 1]).print()  // [[[3, 3, 3]]]
 * t.stridedSlice([1, 0, 0], [2, 2, 3], [1, 1, 1]).print()  // [[[3, 3, 3],
 *                                                     // [4, 4, 4]]]
 * t.stridedSlice([1, -1, 0], [2, -3, 3], [1, -1, 1]).print() // [[[4, 4, 4],
 *                                                     // [3, 3, 3]]]
 * ```
 *
 * @param x The tensor to stride slice.
 * @param begin The coordinates to start the slice from.
 * @param end: The coordinates to end the slice at.
 * @param strides: The size of the slice.
 * @param beginMask: If the ith bit of beginMask is set, begin[i] is ignored
 *      and the fullest possible range in that dimension is used instead.
 * @param endMask: If the ith bit of endMask is set, end[i] is ignored
 *      and the fullest possible range in that dimension is used instead.
 * @param shrinkAxisMask: a bitmask where bit i implies that
 * the ith specification should shrink the dimensionality. begin and end must
 * imply a slice of size 1 in the dimension.
 *
 * @doc {heading: 'Operations', subheading: 'Slicing and Joining'}
 */
function stridedSlice_(x, begin, end, strides, beginMask = 0, endMask = 0, ellipsisMask = 0, newAxisMask = 0, shrinkAxisMask = 0) {
	const $x = convertToTensor(x, 'x', 'stridedSlice', 'string_or_numeric');
	const inputs = {x: $x};
	const attrs = {
		begin,
		end,
		strides,
		beginMask,
		endMask,
		ellipsisMask,
		newAxisMask,
		shrinkAxisMask
	};
	return ENGINE.runKernel(StridedSlice, inputs, attrs);
}

const stridedSlice = /* @__PURE__ */ op({stridedSlice_});

/**
 * @license
 * Copyright 2018 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Computes tan of the input `tf.Tensor` element-wise, `tan(x)`
 *
 * ```js
 * const x = tf.tensor1d([0, Math.PI / 2, Math.PI * 3 / 4]);
 *
 * x.tan().print();  // or tf.tan(x)
 * ```
 * @param x The input tensor.
 *
 * @doc {heading: 'Operations', subheading: 'Basic math'}
 */
function tan_(x) {
	const $x = convertToTensor(x, 'x', 'tan', 'float32');
	const inputs = {x: $x};
	return ENGINE.runKernel(Tan, inputs);
}

const tan = /* @__PURE__ */ op({tan_});

/**
 * @license
 * Copyright 2018 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Creates rank-1 `tf.Tensor` with the provided values, shape and dtype.
 *
 * The same functionality can be achieved with `tf.tensor`, but in general
 * we recommend using `tf.tensor1d` as it makes the code more readable.
 *
 * ```js
 * tf.tensor1d([1, 2, 3]).print();
 * ```
 *
 * @param values The values of the tensor. Can be array of numbers,
 *     or a `TypedArray`.
 * @param dtype The data type.
 *
 * @doc {heading: 'Tensors', subheading: 'Creation'}
 */
function tensor1d(values, dtype) {
	assertNonNull(values);
	const inferredShape = inferShape(values, dtype);
	if (inferredShape.length !== 1) {
		throw new Error('tensor1d() requires values to be a flat/TypedArray');
	}
	const shape = null;
	return makeTensor(values, shape, inferredShape, dtype);
}

/**
 * @license
 * Copyright 2018 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Creates rank-2 `tf.Tensor` with the provided values, shape and dtype.
 *
 * The same functionality can be achieved with `tf.tensor`, but in general
 * we recommend using `tf.tensor2d` as it makes the code more readable.
 *
 *  ```js
 * // Pass a nested array.
 * tf.tensor2d([[1, 2], [3, 4]]).print();
 * ```
 * ```js
 * // Pass a flat array and specify a shape.
 * tf.tensor2d([1, 2, 3, 4], [2, 2]).print();
 * ```
 *
 * @param values The values of the tensor. Can be nested array of numbers,
 *     or a flat array, or a `TypedArray`.
 * @param shape The shape of the tensor. If not provided, it is inferred from
 *     `values`.
 * @param dtype The data type.
 *
 * @doc {heading: 'Tensors', subheading: 'Creation'}
 */
function tensor2d(values, shape, dtype) {
	assertNonNull(values);
	if (shape != null && shape.length !== 2) {
		throw new Error('tensor2d() requires shape to have two numbers');
	}
	const inferredShape = inferShape(values, dtype);
	if (inferredShape.length !== 2 && inferredShape.length !== 1) {
		throw new Error('tensor2d() requires values to be number[][] or flat/TypedArray');
	}
	if (inferredShape.length === 1 && shape == null) {
		throw new Error('tensor2d() requires shape to be provided when `values` ' +
			'are a flat/TypedArray');
	}
	return makeTensor(values, shape, inferredShape, dtype);
}

/**
 * @license
 * Copyright 2018 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Creates rank-4 `tf.Tensor` with the provided values, shape and dtype.
 *
 * The same functionality can be achieved with `tf.tensor`, but in general
 * we recommend using `tf.tensor4d` as it makes the code more readable.
 *
 *  ```js
 * // Pass a nested array.
 * tf.tensor4d([[[[1], [2]], [[3], [4]]]]).print();
 * ```
 * ```js
 * // Pass a flat array and specify a shape.
 * tf.tensor4d([1, 2, 3, 4], [1, 2, 2, 1]).print();
 * ```
 *
 * @param values The values of the tensor. Can be nested array of numbers,
 *     or a flat array, or a `TypedArray`.
 * @param shape The shape of the tensor. Optional. If not provided,
 *   it is inferred from `values`.
 * @param dtype The data type.
 *
 * @doc {heading: 'Tensors', subheading: 'Creation'}
 */
function tensor4d(values, shape, dtype) {
	assertNonNull(values);
	if (shape != null && shape.length !== 4) {
		throw new Error('tensor4d() requires shape to have four numbers');
	}
	const inferredShape = inferShape(values, dtype);
	if (inferredShape.length !== 4 && inferredShape.length !== 1) {
		throw new Error('tensor4d() requires values to be number[][][][] or flat/TypedArray');
	}
	if (inferredShape.length === 1 && shape == null) {
		throw new Error('tensor4d() requires shape to be provided when `values` ' +
			'are a flat array');
	}
	return makeTensor(values, shape, inferredShape, dtype);
}

/**
 * @license
 * Copyright 2018 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Creates rank-5 `tf.Tensor` with the provided values, shape and dtype.
 *
 * The same functionality can be achieved with `tf.tensor`, but in general
 * we recommend using `tf.tensor5d` as it makes the code more readable.
 *
 *  ```js
 * // Pass a nested array.
 * tf.tensor5d([[[[[1],[2]],[[3],[4]]],[[[5],[6]],[[7],[8]]]]]).print();
 * ```
 * ```js
 * // Pass a flat array and specify a shape.
 * tf.tensor5d([1, 2, 3, 4, 5, 6, 7, 8], [1, 2, 2, 2, 1]).print();
 * ```
 *
 * @param values The values of the tensor. Can be nested array of numbers,
 *     or a flat array, or a `TypedArray`.
 * @param shape The shape of the tensor. Optional. If not provided,
 *   it is inferred from `values`.
 * @param dtype The data type.
 *
 * @doc {heading: 'Tensors', subheading: 'Creation'}
 */
function tensor5d(values, shape, dtype) {
	assertNonNull(values);
	if (shape != null && shape.length !== 5) {
		throw new Error('tensor5d() requires shape to have five numbers');
	}
	const inferredShape = inferShape(values, dtype);
	if (inferredShape.length !== 5 && inferredShape.length !== 1) {
		throw new Error('tensor5d() requires values to be ' +
			'number[][][][][] or flat/TypedArray');
	}
	if (inferredShape.length === 1 && shape == null) {
		throw new Error('tensor5d() requires shape to be provided when `values` ' +
			'are a flat array');
	}
	return makeTensor(values, shape, inferredShape, dtype);
}

/**
 * @license
 * Copyright 2018 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Creates rank-6 `tf.Tensor` with the provided values, shape and dtype.
 *
 * The same functionality can be achieved with `tf.tensor`, but in general
 * we recommend using `tf.tensor6d` as it makes the code more readable.
 *
 *  ```js
 * // Pass a nested array.
 * tf.tensor6d([[[[[[1],[2]],[[3],[4]]],[[[5],[6]],[[7],[8]]]]]]).print();
 * ```
 * ```js
 * // Pass a flat array and specify a shape.
 * tf.tensor6d([1, 2, 3, 4, 5, 6, 7, 8], [1, 1, 2, 2, 2, 1]).print();
 * ```
 *
 * @param values The values of the tensor. Can be nested array of numbers,
 *     or a flat array, or a `TypedArray`.
 * @param shape The shape of the tensor. Optional. If not provided,
 *   it is inferred from `values`.
 * @param dtype The data type.
 *
 * @doc {heading: 'Tensors', subheading: 'Creation'}
 */
function tensor6d(values, shape, dtype) {
	assertNonNull(values);
	if (shape != null && shape.length !== 6) {
		throw new Error('tensor6d() requires shape to have six numbers');
	}
	const inferredShape = inferShape(values, dtype);
	if (inferredShape.length !== 6 && inferredShape.length !== 1) {
		throw new Error('tensor6d() requires values to be number[][][][][][] or ' +
			'flat/TypedArray');
	}
	if (inferredShape.length === 1 && shape == null) {
		throw new Error('tensor6d() requires shape to be provided when `values` ' +
			'are a flat array');
	}
	shape = shape ||
		inferredShape;
	return makeTensor(values, shape, inferredShape, dtype);
}

/**
 * @license
 * Copyright 2022 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Creates a new tensor by applying sparse updates to individual
 * values or slices to the passed in tensor according to
 * indices. This operator is the similar to scatterNd op, except that the
 * udpates are scattered on an existing tensor (as opposed to a zero-tensor).
 *
 * If indices contains duplicates, then we pick the last update for the index.
 *
 * If an out of bound index is found on CPU, an error is returned.
 *
 * Warning: There are some GPU specific semantics for this operation.
 *  - If an out of bound index is found, the index is ignored.
 *  - The order in which updates are applied is nondeterministic, so the output
 * will be nondeterministic if indices contains duplicates.
 * ```js
 * const shape = [8];
 * const tensor = tf.ones(shape);
 * const indices = tf.tensor2d([4, 3, 1, 7], [4, 1], 'int32');
 * const updates = tf.tensor1d([9, 10, 11, 12]);
 *
 * tf.tensorScatterUpdate(tensor, indices, updates).print();
 *    //[1, 11, 1, 10, 9, 1, 1, 12]
 * ```
 *
 * @param tensor A Tensor. Tensor to copy/update.
 * @param indices The tensor contains the indices into the output tensor, must
 *     have at least 2 axes: (num_updates, index_depth).
 * @param updates The tensor contains the value for the indices.
 *
 * @doc {heading: 'Operations', subheading: 'Slicing and Joining'}
 */
function tensorScatterUpdate_(tensor, indices, updates) {
	const $tensor = convertToTensor(tensor, 'tensor', 'tensorScatterupdate');
	const $indices = convertToTensor(indices, 'indices', 'tensorScatterupdate', 'int32');
	const $updates = convertToTensor(updates, 'updates', 'tensorScatterupdate');
	validateInput$1($updates, $indices, $tensor.shape);
	if ($tensor.dtype !== $updates.dtype) {
		throw new Error(`tensor and updates must have the same dtype, instead they are ${$tensor.dtype} and ${$updates.dtype}.`);
	}
	const inputs = {
		tensor: $tensor,
		indices: $indices,
		updates: $updates
	};
	const attrs = {};
	// tslint:disable-next-line: no-unnecessary-type-assertion
	return ENGINE.runKernel(TensorScatterUpdate, inputs, attrs);
}

const tensorScatterUpdate = op({tensorScatterUpdate_});

/**
 * @license
 * Copyright 2018 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Finds the values and indices of the `k` largest entries along the last
 * dimension.
 *
 * If the input is a vector (rank=1), finds the k largest entries in the vector
 * and outputs their values and indices as vectors. Thus values[j] is the j-th
 * largest entry in input, and its index is indices[j].
 * For higher rank inputs, computes the top k entries along the last dimension.
 *
 * If two elements are equal, the lower-index element appears first.
 *
 * ```js
 * const a = tf.tensor2d([[1, 5], [4, 3]]);
 * const {values, indices} = tf.topk(a);
 * values.print();
 * indices.print();
 * ```
 * @param x 1-D or higher `tf.Tensor` with last dimension being at least `k`.
 * @param k Number of top elements to look for along the last dimension.
 * @param sorted If true, the resulting `k` elements will be sorted by the
 *     values in descending order.
 *
 * @doc {heading: 'Operations', subheading: 'Evaluation'}
 */
function topk_(x, k = 1, sorted = true) {
	const $x = convertToTensor(x, 'x', 'topk');
	if ($x.rank === 0) {
		throw new Error('topk() expects the input to be of rank 1 or higher');
	}
	const lastDim = $x.shape[$x.shape.length - 1];
	if (k < 0) {
		throw new Error(`'k' passed to topk() must be >= 0 but got ${k}`);
	}
	if (k > lastDim) {
		throw new Error(`'k' passed to topk() must be <= the last dimension (${lastDim}) ` +
			`but got ${k}`);
	}
	const inputs = {x: $x};
	const attrs = {k, sorted};
	const [values, indices] = ENGINE.runKernel(TopK, inputs, attrs);
	return {values, indices};
}

const topk = /* @__PURE__ */ op({topk_});

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Creates a `tf.Tensor` with values sampled from a truncated normal
 * distribution.
 *
 * ```js
 * tf.truncatedNormal([2, 2]).print();
 * ```
 *
 * The generated values follow a normal distribution with specified mean and
 * standard deviation, except that values whose magnitude is more than 2
 * standard deviations from the mean are dropped and re-picked.
 *
 * @param shape An array of integers defining the output tensor shape.
 * @param mean The mean of the normal distribution.
 * @param stdDev The standard deviation of the normal distribution.
 * @param dtype The data type of the output tensor.
 * @param seed The seed for the random number generator.
 *
 * @doc {heading: 'Tensors', subheading: 'Creation'}
 */
function truncatedNormal_(shape, mean = 0, stdDev = 1, dtype, seed) {
	assertNonNegativeIntegerDimensions(shape);
	if (dtype != null && dtype === 'bool') {
		throw new Error(`Unsupported data type $ { dtype }`);
	}
	const randGauss = new MPRandGauss(mean, stdDev, dtype, true /* truncated */, seed);
	const res = buffer(shape, dtype);
	for (let i = 0; i < res.values.length; i++) {
		res.values[i] = randGauss.nextValue();
	}
	return res.toTensor();
}

const truncatedNormal = /* @__PURE__ */ op({truncatedNormal_});

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Finds unique elements along an axis of a tensor.
 *
 * It returns a tensor `values` containing all of the unique elements along the
 * `axis` of the given tensor `x` in the same order that they occur along the
 * `axis` in `x`; `x` does not need to be sorted. It also returns a tensor
 * `indices` the same size as the number of the elements in `x` along the `axis`
 * dimension. It contains the index in the unique output `values`.
 *
 * ```js
 * // A 1-D tensor
 * const a = tf.tensor1d([1, 1, 2, 4, 4, 4, 7, 8, 8]);
 * const {values, indices} = tf.unique(a);
 * values.print();   // [1, 2, 4, 7, 8,]
 * indices.print();  // [0, 0, 1, 2, 2, 2, 3, 4, 4]
 * ```
 *
 * ```js
 * // A 2-D tensor with axis=0
 * //
 * // 'a' is: [[1, 0, 0],
 * //          [1, 0, 0],
 * //          [2, 0, 0]]
 * const a = tf.tensor2d([[1, 0, 0], [1, 0, 0], [2, 0, 0]]);
 * const {values, indices} = tf.unique(a, 0)
 * values.print();   // [[1, 0, 0],
 *                   //  [2, 0, 0]]
 * indices.print();  // [0, 0, 1]
 * ```
 *
 * ```js
 * // A 2-D tensor with axis=1
 * //
 * // 'a' is: [[1, 0, 0],
 * //          [1, 0, 0],
 * //          [2, 0, 0]]
 * const a = tf.tensor2d([[1, 0, 0], [1, 0, 0], [2, 0, 0]]);
 * const {values, indices} = tf.unique(a, 1)
 * values.print();   // [[1, 0],
 *                   //  [1, 0],
 *                   //  [2, 0]]
 * indices.print();  // [0, 1, 1]
 * ```
 * @param x A tensor (int32, string, bool).
 * @param axis The axis of the tensor to find the unique elements.
 * @returns [uniqueElements, indices] (see above for details)
 *
 * @doc {heading: 'Operations', subheading: 'Evaluation'}
 */
function unique_(x, axis = 0) {
	const $x = convertToTensor(x, 'x', 'unique', 'string_or_numeric');
	assert($x.rank > 0, () => 'The input tensor must be at least 1D');
	const inputs = {x: $x};
	const attrs = {axis};
	const [values, indices] = ENGINE.runKernel(Unique, inputs, attrs);
	return {values, indices};
}

const unique = /* @__PURE__ */ op({unique_});

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Computes the sum along segments of a `tf.Tensor`.
 *
 * ```js
 * const x = tf.tensor1d([1, 2, 3, 4]);
 * const segmentIds = tf.tensor1d([1, 2, 0, 1], 'int32');
 * const numSegments = 3;
 *
 * x.unsortedSegmentSum(segmentIds, numSegments).print()
 * //or tf.unsortedSegmentSum(x, segmentIds, numSegments)
 * ```
 * @param x The `tf.Tensor` that will be summed along its segments.
 * @param segmentIds A `tf.Tensor1D` whose rank is equal to the rank of `x`'s
 * dimension along the `axis`.  Maps each element of `x` to a segment.
 * @param numSegments The number of distinct `segmentIds`.
 *
 * @doc {heading: 'Operations', subheading: 'Segment'}
 */
function unsortedSegmentSum_(x, segmentIds, numSegments) {
	const $x = convertToTensor(x, 'x', 'unsortedSegmentSum');
	const $segmentIds = convertToTensor(segmentIds, 'segmentIds', 'unsortedSegmentSum', 'int32');
	assert(isInt(numSegments), () => 'numSegments must be of dtype int');
	const inputs = {x: $x, segmentIds: $segmentIds};
	const attrs = {numSegments};
	return ENGINE.runKernel(UnsortedSegmentSum, inputs, attrs);
}

const unsortedSegmentSum = /* @__PURE__ */ op({unsortedSegmentSum_});

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Unstacks a `tf.Tensor` of rank-`R` into a list of rank-`(R-1)` `tf.Tensor`s.
 *
 * ```js
 * const a = tf.tensor2d([1, 2, 3, 4], [2, 2]);
 *
 * tf.unstack(a).forEach(tensor => tensor.print());
 * ```
 *
 * @param x A tensor object.
 * @param axis The axis to unstack along. Defaults to 0 (the first dim).
 *
 * @doc {heading: 'Tensors', subheading: 'Slicing and Joining'}
 */
function unstack_(x, axis = 0) {
	const $x = convertToTensor(x, 'x', 'unstack', 'string_or_numeric');
	assert(axis >= -$x.shape.length && axis < $x.shape.length, () => `Axis = ${axis} is not in [-${$x.shape.length}, ${$x.shape.length})`);
	const inputs = {value: $x};
	const attrs = {axis};
	return ENGINE.runKernel(Unpack, inputs, attrs);
}

const unstack = /* @__PURE__ */ op({unstack_});

/**
 * @license
 * Copyright 2022 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Searches for where a value would go in a sorted sequence.
 *
 * This is not a method for checking containment (like javascript in).
 *
 * The typical use case for this operation is "binning", "bucketing", or
 * "discretizing". The values are assigned to bucket-indices based on the edges
 * listed in 'sortedSequence'. This operation returns the bucket-index for each
 * value.
 *
 * The index returned corresponds to the first edge greater than the value.
 *
 * The axis is not settable for this operation. It always operates on the
 * innermost dimension (axis=-1). The operation will accept any number of outer
 * dimensions.
 *
 * Note: This operation assumes that 'upperBound' is sorted along the
 * innermost axis, maybe using 'sort(..., axis=-1)'. If the sequence is not
 * sorted no error is raised and the content of the returned tensor is not well
 * defined.
 *
 * ```js
 * const seq = tf.tensor1d([0, 3, 9, 10, 10]);
 * const values = tf.tensor1d([0, 4, 10]);
 * const result = tf.upperBound(seq, values);
 * result.print(); // [1, 2, 5]
 * ```
 * @param sortedSequence: N-D. Sorted sequence.
 * @param values: N-D. Search values.
 * @return An N-D int32 tensor the size of values containing the result of
 *     applying upper bound to each value. The result is not a global index to
 *     the entire Tensor, but the index in the last dimension.
 * @doc {heading: 'Operations', subheading: 'Evaluation'}
 */
function upperBound(sortedSequence, values) {
	return searchSorted(sortedSequence, values, 'right');
}

/**
 * @license
 * Copyright 2018 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Creates a new variable with the provided initial value.
 * ```js
 * const x = tf.variable(tf.tensor([1, 2, 3]));
 * x.assign(tf.tensor([4, 5, 6]));
 *
 * x.print();
 * ```
 *
 * @param initialValue Initial value for the tensor.
 * @param trainable If true, optimizers are allowed to update it.
 * @param name Name of the variable. Defaults to a unique id.
 * @param dtype If set, initialValue will be converted to the given type.
 *
 * @doc {heading: 'Tensors', subheading: 'Creation'}
 */
function variable(initialValue, trainable = true, name, dtype) {
	return ENGINE.makeVariable(initialValue, trainable, name, dtype);
}

/**
 * @license
 * Copyright 2018 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/** An implementation of the Where kernel shared between cpu and webgl */
function whereImpl$1(condShape, condVals) {
	const indices = [];
	for (let i = 0; i < condVals.length; i++) {
		if (condVals[i]) {
			indices.push(i);
		}
	}
	const inBuffer = buffer(condShape, 'int32');
	const out = buffer([indices.length, condShape.length], 'int32');
	for (let i = 0; i < indices.length; i++) {
		const loc = inBuffer.indexToLoc(indices[i]);
		const offset = i * condShape.length;
		out.values.set(loc, offset);
	}
	return out.toTensor();
}

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Returns the coordinates of true elements of condition.
 *
 * The coordinates are returned in a 2-D tensor where the first dimension (rows)
 * represents the number of true elements, and the second dimension (columns)
 * represents the coordinates of the true elements. Keep in mind, the shape of
 * the output tensor can vary depending on how many true values there are in
 * input. Indices are output in row-major order. The resulting tensor has the
 * shape `[numTrueElems, condition.rank]`.
 *
 * This is analogous to calling the python `tf.where(cond)` without an x or y.
 *
 * ```js
 * const cond = tf.tensor1d([false, false, true], 'bool');
 * const result = await tf.whereAsync(cond);
 * result.print();
 * ```
 *
 * @doc {heading: 'Operations', subheading: 'Logical'}
 */
async function whereAsync_(condition) {
	const $condition = convertToTensor(condition, 'condition', 'whereAsync', 'bool');
	const vals = await $condition.data();
	const res = whereImpl$1($condition.shape, vals);
	if (condition !== $condition) {
		$condition.dispose();
	}
	return res;
}

const whereAsync = whereAsync_;

/**
 * @license
 * Copyright 2018 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Apply boolean mask to tensor.
 *
 * ```js
 * const tensor = tf.tensor2d([1, 2, 3, 4, 5, 6], [3, 2]);
 * const mask = tf.tensor1d([1, 0, 1], 'bool');
 * const result = await tf.booleanMaskAsync(tensor, mask);
 * result.print();
 * ```
 *
 * @param tensor N-D tensor.
 * @param mask K-D boolean tensor, K <= N and K must be known statically.
 * @param axis A 0-D int Tensor representing the axis in tensor to mask from.
 *     By default, axis is 0 which will mask from the first dimension.
 *     Otherwise K + axis <= N.
 *
 * @doc {heading: 'Tensors', subheading: 'Slicing and Joining'}
 */
async function booleanMaskAsync_(tensor, mask, axis) {
	const $tensor = convertToTensor(tensor, 'tensor', 'boolMask');
	const $mask = convertToTensor(mask, 'mask', 'boolMask', 'bool');
	const axisFrom = axis == null ? 0 : axis;
	const maskDim = $mask.rank;
	const tensorShape = $tensor.shape;
	assert(maskDim > 0, () => 'mask cannot be scalar');
	assertShapesMatch(tensorShape.slice(axisFrom, axisFrom + maskDim), $mask.shape, `mask's shape must match the first K dimensions of tensor's shape,`);
	let leadingSize = 1;
	for (let i = axisFrom; i < axisFrom + maskDim; i++) {
		leadingSize *= tensorShape[i];
	}
	const targetTensorShape = tensorShape.slice(0, axisFrom)
										 .concat([leadingSize], tensorShape.slice(axisFrom + maskDim));
	const reshapedTensor = reshape$1($tensor, targetTensorShape);
	const reshapedMask = reshape$1($mask, [-1]);
	const positivePositions = await whereAsync(reshapedMask);
	const indices = squeeze(positivePositions, [1]);
	const res = gather(reshapedTensor, indices, axisFrom);
	// Ensure no memory leak.
	if (tensor !== $tensor) {
		$tensor.dispose();
	}
	if (mask !== $mask) {
		$mask.dispose();
	}
	indices.dispose();
	reshapedTensor.dispose();
	reshapedMask.dispose();
	positivePositions.dispose();
	return res;
}

const booleanMaskAsync = booleanMaskAsync_;

/**
 * @license
 * Copyright 2018 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Compute the moving average of a variable.
 *
 * Without zeroDebias, the moving average operation is defined by:
 *   `v += delta`
 * where
 *   `delta = (1 - decay) * (x - v)`
 *
 * With zeroDebias (default), the `delta` term is scaled to debias the
 * effect of the (assumed) zero-initialization of `v`.
 *   `delta /= (1 - decay ^ step)`
 *
 * For more details on the zero-debiasing algorithm, see:
 *   https://arxiv.org/abs/1412.6980
 *
 * Note that this function is completely stateless and does not keep track of
 * step count. The step count needs to be maintained by the caller and passed
 * in as `step`.
 *
 * @param v The current moving average value.
 * @param x New input value, must have the same shape and dtype as `v`.
 * @param decay The decay factor. Typical values are 0.95 and 0.99.
 * @param step Step count.
 * @param zeroDebias: Whether zeroDebias is to be performed (default: `true`).
 * @returns The new moving average value.
 *
 * @doc {heading: 'Operations', subheading: 'Moving Average'}
 */
function movingAverage_(v, x, decay, step, zeroDebias = true) {
	const $v = convertToTensor(v, 'v', 'movingAverage');
	const $x = convertToTensor(x, 'x', 'movingAverage');
	const $decay = convertToTensor(decay, 'decay', 'movingAverage');
	assertTypesMatch($v, $x);
	assert(arraysEqual($v.shape, $x.shape), () => 'Shape mismatch in v and x');
	const one = scalar(1);
	const oneMinusDecay = sub$1(one, $decay);
	let update = mul(sub$1($x, $v), oneMinusDecay);
	if (zeroDebias) {
		assert(step != null, () => 'When using zeroDebias: true, step is required.');
		const $step = convertToTensor(step, 'step', 'movingAverage');
		update = div$1(update, sub$1(one, pow($decay, $step)));
	}
	return add$1($v, update);
}

const movingAverage = /* @__PURE__ */ op({movingAverage_});

/**
 * @license
 * Copyright 2018 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Creates a new tensor by applying sparse updates to individual
 * values or slices within a zero tensor of the given shape tensor according to
 * indices. This operator is the inverse of the `tf.gatherND` operator which
 * extracts values or slices from a given tensor.
 *
 * ```js
 * const indices = tf.tensor2d([4, 3, 1, 7], [4, 1], 'int32');
 * const updates = tf.tensor1d([9, 10, 11, 12]);
 * const shape = [8];
 * tf.scatterND(indices, updates, shape).print() //[0, 11, 0, 10, 9, 0, 0, 12]
 * ```
 *
 * @param indices The tensor contains the indices into the output tensor.
 * @param updates The tensor contains the value for the indices.
 * @param shape: The shape of the output tensor.
 *
 * @doc {heading: 'Operations', subheading: 'Slicing and Joining'}
 */
function scatterND_(indices, updates, shape) {
	assertNonNegativeIntegerDimensions(shape);
	const $indices = convertToTensor(indices, 'indices', 'scatterND', 'int32');
	const $updates = convertToTensor(updates, 'updates', 'scatterND');
	validateInput$1($updates, $indices, shape);
	const inputs = {indices: $indices, updates: $updates};
	const attrs = {shape};
	// tslint:disable-next-line: no-unnecessary-type-assertion
	return ENGINE.runKernel(ScatterNd, inputs, attrs);
}

const scatterND = /* @__PURE__ */ op({scatterND_});

/**
 * Validate sparseToDense inputs.
 *
 * @param sparseIndices A 0-D, 1-D, or 2-D Tensor of type int32.
 * sparseIndices[i] contains the complete index where sparseValues[i] will be
 * placed.
 * @param sparseValues A 0-D or 1-D Tensor. Values
 * corresponding to each row of sparseIndices, or a scalar value to be used for
 * all sparse indices.
 * @param outputShape number[]. Shape of the dense output tensor.
 * @param validateIndices boolean. indice validation is not supported, error
 * will be thrown if it is set.
 */
function validateInput(sparseIndices, sparseValues, outputShape, defaultValues) {
	if (sparseIndices.dtype !== 'int32') {
		throw new Error('tf.sparseToDense() expects the indices to be int32 type,' +
			` but the dtype was ${sparseIndices.dtype}.`);
	}
	if (sparseIndices.rank > 2) {
		throw new Error('sparseIndices should be a scalar, vector, or matrix,' +
			` but got shape ${sparseIndices.shape}.`);
	}
	const numElems = sparseIndices.rank > 0 ? sparseIndices.shape[0] : 1;
	const numDims = sparseIndices.rank > 1 ? sparseIndices.shape[1] : 1;
	if (outputShape.length !== numDims) {
		throw new Error('outputShape has incorrect number of elements:,' +
			` ${outputShape.length}, should be: ${numDims}.`);
	}
	const numValues = sparseValues.size;
	if (!(sparseValues.rank === 0 ||
		sparseValues.rank === 1 && numValues === numElems)) {
		throw new Error('sparseValues has incorrect shape ' +
			`${sparseValues.shape}, should be [] or [${numElems}]`);
	}
	if (sparseValues.dtype !== defaultValues.dtype) {
		throw new Error('sparseValues.dtype must match defaultValues.dtype');
	}
}

/**
 * @license
 * Copyright 2018 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Converts a sparse representation into a dense tensor.
 *
 * Builds an array dense with shape outputShape such that:
 *
 * // If sparseIndices is scalar
 * dense[i] = (i == sparseIndices ? sparseValues : defaultValue)
 *
 * // If sparseIndices is a vector, then for each i
 * dense[sparseIndices[i]] = sparseValues[i]
 *
 * // If sparseIndices is an n by d matrix, then for each i in [0, n)
 * dense[sparseIndices[i][0], ..., sparseIndices[i][d-1]] = sparseValues[i]
 * All other values in dense are set to defaultValue. If sparseValues is a
 * scalar, all sparse indices are set to this single value.
 *
 * If indices are repeated the final value is summed over all values for those
 * indices.
 *
 * ```js
 * const indices = tf.tensor1d([4, 5, 6, 1, 2, 3], 'int32');
 * const values = tf.tensor1d([10, 11, 12, 13, 14, 15], 'float32');
 * const shape = [8];
 * tf.sparseToDense(indices, values, shape).print();
 * ```
 *
 * @param sparseIndices A 0-D, 1-D, or 2-D Tensor of type int32.
 * sparseIndices[i] contains the complete index where sparseValues[i] will be
 * placed.
 * @param sparseValues A 0-D or 1-D Tensor. Values
 * corresponding to each row of sparseIndices, or a scalar value to be used for
 * all sparse indices.
 * @param outputShape Shape of the dense output tensor. The type is inferred.
 * @param defaultValue Scalar. Value to set for indices not specified in
 * sparseIndices. Defaults to zero.
 *
 * @doc {heading: 'Operations', subheading: 'Normalization'}
 */
function sparseToDense_(sparseIndices, sparseValues, outputShape, defaultValue = 0) {
	assertNonNegativeIntegerDimensions(outputShape);
	const $sparseIndices = convertToTensor(sparseIndices, 'sparseIndices', 'sparseToDense', 'int32');
	const $sparseValues = convertToTensor(sparseValues, 'sparseValues', 'sparseToDense', 'string_or_numeric');
	const $defaultValue = convertToTensor(defaultValue, 'defaultValue', 'sparseToDense', $sparseValues.dtype);
	validateInput($sparseIndices, $sparseValues, outputShape, $defaultValue);
	const inputs = {
		sparseIndices: $sparseIndices,
		sparseValues: $sparseValues,
		defaultValue: $defaultValue
	};
	const attrs = {outputShape};
	return ENGINE.runKernel(SparseToDense, inputs, attrs);
}

const sparseToDense = /* @__PURE__ */ op({sparseToDense_});

/**
 * @license
 * Copyright 2018 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Gather slices from input tensor into a Tensor with shape specified by
 * `indices`.
 *
 * `indices` is a K-dimensional integer tensor, best thought of as a
 * (K-1)-dimensional tensor of indices into input, where each element defines a
 * slice of input:
 * output[\\(i_0, ..., i_{K-2}\\)] = input[indices[\\(i_0, ..., i_{K-2}\\)]]
 *
 * Whereas in `tf.gather`, `indices` defines slices into the first dimension of
 * input, in `tf.gatherND`, `indices` defines slices into the first N dimensions
 * of input, where N = indices.shape[-1].
 *
 * The last dimension of indices can be at most the rank of input:
 * indices.shape[-1] <= input.rank
 *
 * The last dimension of `indices` corresponds to elements
 * (if indices.shape[-1] == input.rank) or slices
 * (if indices.shape[-1] < input.rank) along dimension indices.shape[-1] of
 * input.
 * The output tensor has shape
 * indices.shape[:-1] + input.shape[indices.shape[-1]:]
 *
 * Note that on CPU, if an out of bound index is found, an error is returned. On
 * GPU, if an out of bound index is found, a 0 is stored in the corresponding
 * output value.
 *
 * ```js
 * const indices = tf.tensor2d([0, 1, 1, 0], [2,2], 'int32');
 * const input = tf.tensor2d([9, 10, 11, 12], [2, 2]);
 * tf.gatherND(input, indices).print() // [10, 11]
 * ```
 *
 * @param x The tensor from which to gather values.
 * @param indices Index tensor, must be of type int32.
 *
 * @doc {heading: 'Operations', subheading: 'Slicing and Joining'}
 */
function gatherND_(x, indices) {
	const $indices = convertToTensor(indices, 'indices', 'gatherND', 'int32');
	const $x = convertToTensor(x, 'x', 'gatherND', 'string_or_numeric');
	const inputs = {params: $x, indices: $indices};
	return ENGINE.runKernel(GatherNd, inputs);
}

const gatherND = /* @__PURE__ */ op({gatherND_});

/**
 * @license
 * Copyright 2019 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Normalize noise shape based on provided tensor and noise shape.
 *
 * @param x Tensor.
 * @param noiseShape The shape for the randomly generated keep/drop flags, as
 *   an array of numbers. Optional.
 * @returns Normalized noise shape.
 */
function getNoiseShape(x, noiseShape) {
	if (noiseShape == null) {
		return x.shape.slice();
	}
	if (arraysEqual(x.shape, noiseShape)) {
		return noiseShape;
	}
	if (x.shape.length === noiseShape.length) {
		const newDimension = [];
		for (let i = 0; i < x.shape.length; i++) {
			if (noiseShape[i] == null && x.shape[i] != null) {
				newDimension.push(x.shape[i]);
			} else {
				newDimension.push(noiseShape[i]);
			}
		}
		return newDimension;
	}
	return noiseShape;
}

/**
 * @license
 * Copyright 2018 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Computes dropout.
 *
 * ```js
 * const x = tf.tensor1d([1, 2, 2, 1]);
 * const rate = 0.75;
 * const output = tf.dropout(x, rate);
 * output.print();
 * ```
 *
 * @param x A floating point Tensor or TensorLike.
 * @param rate A float in the range [0, 1). The probability that each element
 *   of x is discarded.
 * @param noiseShape An array of numbers of type int32, representing the
 * shape for randomly generated keep/drop flags. If the noiseShape has null
 * value, it will be automatically replaced with the x's relative dimension
 * size. Optional.
 * @param seed Used to create random seeds. Optional.
 * @returns A Tensor of the same shape of x.
 *
 * @doc {heading: 'Operations', subheading: 'Dropout'}
 */
function dropout_(x, rate, noiseShape, seed) {
	const $x = convertToTensor(x, 'x', 'dropout');
	assert($x.dtype === 'float32', () => `x has to be a floating point tensor since it's going to be ` +
		`scaled, but got a ${$x.dtype} tensor instead.`);
	assert(rate >= 0 && rate < 1, () => `rate must be a float in the range [0, 1), but got ${rate}.`);
	if (rate === 0) {
		return x instanceof Tensor ? $x.clone() : $x;
	}
	const $noiseShape = getNoiseShape($x, noiseShape);
	const keepProb = 1 - rate;
	const multiplier = div$1(floor$1(add$1(randomUniform($noiseShape, 0, 1, 'float32', seed), keepProb)), keepProb);
	return mul($x, multiplier);
}

const dropout = /* @__PURE__ */ op({dropout_});

/**
 * @license
 * Copyright 2019 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
function enclosingPowerOfTwo(value) {
	// Return 2**N for integer N such that 2**N >= value.
	return Math.floor(Math.pow(2, Math.ceil(Math.log(value) / Math.log(2.0))));
}

function cosineWindow(windowLength, a, b) {
	const even = 1 - windowLength % 2;
	const newValues = new Float32Array(windowLength);
	for (let i = 0; i < windowLength; ++i) {
		const cosArg = (2.0 * Math.PI * i) / (windowLength + even - 1);
		newValues[i] = a - b * Math.cos(cosArg);
	}
	return tensor1d(newValues, 'float32');
}

/**
 * @license
 * Copyright 2019 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Returns whether the targets are in the top K predictions.
 *
 * ```js
 * const predictions = tf.tensor2d([[20, 10, 40, 30], [30, 50, -20, 10]]);
 * const targets = tf.tensor1d([2, 0]);
 * const precision = await tf.inTopKAsync(predictions, targets);
 * precision.print();
 * ```
 * @param predictions 2-D or higher `tf.Tensor` with last dimension being
 *     at least `k`.
 * @param targets 1-D or higher `tf.Tensor`.
 * @param k Optional Number of top elements to look at for computing precision,
 *     default to 1.
 *
 * @doc {heading: 'Operations', subheading: 'Evaluation'}
 */
async function inTopKAsync_(predictions, targets, k = 1) {
	const $predictions = convertToTensor(predictions, 'predictions', 'inTopK');
	const $targets = convertToTensor(targets, 'targets', 'inTopK');
	assert($predictions.rank > 1, () => 'inTopK() expects the predictions to be of rank 2 or higher, ' +
		`but got ${$predictions.rank}`);
	assert($predictions.rank - 1 === $targets.rank, () => `predictions rank should be 1 larger than ` +
		`targets rank, but got predictions rank ` +
		`${$predictions.rank} and targets rank ${$targets.rank}`);
	assertShapesMatch($predictions.shape.slice(0, $predictions.shape.length - 1), $targets.shape, `predictions's shape should be align with the targets' shape, ` +
		'except the last dimension.');
	const lastDim = $predictions.shape[$predictions.shape.length - 1];
	assert(k > 0 && k <= lastDim, () => `'k' passed to inTopK() must be > 0 && <= the predictions last ` +
		`dimension (${lastDim}), but got ${k}`);
	const predictionsVals = await $predictions.data();
	const targetsVals = await $targets.data();
	// Reshape predictionsVals into a 2d tensor [batch, lastDim]
	// and look up topK along lastDim.
	const [batch, size] = [predictionsVals.length / lastDim, lastDim];
	const precision = getTypedArrayFromDType('bool', batch);
	for (let b = 0; b < batch; b++) {
		const offset = b * size;
		const vals = predictionsVals.subarray(offset, offset + size);
		const valAndInd = [];
		for (let i = 0; i < vals.length; i++) {
			valAndInd.push({value: vals[i], index: i});
		}
		valAndInd.sort((a, b) => b.value - a.value);
		precision[b] = 0;
		for (let i = 0; i < k; i++) {
			if (valAndInd[i].index === targetsVals[b]) {
				precision[b] = 1;
				break;
			}
		}
	}
	if (predictions !== $predictions) {
		$predictions.dispose();
	}
	if (targets !== $targets) {
		$targets.dispose();
	}
	// Output precision has the same shape as targets.
	return tensor(precision, $targets.shape, 'bool');
}

const inTopKAsync = inTopKAsync_;

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Computes the derivative of the filter of a 2D convolution.
 *
 * @param x The input tensor, of rank 4 or rank 3 of shape
 *     [batch, height, width, inChannels]. If rank 3, batch of 1 is assumed.
 * @param dy The dy image, of rank 4 or rank 3, of shape
 *     [batch, height, width, outDepth]. If rank 3, batch of 1 is assumed.
 * @param filterShape The shape of the filter, length 4,
 *     [filterHeight, filterWidth, inDepth, outDepth].
 * @param strides The strides of the convolution: [strideHeight,
 * strideWidth].
 * @param pad A string from: 'same', 'valid'. The type of padding algorithm
 *     used in the forward prop of the op.
 * @param dataFormat: An optional string from: "NHWC", "NCHW". Defaults to
 *     "NHWC". Specify the data format of the input and output data. With the
 *     default format "NHWC", the data is stored in the order of: [batch,
 *     height, width, channels].
 * @param dimRoundingMode A string from: 'ceil', 'round', 'floor'. If none is
 *     provided, it will default to truncate.
 */
function conv2DBackpropFilter_(x, dy, filterShape, strides, pad, dataFormat = 'NHWC', dimRoundingMode) {
	let x4D = x;
	if (x.rank === 3) {
		x4D = reshape$1(x, [1, x.shape[0], x.shape[1], x.shape[2]]);
	}
	let dy4D = dy;
	if (dy4D.rank === 3) {
		dy4D = reshape$1(dy, [1, dy.shape[0], dy.shape[1], dy.shape[2]]);
	}
	assert(x4D.rank === 4, () => `Error in conv2dDerFilter: input must be rank 4, but got shape ` +
		`${x4D.shape}.`);
	assert(dy4D.rank === 4, () => `Error in conv2dDerFilter: dy must be rank 4, but got shape ` +
		`${dy4D.shape}.`);
	assert(filterShape.length === 4, () => `Error in conv2dDerFilter: filterShape must be length 4, but got ` +
		`${filterShape}.`);
	const inDepth = dataFormat === 'NHWC' ? x4D.shape[3] : x4D.shape[1];
	const outDepth = dataFormat === 'NHWC' ? dy4D.shape[3] : dy4D.shape[1];
	assert(inDepth === filterShape[2], () => `Error in conv2dDerFilter: depth of input ${inDepth}) must ` +
		`match input depth in filter (${filterShape[2]}.`);
	assert(outDepth === filterShape[3], () => `Error in conv2dDerFilter: depth of dy (${outDepth}) must ` +
		`match output depth for filter (${filterShape[3]}).`);
	checkPadOnDimRoundingMode('conv2dDerFilter', pad, dimRoundingMode);
	const inputs = {x: x4D, dy: dy4D};
	const attrs = {strides, pad, dataFormat, dimRoundingMode, filterShape};
	// tslint:disable-next-line: no-unnecessary-type-assertion
	return ENGINE.runKernel(Conv2DBackpropFilter, inputs, attrs);
}

const conv2DBackpropFilter = /* @__PURE__ */ op({conv2DBackpropFilter_});

/**
 * @license
 * Copyright 2019 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
// Returns gradient for fused activation.
function getFusedDyActivation(dy, y, activation) {
	if (activation == null || activation === 'linear') {
		return dy;
	}
	if (activation === 'relu') {
		return mul(dy, step$1(y));
	}
	throw new Error(`Cannot compute gradient for fused activation ${activation}.`);
}

// Returns gradient for fused bias.
function getFusedBiasGradient(bias, dyActivation) {
	let res = dyActivation;
	const reduceAxes = getReductionAxes(bias.shape, dyActivation.shape);
	if (reduceAxes.length > 0) {
		res = sum$1(res, reduceAxes);
	}
	return reshape$1(res, bias.shape);
}

function applyActivation$1(x, activation, preluActivationWeights, leakyreluAlpha) {
	if (activation === 'linear') {
		return x;
	} else if (activation === 'relu') {
		return relu$1(x);
	} else if (activation === 'elu') {
		return elu$1(x);
	} else if (activation === 'relu6') {
		return relu6$1(x);
	} else if (activation === 'prelu') {
		return prelu$1(x, preluActivationWeights);
	} else if (activation === 'leakyrelu') {
		return leakyRelu$1(x, leakyreluAlpha);
	} else if (activation === 'sigmoid') {
		return sigmoid$1(x);
	}
	throw new Error(`Unknown fused activation ${activation}.`);
}

// Whether we should call fused ops.
const shouldFuse = (gradientDepth, activation) => {
	const gradientMode = gradientDepth > 0;
	return !gradientMode || activation === 'linear';
};

/**
 * @license
 * Copyright 2019 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Computes a 2D convolution over the input x, optionally fused with adding a
 * bias and applying an activation.
 *
 * ```js
 * const inputDepth = 2;
 * const inShape = [2, 2, 2, inputDepth];
 * const outputDepth = 2;
 * const fSize = 1;
 * const pad = 0;
 * const strides = 1;
 *
 * const x = tf.tensor4d( [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15,
 * 16], inShape);
 * const w = tf.tensor4d([-1, 1, -2, 0.5], [fSize, fSize, inputDepth,
 * outputDepth]);
 *
 * tf.fused.conv2d({ x, filter: w, strides, pad, dataFormat: 'NHWC',
 * dilations: [1, 1], bias: tf.scalar(5), activation: 'relu' }).print();
 * ```
 *
 * @param obj An object with the following properties:
 * @param x The input tensor, of rank 4 or rank 3, of shape
 *     `[batch, height, width, inChannels]`. If rank 3, batch of 1 is
 * assumed.
 * @param filter The filter, rank 4, of shape
 *     `[filterHeight, filterWidth, inDepth, outDepth]`.
 * @param strides The strides of the convolution: `[strideHeight,
 * strideWidth]`.
 * @param pad The type of padding algorithm.
 *   - `same` and stride 1: output will be of same size as input,
 *       regardless of filter size.
 *   - `valid` output will be smaller than input if filter is larger
 *       than 1x1.
 *   - For more info, see this guide:
 *     [https://www.tensorflow.org/api_docs/python/tf/nn/convolution](
 *          https://www.tensorflow.org/api_docs/python/tf/nn/convolution)
 * @param dataFormat An optional string from: "NHWC", "NCHW". Defaults to
 *     "NHWC". Specify the data format of the input and output data. With the
 *     default format "NHWC", the data is stored in the order of: [batch,
 *     height, width, channels]. Only "NHWC" is currently supported.
 * @param dilations The dilation rates: `[dilationHeight, dilationWidth]`
 *     in which we sample input values across the height and width dimensions
 *     in atrous convolution. Defaults to `[1, 1]`. If `dilations` is a single
 *     number, then `dilationHeight == dilationWidth`. If it is greater than
 *     1, then all values of `strides` must be 1.
 * @param dimRoundingMode A string from: 'ceil', 'round', 'floor'. If none is
 *     provided, it will default to truncate.
 * @param bias Tensor to be added to the result.
 * @param activation Name of activation kernel (defaults to `linear`) to be
 *     applied
 *      after biasAdd.
 * @param preluActivationWeights Tensor of prelu weights to be applied as part
 *     of a `prelu` activation, typically the same shape as `x`.
 * @param leakyreluAlpha Optional. Alpha to be applied as part of a `leakyrelu`
 *     activation.
 */
function fusedConv2d_({
						  x,
						  filter,
						  strides,
						  pad,
						  dataFormat = 'NHWC',
						  dilations = [1, 1],
						  dimRoundingMode,
						  bias,
						  activation = 'linear',
						  preluActivationWeights,
						  leakyreluAlpha
					  }) {
	activation = activation || 'linear';
	if (shouldFuse(ENGINE.state.gradientDepth, activation) === false) {
		// TODO: Transpose bias and preluActivationWeights properly for NCHW
		// format before computation.
		assert(dataFormat === 'NHWC', () => `Error in fused conv2d: got dataFormat of ${dataFormat} but ` +
			`only NHWC is currently supported for the case of gradient depth ` +
			`is 0 and the activation is not linear.`);
		let result = conv2d$1(x, filter, strides, pad, dataFormat, dilations, dimRoundingMode);
		if (bias != null) {
			result = add$1(result, bias);
		}
		return applyActivation$1(result, activation, preluActivationWeights, leakyreluAlpha);
	}
	const $x = convertToTensor(x, 'x', 'conv2d', 'float32');
	const $filter = convertToTensor(filter, 'filter', 'conv2d', 'float32');
	let x4D = $x;
	let reshapedTo4D = false;
	if ($x.rank === 3) {
		reshapedTo4D = true;
		x4D = reshape$1($x, [1, $x.shape[0], $x.shape[1], $x.shape[2]]);
	}
	assert(x4D.rank === 4, () => `Error in fused conv2d: input must be rank 4, but got rank ` +
		`${x4D.rank}.`);
	assert($filter.rank === 4, () => `Error in fused conv2d: filter must be rank 4, but got rank ` +
		`${$filter.rank}.`);
	checkPadOnDimRoundingMode('fused conv2d', pad, dimRoundingMode);
	const inputChannels = dataFormat === 'NHWC' ? x4D.shape[3] : x4D.shape[1];
	assert($filter.shape[2] === inputChannels, () => `Error in conv2d: depth of input (${inputChannels}) must match ` +
		`input depth for filter ${$filter.shape[2]}.`);
	assert(eitherStridesOrDilationsAreOne(strides, dilations), () => 'Error in conv2D: Either strides or dilations must be 1. ' +
		`Got strides ${strides} and dilations '${dilations}'`);
	const convInfo = computeConv2DInfo(x4D.shape, $filter.shape, strides, dilations, pad, dimRoundingMode);
	let $bias;
	if (bias != null) {
		$bias = convertToTensor(bias, 'bias', 'fused conv2d');
		[$bias] = makeTypesMatch($bias, $x);
		// According to TensorFlow, the bias is supposed be a 1-D tensor or a
		// scalar.
		//
		// 3-D or 4-D bias is not disabled for NHWC format, because they are
		// currently being used in some cases. For examplem in our code base,
		// https://github.com/tensorflow/tfjs/blob/b53bd47e880367ae57493f0ea628abaf08db2d5d/tfjs-core/src/ops/fused/fused_conv2d_test.ts#L1972.
		if (dataFormat === 'NHWC') {
			assertAndGetBroadcastShape(convInfo.outShape, $bias.shape);
		} else {
			assert($bias.shape.length <= 1, () => `Error in fused conv2d: only supports scalar or 1-D Tensor ` +
				`bias for NCHW format but got the bias of ` +
				`rank-${$bias.shape.length}.`);
			assert($bias.shape.length === 0 || $bias.shape[0] === convInfo.outChannels ||
				$bias.shape[0] === 1, () => `Error in fused conv2d: bias shape (${$bias.shape}) is not ` +
				`compatible with the number of output channels ` +
				`(${convInfo.outChannels})`);
		}
	}
	let $preluActivationWeights;
	if (preluActivationWeights != null) {
		// PReLU's activation weights could be a scalar, a 1-D tensor or a 3-D
		// tensor.
		const alphaShape = preluActivationWeights.shape;
		assert(alphaShape.length <= 1 || alphaShape.length === 3, () => `Error in fused conv2d: only supports scalar, 1-D Tensor or ` +
			`3-D Tensor PReLU activation weights but got a tensor of ` +
			`rank-${alphaShape.length}.`);
		if (alphaShape.length === 1) {
			// Whether the data format is NCHW or NHWC, the 1-D PReLU activation
			// weights tensor should be aligned with the output channels of conv2d
			// result.
			assert(alphaShape[0] === 1 || alphaShape[0] === convInfo.outChannels, () => `Error in fused conv2d: PReLU activation weights ` +
				`(${alphaShape}) is not compatible with the number of output ` +
				`channels (${convInfo.outChannels}).`);
		} else if (alphaShape.length === 3) {
			// Whether the data format is NCHW or NHWC, the PReLU activation weights
			// tensor should has the compatible shape with the result of conv2d.
			try {
				assertAndGetBroadcastShape(alphaShape, convInfo.outShape);
			} catch (e) {
				const errMsg = `Error in fused conv2d: PReLU activation weights (${alphaShape}) ` +
					`is not compatible with the output shape of the conv2d ` +
					`(${convInfo.outShape}).`;
				throw Error(errMsg);
			}
		}
		$preluActivationWeights = convertToTensor(preluActivationWeights, 'prelu weights', 'fused conv2d');
	}
	const grad = (dy, saved) => {
		assert(dataFormat === 'NHWC', () => `Error in gradient of fused conv2D: got dataFormat of ${dataFormat} but only NHWC is currently supported.`);
		const [$filter, x4D, y, $bias] = saved;
		const dyActivation = getFusedDyActivation(dy, y, activation);
		assert(tupleValuesAreOne(dilations), () => 'Error in gradient of fused conv2D: ' +
			`dilation rates greater than 1 ` +
			`are not yet supported in gradients. Got dilations '${dilations}'`);
		const xDer = conv2DBackpropInput(x4D.shape, dyActivation, $filter, strides, pad);
		const filterDer = conv2DBackpropFilter(x4D, dyActivation, $filter.shape, strides, pad);
		const der = [xDer, filterDer];
		if ($bias != null) {
			const biasDer = getFusedBiasGradient($bias, dyActivation);
			der.push(biasDer);
		}
		return der;
	};
	const inputs = {
		x: x4D,
		filter: $filter,
		bias: $bias,
		preluActivationWeights: $preluActivationWeights
	};
	const attrs = {
		strides,
		pad,
		dataFormat,
		dilations,
		dimRoundingMode,
		activation,
		leakyreluAlpha
	};
	// Depending on the the params passed in we will have different number of
	// inputs and thus a a different number of elements in the gradient.
	if (bias == null) {
		const customOp = customGrad((x4D, filter, save) => {
			let res =
				// tslint:disable-next-line: no-unnecessary-type-assertion
				ENGINE.runKernel(FusedConv2D, inputs, attrs);
			save([filter, x4D, res]);
			if (reshapedTo4D) {
				// tslint:disable-next-line: no-unnecessary-type-assertion
				res = reshape$1(res, [res.shape[1], res.shape[2], res.shape[3]]);
			}
			return {value: res, gradFunc: grad};
		});
		return customOp(x4D, $filter);
	} else {
		const customOpWithBias = customGrad((x4D, filter, bias, save) => {
			let res = ENGINE.runKernel(FusedConv2D, inputs, attrs);
			save([filter, x4D, res, bias]);
			if (reshapedTo4D) {
				// tslint:disable-next-line: no-unnecessary-type-assertion
				res = reshape$1(res, [res.shape[1], res.shape[2], res.shape[3]]);
			}
			return {value: res, gradFunc: grad};
		});
		return customOpWithBias(x4D, $filter, $bias);
	}
}

const conv2d = /* @__PURE__ */ op({fusedConv2d_});

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
function depthwiseConv2dNativeBackpropFilter_(x, dy, filterShape, strides, pad, dilations = [1, 1], dimRoundingMode) {
	let x4D = x;
	if (x.rank === 3) {
		x4D = reshape$1(x, [1, x.shape[0], x.shape[1], x.shape[2]]);
	}
	let dy4D = dy;
	if (dy4D.rank === 3) {
		dy4D = reshape$1(dy, [1, dy.shape[0], dy.shape[1], dy.shape[2]]);
	}
	const inputs = {x: x4D, dy: dy4D};
	const attrs = {strides, pad, dimRoundingMode, dilations, filterShape};
	// tslint:disable-next-line: no-unnecessary-type-assertion
	return ENGINE.runKernel(DepthwiseConv2dNativeBackpropFilter, inputs, attrs);
}

const depthwiseConv2dNativeBackpropFilter = op({depthwiseConv2dNativeBackpropFilter_});

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
function depthwiseConv2dNativeBackpropInput_(xShape, dy, filter, strides, pad, dilations = [1, 1], dimRoundingMode) {
	let dy4D = dy;
	let reshapedTo4D = false;
	if (dy.rank === 3) {
		reshapedTo4D = true;
		dy4D = reshape$1(dy, [1, dy.shape[0], dy.shape[1], dy.shape[2]]);
	}
	const inputs = {dy: dy4D, filter};
	const attrs = {strides, pad, dimRoundingMode, dilations, inputShape: xShape};
	const res =
		// tslint:disable-next-line: no-unnecessary-type-assertion
		ENGINE.runKernel(DepthwiseConv2dNativeBackpropInput, inputs, attrs);
	if (reshapedTo4D) {
		return reshape$1(res, [res.shape[1], res.shape[2], res.shape[3]]);
	}
	return res;
}

const depthwiseConv2dNativeBackpropInput = op({depthwiseConv2dNativeBackpropInput_});

/**
 * @license
 * Copyright 2019 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Computes depthwise 2D convolution, optionally fused with adding a
 * bias and applying an activation.
 *
 * Given a 4D `input` array and a `filter` array of shape
 * `[filterHeight, filterWidth, inChannels, channelMultiplier]` containing
 * `inChannels` convolutional filters of depth 1, this op applies a
 * different filter to each input channel (expanding from 1 channel to
 * `channelMultiplier` channels for each), then concatenates the results
 * together. The output has `inChannels * channelMultiplier` channels.
 *
 * See
 * [https://www.tensorflow.org/api_docs/python/tf/nn/depthwise_conv2d](
 *     https://www.tensorflow.org/api_docs/python/tf/nn/depthwise_conv2d)
 * for more details.
 *
 * @param obj An object with the following properties:
 * @param x The input tensor, of rank 4 or rank 3, of shape
 *     `[batch, height, width, inChannels]`. If rank 3, batch of 1 is
 * assumed.
 * @param filter The filter tensor, rank 4, of shape
 *     `[filterHeight, filterWidth, inChannels, channelMultiplier]`.
 * @param strides The strides of the convolution: `[strideHeight,
 * strideWidth]`. If strides is a single number, then `strideHeight ==
 * strideWidth`.
 * @param pad The type of padding algorithm.
 *   - `same` and stride 1: output will be of same size as input,
 *       regardless of filter size.
 *   - `valid`: output will be smaller than input if filter is larger
 *       than 1x1.
 *   - For more info, see this guide:
 *     [https://www.tensorflow.org/api_docs/python/tf/nn/convolution](
 *          https://www.tensorflow.org/api_docs/python/tf/nn/convolution)
 * @param dilations The dilation rates: `[dilationHeight, dilationWidth]`
 *     in which we sample input values across the height and width dimensions
 *     in atrous convolution. Defaults to `[1, 1]`. If `rate` is a single
 *     number, then `dilationHeight == dilationWidth`. If it is greater than
 *     1, then all values of `strides` must be 1.
 * @param dataFormat: An optional string from: "NHWC", "NCHW". Defaults to
 *     "NHWC". Specify the data format of the input and output data. With the
 *     default format "NHWC", the data is stored in the order of: [batch,
 *     height, width, channels]. Only "NHWC" is currently supported.
 * @param dimRoundingMode A string from: 'ceil', 'round', 'floor'. If none is
 *     provided, it will default to truncate.
 * @param bias Tensor to be added to the result.
 * @param activation Name of activation kernel (defaults to `linear`).
 * @param preluActivationWeights Tensor of prelu weights to be applied as part
 *     of a `prelu` activation, typically the same shape as `x`.
 * @param leakyreluAlpha Optional. Alpha to be applied as part of a `leakyrelu`
 *     activation.
 */
function fusedDepthwiseConv2d_({
								   x,
								   filter,
								   strides,
								   pad,
								   dataFormat = 'NHWC',
								   dilations = [1, 1],
								   dimRoundingMode,
								   bias,
								   activation = 'linear',
								   preluActivationWeights,
								   leakyreluAlpha
							   }) {
	if (shouldFuse(ENGINE.state.gradientDepth, activation) === false) {
		let result = depthwiseConv2d$1(x, filter, strides, pad, dataFormat, dilations, dimRoundingMode);
		if (bias != null) {
			result = add$1(result, bias);
		}
		return applyActivation$1(result, activation, preluActivationWeights, leakyreluAlpha);
	}
	const $x = convertToTensor(x, 'x', 'depthwiseConv2d', 'float32');
	const $filter = convertToTensor(filter, 'filter', 'depthwiseConv2d', 'float32');
	let x4D = $x;
	let reshapedTo4D = false;
	if ($x.rank === 3) {
		reshapedTo4D = true;
		x4D = reshape$1($x, [1, $x.shape[0], $x.shape[1], $x.shape[2]]);
	}
	assert(x4D.rank === 4, () => `Error in fused depthwiseConv2d: input must be rank 4, but got ` +
		`rank ${x4D.rank}.`);
	assert($filter.rank === 4, () => `Error in fused depthwiseConv2d: filter must be rank 4, ` +
		`but got rank ${$filter.rank}.`);
	assert(x4D.shape[3] === $filter.shape[2], () => `Error in fused depthwiseConv2d: number of input channels ` +
		`(${x4D.shape[3]}) must match the inChannels dimension in ` +
		`filter ${$filter.shape[2]}.`);
	if (dilations == null) {
		dilations = [1, 1];
	}
	assert(eitherStridesOrDilationsAreOne(strides, dilations), () => 'Error in fused depthwiseConv2d: Either strides or dilations must ' +
		`be 1. Got strides ${strides} and dilations '${dilations}'`);
	checkPadOnDimRoundingMode('fused depthwiseConv2d', pad, dimRoundingMode);
	const convInfo = computeConv2DInfo(x4D.shape, $filter.shape, strides, dilations, pad, dimRoundingMode, true /* depthwise */);
	let $bias;
	if (bias != null) {
		$bias = convertToTensor(bias, 'bias', 'fused conv2d');
		[$bias] = makeTypesMatch($bias, $x);
		assertAndGetBroadcastShape(convInfo.outShape, $bias.shape);
	}
	let $preluActivationWeights;
	if (preluActivationWeights != null) {
		$preluActivationWeights = convertToTensor(preluActivationWeights, 'prelu weights', 'fused depthwiseConv2d');
	}
	const grad = (dy, saved) => {
		assert(tupleValuesAreOne(dilations), () => 'Error in gradient of fused depthwiseConv2d: dilation rates ' +
			`greater than 1 are not yet supported. Got dilations ` +
			`'${dilations}'`);
		const [$filter, x4D, y, bias] = saved;
		const dyActivation = getFusedDyActivation(dy, y, activation);
		const xDer = depthwiseConv2dNativeBackpropInput(x4D.shape, dyActivation, $filter, strides, pad, dilations, dimRoundingMode);
		const filterDer = depthwiseConv2dNativeBackpropFilter(x4D, dyActivation, $filter.shape, strides, pad, dilations, dimRoundingMode);
		if (bias != null) {
			const biasDer = getFusedBiasGradient($bias, dyActivation);
			return [xDer, filterDer, biasDer];
		}
		return [xDer, filterDer];
	};
	const inputs = {
		x: x4D,
		filter: $filter,
		bias: $bias,
		preluActivationWeights: $preluActivationWeights
	};
	const attrs = {
		strides,
		pad,
		dataFormat,
		dilations,
		dimRoundingMode,
		activation,
		leakyreluAlpha
	};
	// Depending on the the params passed in we will have different number of
	// inputs and thus a a different number of elements in the gradient.
	if (bias == null) {
		const customOp = customGrad((x4D, filter, save) => {
			// tslint:disable-next-line: no-unnecessary-type-assertion
			let res = ENGINE.runKernel(FusedDepthwiseConv2D, inputs, attrs);
			save([filter, x4D, res]);
			if (reshapedTo4D) {
				// tslint:disable-next-line: no-unnecessary-type-assertion
				res = reshape$1(res, [res.shape[1], res.shape[2], res.shape[3]]);
			}
			return {value: res, gradFunc: grad};
		});
		return customOp(x4D, $filter);
	} else {
		const customOpWithBias = customGrad((x4D, filter, bias, save) => {
			// tslint:disable-next-line: no-unnecessary-type-assertion
			let res = ENGINE.runKernel(FusedDepthwiseConv2D, inputs, attrs);
			save([filter, x4D, res, bias]);
			if (reshapedTo4D) {
				// tslint:disable-next-line: no-unnecessary-type-assertion
				res = reshape$1(res, [res.shape[1], res.shape[2], res.shape[3]]);
			}
			return {value: res, gradFunc: grad};
		});
		return customOpWithBias(x4D, $filter, $bias);
	}
}

const depthwiseConv2d = /* @__PURE__ */ op({fusedDepthwiseConv2d_});

/**
 * @license
 * Copyright 2019 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Computes the dot product of two matrices with optional activation and bias.
 *
 * ```js
 * const a = tf.tensor2d([-1, -2], [1, 2]);
 * const b = tf.tensor2d([1, 2, 3, 4], [2, 2]);
 * const bias = tf.tensor2d([1, 2], [1, 2]);
 *
 * tf.fused.matMul({a, b, bias, activation: 'relu'}).print();
 * ```
 *
 * @param obj An object with the following properties:
 * - `a` First matrix in dot product operation.
 * - `b` Second matrix in dot product operation.
 * - `transposeA` If true, `a` is transposed before multiplication.
 * - `transposeB` If true, `b` is transposed before multiplication.
 * - `bias` Matrix to be added to the result.
 * - `activation` Name of activation kernel (defaults to `linear`).
 * - `preluActivationWeights` Tensor of prelu weights.
 * - `leakyreluAlpha` Alpha of leakyrelu.
 */
function fusedMatMul_({a, b, transposeA = false, transposeB = false, bias, activation = 'linear', preluActivationWeights, leakyreluAlpha = 0.2,}) {
	if (shouldFuse(ENGINE.state.gradientDepth, activation) === false) {
		let result = matMul$1(a, b, transposeA, transposeB);
		if (bias != null) {
			result = add$1(result, bias);
		}
		return applyActivation$1(result, activation, preluActivationWeights, leakyreluAlpha);
	}
	let $a = convertToTensor(a, 'a', 'fused matMul');
	let $b = convertToTensor(b, 'b', 'fused matMul');
	[$a, $b] = makeTypesMatch($a, $b);
	const innerShapeA = transposeA ? $a.shape[$a.rank - 2] : $a.shape[$a.rank - 1];
	const innerShapeB = transposeB ? $b.shape[$b.rank - 1] : $b.shape[$b.rank - 2];
	const outerShapeA = transposeA ? $a.shape[$a.rank - 1] : $a.shape[$a.rank - 2];
	const outerShapeB = transposeB ? $b.shape[$b.rank - 2] : $b.shape[$b.rank - 1];
	const outerDimsA = $a.shape.slice(0, -2);
	const outerDimsB = $b.shape.slice(0, -2);
	const batchDimA = sizeFromShape(outerDimsA);
	const batchDimB = sizeFromShape(outerDimsB);
	assert(innerShapeA === innerShapeB, () => `Error in fused matMul: inner shapes (${innerShapeA}) and (` +
		`${innerShapeB}) of Tensors with shapes ${$a.shape} and ` +
		`${$b.shape} and transposeA=${transposeA}` +
		` and transposeB=${transposeB} must match.`);
	const outShapeOuterDims = assertAndGetBroadcastShape($a.shape.slice(0, -2), $b.shape.slice(0, -2));
	const outShape = outShapeOuterDims.concat([outerShapeA, outerShapeB]);
	const a3D = transposeA ?
		reshape$1($a, [batchDimA, innerShapeA, outerShapeA]) :
		reshape$1($a, [batchDimA, outerShapeA, innerShapeA]);
	const b3D = transposeB ?
		reshape$1($b, [batchDimB, outerShapeB, innerShapeB]) :
		reshape$1($b, [batchDimB, innerShapeB, outerShapeB]);
	let $bias;
	if (bias != null) {
		$bias = convertToTensor(bias, 'bias', 'fused matMul');
		[$bias] = makeTypesMatch($bias, $a);
		assertAndGetBroadcastShape(outShape, $bias.shape);
	}
	let $preluActivationWeights;
	if (preluActivationWeights != null) {
		$preluActivationWeights = convertToTensor(preluActivationWeights, 'prelu weights', 'fused matMul');
	}
	const grad = (dy, saved) => {
		const [a3D, b3D, y, $bias] = saved;
		// we reshape dy because the result of the forward is not
		// necessarily going to be a 3d tensor due to a reshape done at the end of
		// the customOp.
		const dyActivation = getFusedDyActivation(reshape$1(dy, y.shape), y, activation);
		let aDer;
		let bDer;
		if (!transposeA && !transposeB) {
			aDer = matMul$1(dyActivation, b3D, false, true);
			bDer = matMul$1(a3D, dyActivation, true, false);
		} else if (!transposeA && transposeB) {
			aDer = matMul$1(dyActivation, b3D, false, false);
			bDer = matMul$1(dyActivation, a3D, true, false);
		} else if (transposeA && !transposeB) {
			aDer = matMul$1(b3D, dyActivation, false, true);
			bDer = matMul$1(a3D, dyActivation, false, false);
		} else {
			aDer = matMul$1(b3D, dyActivation, true, true);
			bDer = matMul$1(dyActivation, a3D, true, true);
		}
		if (bias != null) {
			const biasDer = getFusedBiasGradient($bias, dyActivation);
			return [aDer, bDer, biasDer];
		} else {
			return [aDer, bDer];
		}
	};
	const inputs = {
		a: a3D,
		b: b3D,
		bias: $bias,
		preluActivationWeights: $preluActivationWeights
	};
	const attrs = {transposeA, transposeB, activation, leakyreluAlpha};
	// Depending on the the params passed in we will have different number of
	// inputs and thus a a different number of elements in the gradient.
	if (bias == null) {
		const customOp = customGrad((a3D, b3D, save) => {
			const res =
				// tslint:disable-next-line: no-unnecessary-type-assertion
				ENGINE.runKernel(_FusedMatMul, inputs, attrs);
			save([a3D, b3D, res]);
			return {value: reshape$1(res, outShape), gradFunc: grad};
		});
		return customOp(a3D, b3D);
	} else {
		const customOpWithBias = customGrad((a3D, b3D, $bias, save) => {
			const res =
				// tslint:disable-next-line: no-unnecessary-type-assertion
				ENGINE.runKernel(_FusedMatMul, inputs, attrs);
			save([a3D, b3D, res, $bias]);
			return {value: reshape$1(res, outShape), gradFunc: grad};
		});
		return customOpWithBias(a3D, b3D, $bias);
	}
}

const matMul = /* @__PURE__ */ op({fusedMatMul_});

/**
 * @license
 * Copyright 2019 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */

var fused_ops = /*#__PURE__*/Object.freeze({
	__proto__: null,
	conv2d: conv2d,
	depthwiseConv2d: depthwiseConv2d,
	matMul: matMul
});

/**
 * @license
 * Copyright 2019 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Generate a hamming window.
 *
 * See: https://en.wikipedia.org/wiki/Window_function#Hann_and_Hamming_windows
 *
 * ```js
 * tf.signal.hammingWindow(10).print();
 * ```
 * @param The length of window
 *
 * @doc {heading: 'Operations', subheading: 'Signal', namespace: 'signal'}
 */
function hammingWindow_(windowLength) {
	return cosineWindow(windowLength, 0.54, 0.46);
}

const hammingWindow = /* @__PURE__ */ op({hammingWindow_});

/**
 * @license
 * Copyright 2019 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Generate a Hann window.
 *
 * See: https://en.wikipedia.org/wiki/Window_function#Hann_and_Hamming_windows
 *
 * ```js
 * tf.signal.hannWindow(10).print();
 * ```
 * @param The length of window
 *
 * @doc {heading: 'Operations', subheading: 'Signal', namespace: 'signal'}
 */
function hannWindow_(windowLength) {
	return cosineWindow(windowLength, 0.5, 0.5);
}

const hannWindow = /* @__PURE__ */ op({hannWindow_});

/**
 * @license
 * Copyright 2019 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Expands input into frames of frameLength.
 * Slides a window size with frameStep.
 *
 * ```js
 * tf.signal.frame([1, 2, 3], 2, 1).print();
 * ```
 * @param signal The input tensor to be expanded
 * @param frameLength Length of each frame
 * @param frameStep The frame hop size in samples.
 * @param padEnd Whether to pad the end of signal with padValue.
 * @param padValue A number to use where the input signal does
 *     not exist when padEnd is True.
 *
 * @doc {heading: 'Operations', subheading: 'Signal', namespace: 'signal'}
 */
function frame_(signal, frameLength, frameStep, padEnd = false, padValue = 0) {
	let start = 0;
	const output = [];
	while (start + frameLength <= signal.size) {
		output.push(slice$1(signal, start, frameLength));
		start += frameStep;
	}
	if (padEnd) {
		while (start < signal.size) {
			const padLen = (start + frameLength) - signal.size;
			const pad = concat([
				slice$1(signal, start, frameLength - padLen), fill$1([padLen], padValue)
			]);
			output.push(pad);
			start += frameStep;
		}
	}
	if (output.length === 0) {
		return tensor2d([], [0, frameLength]);
	}
	return reshape$1(concat(output), [output.length, frameLength]);
}

const frame = /* @__PURE__ */ op({frame_});

/**
 * @license
 * Copyright 2019 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Computes the Short-time Fourier Transform of signals
 * See: https://en.wikipedia.org/wiki/Short-time_Fourier_transform
 *
 * ```js
 * const input = tf.tensor1d([1, 1, 1, 1, 1])
 * tf.signal.stft(input, 3, 1).print();
 * ```
 * @param signal 1-dimensional real value tensor.
 * @param frameLength The window length of samples.
 * @param frameStep The number of samples to step.
 * @param fftLength The size of the FFT to apply.
 * @param windowFn A callable that takes a window length and returns 1-d tensor.
 *
 * @doc {heading: 'Operations', subheading: 'Signal', namespace: 'signal'}
 */
function stft_(signal, frameLength, frameStep, fftLength, windowFn = hannWindow) {
	if (fftLength == null) {
		fftLength = enclosingPowerOfTwo(frameLength);
	}
	const framedSignal = frame(signal, frameLength, frameStep);
	const windowedSignal = mul(framedSignal, windowFn(frameLength));
	return rfft(windowedSignal, fftLength);
}

const stft = /* @__PURE__ */ op({stft_});

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Extracts crops from the input image tensor and resizes them using bilinear
 * sampling or nearest neighbor sampling (possibly with aspect ratio change)
 * to a common output size specified by cropSize.
 *
 * @param image 4d tensor of shape `[batch,imageHeight,imageWidth, depth]`,
 *     where imageHeight and imageWidth must be positive, specifying the
 *     batch of images from which to take crops
 * @param boxes 2d float32 tensor of shape `[numBoxes, 4]`. Each entry is
 *     `[y1, x1, y2, x2]`, where `(y1, x1)` and `(y2, x2)` are the normalized
 *     coordinates of the box in the `boxInd[i]`th image in the batch
 * @param boxInd 1d int32 tensor of shape `[numBoxes]` with values in range
 *     `[0, batch)` that specifies the image that the `i`-th box refers to.
 * @param cropSize 1d int32 tensor of 2 elements `[cropHeigh, cropWidth]`
 *     specifying the size to which all crops are resized to.
 * @param method Optional string from `'bilinear' | 'nearest'`,
 *     defaults to bilinear, which specifies the sampling method for resizing
 * @param extrapolationValue A threshold for deciding when to remove boxes based
 *     on score. Defaults to 0.
 * @return A 4D tensor of the shape `[numBoxes,cropHeight,cropWidth,depth]`
 *
 * @doc {heading: 'Operations', subheading: 'Images', namespace: 'image'}
 */
function cropAndResize_(image, boxes, boxInd, cropSize, method = 'bilinear', extrapolationValue = 0) {
	const $image = convertToTensor(image, 'image', 'cropAndResize');
	const $boxes = convertToTensor(boxes, 'boxes', 'cropAndResize', 'float32');
	const $boxInd = convertToTensor(boxInd, 'boxInd', 'cropAndResize', 'int32');
	const numBoxes = $boxes.shape[0];
	assert($image.rank === 4, () => 'Error in cropAndResize: image must be rank 4,' +
		`but got rank ${$image.rank}.`);
	assert($boxes.rank === 2 && $boxes.shape[1] === 4, () => `Error in cropAndResize: boxes must be have size [${numBoxes},4] ` +
		`but had shape ${$boxes.shape}.`);
	assert($boxInd.rank === 1 && $boxInd.shape[0] === numBoxes, () => `Error in cropAndResize: boxInd must be have size [${numBoxes}] ` +
		`but had shape ${$boxes.shape}.`);
	assert(cropSize.length === 2, () => `Error in cropAndResize: cropSize must be of length 2, but got ` +
		`length ${cropSize.length}.`);
	assert(cropSize[0] >= 1 && cropSize[1] >= 1, () => `cropSize must be atleast [1,1], but was ${cropSize}`);
	assert(method === 'bilinear' || method === 'nearest', () => `method must be bilinear or nearest, but was ${method}`);
	const inputs = {image: $image, boxes: $boxes, boxInd: $boxInd};
	const attrs = {method, extrapolationValue, cropSize};
	const res = ENGINE.runKernel(CropAndResize, inputs, attrs);
	return res;
}

const cropAndResize = /* @__PURE__ */ op({cropAndResize_});

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */

/**
 * Flips the image left to right. Currently available in the CPU, WebGL, and
 * WASM backends.
 *
 * @param image 4d tensor of shape `[batch, imageHeight, imageWidth, depth]`.
 */
/** @doc {heading: 'Operations', subheading: 'Images', namespace: 'image'} */
function flipLeftRight_(image) {
	const $image = convertToTensor(image, 'image', 'flipLeftRight', 'float32');
	assert($image.rank === 4, () => 'Error in flipLeftRight: image must be rank 4,' +
		`but got rank ${$image.rank}.`);
	const inputs = {image: $image};
	const res = ENGINE.runKernel(FlipLeftRight, inputs, {});
	return res;
}

const flipLeftRight = /* @__PURE__ */ op({flipLeftRight_});

/**
 * @license
 * Copyright 2021 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Converts images from grayscale to RGB format.
 *
 * @param image A grayscale tensor to convert. The `image`'s last dimension must
 *     be size 1 with at least a two-dimensional shape.
 *
 * @doc {heading: 'Operations', subheading: 'Images', namespace: 'image'}
 */
function grayscaleToRGB_(image) {
	const $image = convertToTensor(image, 'image', 'grayscaleToRGB');
	const lastDimsIdx = $image.rank - 1;
	const lastDims = $image.shape[lastDimsIdx];
	assert($image.rank >= 2, () => 'Error in grayscaleToRGB: images must be at least rank 2, ' +
		`but got rank ${$image.rank}.`);
	assert(lastDims === 1, () => 'Error in grayscaleToRGB: last dimension of a grayscale image ' +
		`should be size 1, but got size ${lastDims}.`);
	const reps = new Array($image.rank);
	reps.fill(1, 0, lastDimsIdx);
	reps[lastDimsIdx] = 3;
	return tile($image, reps);
}

const grayscaleToRGB = /* @__PURE__ */ op({grayscaleToRGB_});

/**
 * @license
 * Copyright 2023 Google LLC.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Converts images from RGB format to grayscale.
 *
 * @param image A RGB tensor to convert. The `image`'s last dimension must
 *     be size 3 with at least a two-dimensional shape.
 *
 * @doc {heading: 'Operations', subheading: 'Images', namespace: 'image'}
 */
function rgbToGrayscale_(image) {
	const $image = convertToTensor(image, 'image', 'RGBToGrayscale');
	const lastDimsIdx = $image.rank - 1;
	const lastDims = $image.shape[lastDimsIdx];
	assert($image.rank >= 2, () => 'Error in RGBToGrayscale: images must be at least rank 2, ' +
		`but got rank ${$image.rank}.`);
	assert(lastDims === 3, () => 'Error in RGBToGrayscale: last dimension of an RGB image ' +
		`should be size 3, but got size ${lastDims}.`);
	// Remember original dtype so we can convert back if needed
	const origDtype = $image.dtype;
	const fltImage = cast$1($image, 'float32');
	const rgbWeights = tensor1d([0.2989, 0.5870, 0.1140]);
	let grayFloat;
	switch ($image.rank) {
		case 2:
			grayFloat = einsum('ij,j->i', fltImage, rgbWeights);
			break;
		case 3:
			grayFloat = einsum('ijk,k->ij', fltImage, rgbWeights);
			break;
		case 4:
			grayFloat = einsum('ijkl,l->ijk', fltImage, rgbWeights);
			break;
		case 5:
			grayFloat = einsum('ijklm,m->ijkl', fltImage, rgbWeights);
			break;
		case 6:
			grayFloat = einsum('ijklmn,n->ijklm', fltImage, rgbWeights);
			break;
		default:
			throw new Error('Not a valid tensor rank.');
	}
	grayFloat = expandDims(grayFloat, -1);
	return cast$1(grayFloat, origDtype);
}

const rgbToGrayscale = /* @__PURE__ */ op({rgbToGrayscale_});

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Rotates the input image tensor counter-clockwise with an optional offset
 * center of rotation. Currently available in the CPU, WebGL, and WASM backends.
 *
 * @param image 4d tensor of shape `[batch, imageHeight, imageWidth, depth]`.
 * @param radians The amount of rotation.
 * @param fillValue The value to fill in the empty space leftover
 *     after rotation. Can be either a single grayscale value (0-255), or an
 *     array of three numbers `[red, green, blue]` specifying the red, green,
 *     and blue channels. Defaults to `0` (black).
 * @param center The center of rotation. Can be either a single value (0-1), or
 *     an array of two numbers `[centerX, centerY]`. Defaults to `0.5` (rotates
 *     the image around its center).
 *
 * @doc {heading: 'Operations', subheading: 'Images', namespace: 'image'}
 */
function rotateWithOffset_(image, radians, fillValue = 0, center = 0.5) {
	const $image = convertToTensor(image, 'image', 'rotateWithOffset', 'float32');
	assert($image.rank === 4, () => 'Error in rotateWithOffset: image must be rank 4,' +
		`but got rank ${$image.rank}.`);
	const inputs = {image: $image};
	const attrs = {radians, fillValue, center};
	const res = ENGINE.runKernel(RotateWithOffset, inputs, attrs);
	return res;
}

const rotateWithOffset = /* @__PURE__ */ op({rotateWithOffset_});

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
function nonMaxSuppSanityCheck(boxes, scores, maxOutputSize, iouThreshold, scoreThreshold, softNmsSigma) {
	if (iouThreshold == null) {
		iouThreshold = 0.5;
	}
	if (scoreThreshold == null) {
		scoreThreshold = Number.NEGATIVE_INFINITY;
	}
	if (softNmsSigma == null) {
		softNmsSigma = 0.0;
	}
	const numBoxes = boxes.shape[0];
	maxOutputSize = Math.min(maxOutputSize, numBoxes);
	assert(0 <= iouThreshold && iouThreshold <= 1, () => `iouThreshold must be in [0, 1], but was '${iouThreshold}'`);
	assert(boxes.rank === 2, () => `boxes must be a 2D tensor, but was of rank '${boxes.rank}'`);
	assert(boxes.shape[1] === 4, () => `boxes must have 4 columns, but 2nd dimension was ${boxes.shape[1]}`);
	assert(scores.rank === 1, () => 'scores must be a 1D tensor');
	assert(scores.shape[0] === numBoxes, () => `scores has incompatible shape with boxes. Expected ${numBoxes}, ` +
		`but was ${scores.shape[0]}`);
	assert(0 <= softNmsSigma && softNmsSigma <= 1, () => `softNmsSigma must be in [0, 1], but was '${softNmsSigma}'`);
	return {maxOutputSize, iouThreshold, scoreThreshold, softNmsSigma};
}

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Performs non maximum suppression of bounding boxes based on
 * iou (intersection over union).
 *
 * @param boxes a 2d tensor of shape `[numBoxes, 4]`. Each entry is
 *     `[y1, x1, y2, x2]`, where `(y1, x1)` and `(y2, x2)` are the corners of
 *     the bounding box.
 * @param scores a 1d tensor providing the box scores of shape `[numBoxes]`.
 * @param maxOutputSize The maximum number of boxes to be selected.
 * @param iouThreshold A float representing the threshold for deciding whether
 *     boxes overlap too much with respect to IOU. Must be between [0, 1].
 *     Defaults to 0.5 (50% box overlap).
 * @param scoreThreshold A threshold for deciding when to remove boxes based
 *     on score. Defaults to -inf, which means any score is accepted.
 * @return A 1D tensor with the selected box indices.
 *
 * @doc {heading: 'Operations', subheading: 'Images', namespace: 'image'}
 */
function nonMaxSuppression_(boxes, scores, maxOutputSize, iouThreshold = 0.5, scoreThreshold = Number.NEGATIVE_INFINITY) {
	const $boxes = convertToTensor(boxes, 'boxes', 'nonMaxSuppression', 'float32');
	const $scores = convertToTensor(scores, 'scores', 'nonMaxSuppression', 'float32');
	const inputs = nonMaxSuppSanityCheck($boxes, $scores, maxOutputSize, iouThreshold, scoreThreshold);
	maxOutputSize = inputs.maxOutputSize;
	iouThreshold = inputs.iouThreshold;
	scoreThreshold = inputs.scoreThreshold;
	const attrs = {maxOutputSize, iouThreshold, scoreThreshold};
	return ENGINE.runKernel(NonMaxSuppressionV3, {boxes: $boxes, scores: $scores}, attrs);
}

const nonMaxSuppression = /* @__PURE__ */ op({nonMaxSuppression_});

/**
 * @license
 * Copyright 2019 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Inserts a value into a sorted array. This method allows duplicate, meaning it
 * allows inserting duplicate value, in which case, the element will be inserted
 * at the lowest index of the value.
 * @param arr The array to modify.
 * @param element The element to insert.
 * @param comparator Optional. If no comparator is specified, elements are
 * compared using array_util.defaultComparator, which is suitable for Strings
 * and Numbers in ascending arrays. If the array contains multiple instances of
 * the target value, the left-most instance will be returned. To provide a
 * comparator, it should take 2 arguments to compare and return a negative,
 * zero, or a positive number.
 */
function binaryInsert(arr, element, comparator) {
	const index = binarySearch(arr, element, comparator);
	const insertionPoint = index < 0 ? -(index + 1) : index;
	arr.splice(insertionPoint, 0, element);
}

/**
 * Searches the array for the target using binary search, returns the index
 * of the found element, or position to insert if element not found. If no
 * comparator is specified, elements are compared using array_
 * util.defaultComparator, which is suitable for Strings and Numbers in
 * ascending arrays. If the array contains multiple instances of the target
 * value, the left-most instance will be returned.
 * @param arr The array to be searched in.
 * @param target The target to be searched for.
 * @param comparator Should take 2 arguments to compare and return a negative,
 *    zero, or a positive number.
 * @return Lowest index of the target value if found, otherwise the insertion
 *    point where the target should be inserted, in the form of
 *    (-insertionPoint - 1).
 */
function binarySearch(arr, target, comparator) {
	return binarySearch_(arr, target, comparator || defaultComparator);
}

/**
 * Compares its two arguments for order.
 * @param a The first element to be compared.
 * @param b The second element to be compared.
 * @return A negative number, zero, or a positive number as the first
 *     argument is less than, equal to, or greater than the second.
 */
function defaultComparator(a, b) {
	return a > b ? 1 : a < b ? -1 : 0;
}

function binarySearch_(arr, target, comparator) {
	let left = 0;
	let right = arr.length;
	let middle = 0;
	let found = false;
	while (left < right) {
		middle = left + ((right - left) >>> 1);
		const compareResult = comparator(target, arr[middle]);
		if (compareResult > 0) {
			left = middle + 1;
		} else {
			right = middle;
			// If compareResult is 0, the value is found. We record it is found,
			// and then keep looking because there may be duplicate.
			found = !compareResult;
		}
	}
	return found ? left : -left - 1;
}

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
function nonMaxSuppressionV3Impl(boxes, scores, maxOutputSize, iouThreshold, scoreThreshold) {
	return nonMaxSuppressionImpl_(boxes, scores, maxOutputSize, iouThreshold, scoreThreshold, 0 /* softNmsSigma */);
}

function nonMaxSuppressionV4Impl(boxes, scores, maxOutputSize, iouThreshold, scoreThreshold, padToMaxOutputSize) {
	return nonMaxSuppressionImpl_(boxes, scores, maxOutputSize, iouThreshold, scoreThreshold, 0 /* softNmsSigma */, false /* returnScoresTensor */, padToMaxOutputSize /* padToMaxOutputSize */, true
		/* returnValidOutputs */);
}

function nonMaxSuppressionV5Impl(boxes, scores, maxOutputSize, iouThreshold, scoreThreshold, softNmsSigma) {
	return nonMaxSuppressionImpl_(boxes, scores, maxOutputSize, iouThreshold, scoreThreshold, softNmsSigma, true /* returnScoresTensor */);
}

function nonMaxSuppressionImpl_(boxes, scores, maxOutputSize, iouThreshold, scoreThreshold, softNmsSigma, returnScoresTensor = false, padToMaxOutputSize = false, returnValidOutputs = false) {
	// The list is sorted in ascending order, so that we can always pop the
	// candidate with the largest score in O(1) time.
	const candidates = [];
	for (let i = 0; i < scores.length; i++) {
		if (scores[i] > scoreThreshold) {
			candidates.push({score: scores[i], boxIndex: i, suppressBeginIndex: 0});
		}
	}
	candidates.sort(ascendingComparator);
	// If softNmsSigma is 0, the outcome of this algorithm is exactly same as
	// before.
	const scale = softNmsSigma > 0 ? (-0.5 / softNmsSigma) : 0.0;
	const selectedIndices = [];
	const selectedScores = [];
	while (selectedIndices.length < maxOutputSize && candidates.length > 0) {
		const candidate = candidates.pop();
		const {score: originalScore, boxIndex, suppressBeginIndex} = candidate;
		if (originalScore < scoreThreshold) {
			break;
		}
		// Overlapping boxes are likely to have similar scores, therefore we
		// iterate through the previously selected boxes backwards in order to
		// see if candidate's score should be suppressed. We use
		// suppressBeginIndex to track and ensure a candidate can be suppressed
		// by a selected box no more than once. Also, if the overlap exceeds
		// iouThreshold, we simply ignore the candidate.
		let ignoreCandidate = false;
		for (let j = selectedIndices.length - 1; j >= suppressBeginIndex; --j) {
			const iou = intersectionOverUnion(boxes, boxIndex, selectedIndices[j]);
			if (iou >= iouThreshold) {
				ignoreCandidate = true;
				break;
			}
			candidate.score =
				candidate.score * suppressWeight(iouThreshold, scale, iou);
			if (candidate.score <= scoreThreshold) {
				break;
			}
		}
		// At this point, if `candidate.score` has not dropped below
		// `scoreThreshold`, then we know that we went through all of the
		// previous selections and can safely update `suppressBeginIndex` to the
		// end of the selected array. Then we can re-insert the candidate with
		// the updated score and suppressBeginIndex back in the candidate list.
		// If on the other hand, `candidate.score` has dropped below the score
		// threshold, we will not add it back to the candidates list.
		candidate.suppressBeginIndex = selectedIndices.length;
		if (!ignoreCandidate) {
			// Candidate has passed all the tests, and is not suppressed, so
			// select the candidate.
			if (candidate.score === originalScore) {
				selectedIndices.push(boxIndex);
				selectedScores.push(candidate.score);
			} else if (candidate.score > scoreThreshold) {
				// Candidate's score is suppressed but is still high enough to be
				// considered, so add back to the candidates list.
				binaryInsert(candidates, candidate, ascendingComparator);
			}
		}
	}
	// NonMaxSuppressionV4 feature: padding output to maxOutputSize.
	const validOutputs = selectedIndices.length;
	const elemsToPad = maxOutputSize - validOutputs;
	if (padToMaxOutputSize && elemsToPad > 0) {
		selectedIndices.push(...new Array(elemsToPad).fill(0));
		selectedScores.push(...new Array(elemsToPad).fill(0.0));
	}
	const result = {selectedIndices};
	if (returnScoresTensor) {
		result['selectedScores'] = selectedScores;
	}
	if (returnValidOutputs) {
		result['validOutputs'] = validOutputs;
	}
	return result;
}

function intersectionOverUnion(boxes, i, j) {
	const iCoord = boxes.subarray(i * 4, i * 4 + 4);
	const jCoord = boxes.subarray(j * 4, j * 4 + 4);
	const yminI = Math.min(iCoord[0], iCoord[2]);
	const xminI = Math.min(iCoord[1], iCoord[3]);
	const ymaxI = Math.max(iCoord[0], iCoord[2]);
	const xmaxI = Math.max(iCoord[1], iCoord[3]);
	const yminJ = Math.min(jCoord[0], jCoord[2]);
	const xminJ = Math.min(jCoord[1], jCoord[3]);
	const ymaxJ = Math.max(jCoord[0], jCoord[2]);
	const xmaxJ = Math.max(jCoord[1], jCoord[3]);
	const areaI = (ymaxI - yminI) * (xmaxI - xminI);
	const areaJ = (ymaxJ - yminJ) * (xmaxJ - xminJ);
	if (areaI <= 0 || areaJ <= 0) {
		return 0.0;
	}
	const intersectionYmin = Math.max(yminI, yminJ);
	const intersectionXmin = Math.max(xminI, xminJ);
	const intersectionYmax = Math.min(ymaxI, ymaxJ);
	const intersectionXmax = Math.min(xmaxI, xmaxJ);
	const intersectionArea = Math.max(intersectionYmax - intersectionYmin, 0.0) *
		Math.max(intersectionXmax - intersectionXmin, 0.0);
	return intersectionArea / (areaI + areaJ - intersectionArea);
}

// A Gaussian penalty function, this method always returns values in [0, 1].
// The weight is a function of similarity, the more overlap two boxes are, the
// smaller the weight is,meaning highly overlapping boxes will be significantly
// penalized. On the other hand, a non-overlapping box will not be penalized.
function suppressWeight(iouThreshold, scale, iou) {
	const weight = Math.exp(scale * iou * iou);
	return iou <= iouThreshold ? weight : 0.0;
}

function ascendingComparator(c1, c2) {
	// For objects with same scores, we make the object with the larger index go
	// first. In an array that pops from the end, this means that the object with
	// the smaller index will be popped first. This ensures the same output as
	// the TensorFlow python version.
	return (c1.score - c2.score) ||
		((c1.score === c2.score) && (c2.boxIndex - c1.boxIndex));
}

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Performs non maximum suppression of bounding boxes based on
 * iou (intersection over union).
 *
 * This is the async version of `nonMaxSuppression`
 *
 * @param boxes a 2d tensor of shape `[numBoxes, 4]`. Each entry is
 *     `[y1, x1, y2, x2]`, where `(y1, x1)` and `(y2, x2)` are the corners of
 *     the bounding box.
 * @param scores a 1d tensor providing the box scores of shape `[numBoxes]`.
 * @param maxOutputSize The maximum number of boxes to be selected.
 * @param iouThreshold A float representing the threshold for deciding whether
 *     boxes overlap too much with respect to IOU. Must be between [0, 1].
 *     Defaults to 0.5 (50% box overlap).
 * @param scoreThreshold A threshold for deciding when to remove boxes based
 *     on score. Defaults to -inf, which means any score is accepted.
 * @return A 1D tensor with the selected box indices.
 *
 * @doc {heading: 'Operations', subheading: 'Images', namespace: 'image'}
 */
async function nonMaxSuppressionAsync_(boxes, scores, maxOutputSize, iouThreshold = 0.5, scoreThreshold = Number.NEGATIVE_INFINITY) {
	const $boxes = convertToTensor(boxes, 'boxes', 'nonMaxSuppressionAsync');
	const $scores = convertToTensor(scores, 'scores', 'nonMaxSuppressionAsync');
	const inputs = nonMaxSuppSanityCheck($boxes, $scores, maxOutputSize, iouThreshold, scoreThreshold);
	maxOutputSize = inputs.maxOutputSize;
	iouThreshold = inputs.iouThreshold;
	scoreThreshold = inputs.scoreThreshold;
	const boxesAndScores = await Promise.all([$boxes.data(), $scores.data()]);
	const boxesVals = boxesAndScores[0];
	const scoresVals = boxesAndScores[1];
	// We call a cpu based impl directly with the typedarray data  here rather
	// than a kernel because all kernels are synchronous (and thus cannot await
	// .data()).
	const {selectedIndices} = nonMaxSuppressionV3Impl(boxesVals, scoresVals, maxOutputSize, iouThreshold, scoreThreshold);
	if ($boxes !== boxes) {
		$boxes.dispose();
	}
	if ($scores !== scores) {
		$scores.dispose();
	}
	return tensor1d(selectedIndices, 'int32');
}

const nonMaxSuppressionAsync = nonMaxSuppressionAsync_;

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Performs non maximum suppression of bounding boxes based on
 * iou (intersection over union).
 *
 * This op also supports a Soft-NMS mode (cf.
 * Bodla et al, https://arxiv.org/abs/1704.04503) where boxes reduce the score
 * of other overlapping boxes, therefore favoring different regions of the image
 * with high scores. To enable this Soft-NMS mode, set the `softNmsSigma`
 * parameter to be larger than 0.
 *
 * @param boxes a 2d tensor of shape `[numBoxes, 4]`. Each entry is
 *     `[y1, x1, y2, x2]`, where `(y1, x1)` and `(y2, x2)` are the corners of
 *     the bounding box.
 * @param scores a 1d tensor providing the box scores of shape `[numBoxes]`.
 * @param maxOutputSize The maximum number of boxes to be selected.
 * @param iouThreshold A float representing the threshold for deciding whether
 *     boxes overlap too much with respect to IOU. Must be between [0, 1].
 *     Defaults to 0.5 (50% box overlap).
 * @param scoreThreshold A threshold for deciding when to remove boxes based
 *     on score. Defaults to -inf, which means any score is accepted.
 * @param softNmsSigma A float representing the sigma parameter for Soft NMS.
 *     When sigma is 0, it falls back to nonMaxSuppression.
 * @return A map with the following properties:
 *     - selectedIndices: A 1D tensor with the selected box indices.
 *     - selectedScores: A 1D tensor with the corresponding scores for each
 *       selected box.
 *
 * @doc {heading: 'Operations', subheading: 'Images', namespace: 'image'}
 */
function nonMaxSuppressionWithScore_(boxes, scores, maxOutputSize, iouThreshold = 0.5, scoreThreshold = Number.NEGATIVE_INFINITY, softNmsSigma = 0.0) {
	const $boxes = convertToTensor(boxes, 'boxes', 'nonMaxSuppression');
	const $scores = convertToTensor(scores, 'scores', 'nonMaxSuppression');
	const params = nonMaxSuppSanityCheck($boxes, $scores, maxOutputSize, iouThreshold, scoreThreshold, softNmsSigma);
	maxOutputSize = params.maxOutputSize;
	iouThreshold = params.iouThreshold;
	scoreThreshold = params.scoreThreshold;
	softNmsSigma = params.softNmsSigma;
	const inputs = {boxes: $boxes, scores: $scores};
	const attrs = {maxOutputSize, iouThreshold, scoreThreshold, softNmsSigma};
	// tslint:disable-next-line: no-unnecessary-type-assertion
	const result = ENGINE.runKernel(NonMaxSuppressionV5, inputs, attrs);
	return {selectedIndices: result[0], selectedScores: result[1]};
}

const nonMaxSuppressionWithScore = /* @__PURE__ */ op({nonMaxSuppressionWithScore_});

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Asynchronously performs non maximum suppression of bounding boxes based on
 * iou (intersection over union).
 *
 * This op also supports a Soft-NMS mode (cf.
 * Bodla et al, https://arxiv.org/abs/1704.04503) where boxes reduce the score
 * of other overlapping boxes, therefore favoring different regions of the image
 * with high scores. To enable this Soft-NMS mode, set the `softNmsSigma`
 * parameter to be larger than 0.
 *
 * @param boxes a 2d tensor of shape `[numBoxes, 4]`. Each entry is
 *     `[y1, x1, y2, x2]`, where `(y1, x1)` and `(y2, x2)` are the corners of
 *     the bounding box.
 * @param scores a 1d tensor providing the box scores of shape `[numBoxes]`.
 * @param maxOutputSize The maximum number of boxes to be selected.
 * @param iouThreshold A float representing the threshold for deciding whether
 *     boxes overlap too much with respect to IOU. Must be between [0, 1].
 *     Defaults to 0.5 (50% box overlap).
 * @param scoreThreshold A threshold for deciding when to remove boxes based
 *     on score. Defaults to -inf, which means any score is accepted.
 * @param softNmsSigma A float representing the sigma parameter for Soft NMS.
 *     When sigma is 0, it falls back to nonMaxSuppression.
 * @return A map with the following properties:
 *     - selectedIndices: A 1D tensor with the selected box indices.
 *     - selectedScores: A 1D tensor with the corresponding scores for each
 *       selected box.
 *
 * @doc {heading: 'Operations', subheading: 'Images', namespace: 'image'}
 */
async function nonMaxSuppressionWithScoreAsync_(boxes, scores, maxOutputSize, iouThreshold = 0.5, scoreThreshold = Number.NEGATIVE_INFINITY, softNmsSigma = 0.0) {
	const $boxes = convertToTensor(boxes, 'boxes', 'nonMaxSuppressionAsync');
	const $scores = convertToTensor(scores, 'scores', 'nonMaxSuppressionAsync');
	const params = nonMaxSuppSanityCheck($boxes, $scores, maxOutputSize, iouThreshold, scoreThreshold, softNmsSigma);
	maxOutputSize = params.maxOutputSize;
	iouThreshold = params.iouThreshold;
	scoreThreshold = params.scoreThreshold;
	softNmsSigma = params.softNmsSigma;
	const boxesAndScores = await Promise.all([$boxes.data(), $scores.data()]);
	const boxesVals = boxesAndScores[0];
	const scoresVals = boxesAndScores[1];
	// We call a cpu based impl directly with the typedarray data  here rather
	// than a kernel because all kernels are synchronous (and thus cannot await
	// .data()).
	const {selectedIndices, selectedScores} = nonMaxSuppressionV5Impl(boxesVals, scoresVals, maxOutputSize, iouThreshold, scoreThreshold, softNmsSigma);
	if ($boxes !== boxes) {
		$boxes.dispose();
	}
	if ($scores !== scores) {
		$scores.dispose();
	}
	return {
		selectedIndices: tensor1d(selectedIndices, 'int32'),
		selectedScores: tensor1d(selectedScores)
	};
}

const nonMaxSuppressionWithScoreAsync = nonMaxSuppressionWithScoreAsync_;

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Asynchronously performs non maximum suppression of bounding boxes based on
 * iou (intersection over union), with an option to pad results.
 *
 * @param boxes a 2d tensor of shape `[numBoxes, 4]`. Each entry is
 *     `[y1, x1, y2, x2]`, where `(y1, x1)` and `(y2, x2)` are the corners of
 *     the bounding box.
 * @param scores a 1d tensor providing the box scores of shape `[numBoxes]`.
 * @param maxOutputSize The maximum number of boxes to be selected.
 * @param iouThreshold A float representing the threshold for deciding whether
 *     boxes overlap too much with respect to IOU. Must be between [0, 1].
 *     Defaults to 0.5 (50% box overlap).
 * @param scoreThreshold A threshold for deciding when to remove boxes based
 *     on score. Defaults to -inf, which means any score is accepted.
 * @param padToMaxOutputSize Defaults to false. If true, size of output
 *     `selectedIndices` is padded to maxOutputSize.
 * @return A map with the following properties:
 *     - selectedIndices: A 1D tensor with the selected box indices.
 *     - validOutputs: A scalar denoting how many elements in `selectedIndices`
 *       are valid. Valid elements occur first, then padding.
 *
 * @doc {heading: 'Operations', subheading: 'Images', namespace: 'image'}
 */
function nonMaxSuppressionPadded_(boxes, scores, maxOutputSize, iouThreshold = 0.5, scoreThreshold = Number.NEGATIVE_INFINITY, padToMaxOutputSize = false) {
	const $boxes = convertToTensor(boxes, 'boxes', 'nonMaxSuppression');
	const $scores = convertToTensor(scores, 'scores', 'nonMaxSuppression');
	const params = nonMaxSuppSanityCheck($boxes, $scores, maxOutputSize, iouThreshold, scoreThreshold, null /* softNmsSigma */);
	const $maxOutputSize = params.maxOutputSize;
	const $iouThreshold = params.iouThreshold;
	const $scoreThreshold = params.scoreThreshold;
	const inputs = {boxes: $boxes, scores: $scores};
	const attrs = {
		maxOutputSize: $maxOutputSize,
		iouThreshold: $iouThreshold,
		scoreThreshold: $scoreThreshold,
		padToMaxOutputSize
	};
	// tslint:disable-next-line: no-unnecessary-type-assertion
	const result = ENGINE.runKernel(NonMaxSuppressionV4, inputs, attrs);
	return {selectedIndices: result[0], validOutputs: result[1]};
}

const nonMaxSuppressionPadded = /* @__PURE__ */ op({nonMaxSuppressionPadded_});

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Asynchronously performs non maximum suppression of bounding boxes based on
 * iou (intersection over union), with an option to pad results.
 *
 * @param boxes a 2d tensor of shape `[numBoxes, 4]`. Each entry is
 *     `[y1, x1, y2, x2]`, where `(y1, x1)` and `(y2, x2)` are the corners of
 *     the bounding box.
 * @param scores a 1d tensor providing the box scores of shape `[numBoxes]`.
 * @param maxOutputSize The maximum number of boxes to be selected.
 * @param iouThreshold A float representing the threshold for deciding whether
 *     boxes overlap too much with respect to IOU. Must be between [0, 1].
 *     Defaults to 0.5 (50% box overlap).
 * @param scoreThreshold A threshold for deciding when to remove boxes based
 *     on score. Defaults to -inf, which means any score is accepted.
 * @param padToMaxOutputSize Defaults to false. If true, size of output
 *     `selectedIndices` is padded to maxOutputSize.
 * @return A map with the following properties:
 *     - selectedIndices: A 1D tensor with the selected box indices.
 *     - validOutputs: A scalar denoting how many elements in `selectedIndices`
 *       are valid. Valid elements occur first, then padding.
 *
 * @doc {heading: 'Operations', subheading: 'Images', namespace: 'image'}
 */
async function nonMaxSuppressionPaddedAsync_(boxes, scores, maxOutputSize, iouThreshold = 0.5, scoreThreshold = Number.NEGATIVE_INFINITY, padToMaxOutputSize = false) {
	const $boxes = convertToTensor(boxes, 'boxes', 'nonMaxSuppressionAsync');
	const $scores = convertToTensor(scores, 'scores', 'nonMaxSuppressionAsync');
	const params = nonMaxSuppSanityCheck($boxes, $scores, maxOutputSize, iouThreshold, scoreThreshold, null /* softNmsSigma */);
	const $maxOutputSize = params.maxOutputSize;
	const $iouThreshold = params.iouThreshold;
	const $scoreThreshold = params.scoreThreshold;
	const [boxesVals, scoresVals] = await Promise.all([$boxes.data(), $scores.data()]);
	// We call a cpu based impl directly with the typedarray data here rather
	// than a kernel because all kernels are synchronous (and thus cannot await
	// .data()).
	const {selectedIndices, validOutputs} = nonMaxSuppressionV4Impl(boxesVals, scoresVals, $maxOutputSize, $iouThreshold, $scoreThreshold, padToMaxOutputSize);
	if ($boxes !== boxes) {
		$boxes.dispose();
	}
	if ($scores !== scores) {
		$scores.dispose();
	}
	return {
		selectedIndices: tensor1d(selectedIndices, 'int32'),
		validOutputs: scalar(validOutputs, 'int32')
	};
}

const nonMaxSuppressionPaddedAsync = nonMaxSuppressionPaddedAsync_;

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Bilinear resize a single 3D image or a batch of 3D images to a new shape.
 *
 * @param images The images, of rank 4 or rank 3, of shape
 *     `[batch, height, width, inChannels]`. If rank 3, batch of 1 is assumed.
 * @param size The new shape `[newHeight, newWidth]` to resize the
 *     images to. Each channel is resized individually.
 * @param alignCorners Defaults to `false`. If true, rescale
 *     input by `(new_height - 1) / (height - 1)`, which exactly aligns the 4
 *     corners of images and resized images. If false, rescale by
 *     `new_height / height`. Treat similarly the width dimension.
 * @param halfPixelCenters Defaults to `false`. Whether to assume pixel centers
 *     are at 0.5, which would make the floating point coordinates of the top
 *     left pixel 0.5, 0.5.
 *
 * @doc {heading: 'Operations', subheading: 'Images', namespace: 'image'}
 */
function resizeBilinear_(images, size, alignCorners = false, halfPixelCenters = false) {
	const $images = convertToTensor(images, 'images', 'resizeBilinear');
	assert($images.rank === 3 || $images.rank === 4, () => `Error in resizeBilinear: x must be rank 3 or 4, but got ` +
		`rank ${$images.rank}.`);
	assert(size.length === 2, () => `Error in resizeBilinear: new shape must 2D, but got shape ` +
		`${size}.`);
	assert(halfPixelCenters === false || alignCorners === false, () => `Error in resizeBilinear: If halfPixelCenters is true, ` +
		`alignCorners must be false.`);
	let batchImages = $images;
	let reshapedTo4D = false;
	if ($images.rank === 3) {
		reshapedTo4D = true;
		batchImages = reshape$1($images, [1, $images.shape[0], $images.shape[1], $images.shape[2]]);
	}
	const inputs = {images: batchImages};
	const attrs = {alignCorners, halfPixelCenters, size};
	// tslint:disable-next-line: no-unnecessary-type-assertion
	const res = ENGINE.runKernel(ResizeBilinear, inputs, attrs);
	if (reshapedTo4D) {
		return reshape$1(res, [res.shape[1], res.shape[2], res.shape[3]]);
	}
	return res;
}

const resizeBilinear = /* @__PURE__ */ op({resizeBilinear_});

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * NearestNeighbor resize a batch of 3D images to a new shape.
 *
 * @param images The images, of rank 4 or rank 3, of shape
 *     `[batch, height, width, inChannels]`. If rank 3, batch of 1 is assumed.
 * @param size The new shape `[newHeight, newWidth]` to resize the
 *     images to. Each channel is resized individually.
 * @param alignCorners Defaults to False. If true, rescale
 *     input by `(new_height - 1) / (height - 1)`, which exactly aligns the 4
 *     corners of images and resized images. If false, rescale by
 *     `new_height / height`. Treat similarly the width dimension.
 * @param halfPixelCenters Defaults to `false`. Whether to assume pixels are of
 *      half the actual dimensions, and yield more accurate resizes. This flag
 *      would also make the floating point coordinates of the top left pixel
 *      0.5, 0.5.
 *
 * @doc {heading: 'Operations', subheading: 'Images', namespace: 'image'}
 */
function resizeNearestNeighbor_(images, size, alignCorners = false, halfPixelCenters = false) {
	const $images = convertToTensor(images, 'images', 'resizeNearestNeighbor');
	assert($images.rank === 3 || $images.rank === 4, () => `Error in resizeNearestNeighbor: x must be rank 3 or 4, but got ` +
		`rank ${$images.rank}.`);
	assert(size.length === 2, () => `Error in resizeNearestNeighbor: new shape must 2D, but got shape ` +
		`${size}.`);
	assert($images.dtype === 'float32' || $images.dtype === 'int32', () => '`images` must have `int32` or `float32` as dtype');
	assert(halfPixelCenters === false || alignCorners === false, () => `Error in resizeNearestNeighbor: If halfPixelCenters is true, ` +
		`alignCorners must be false.`);
	let batchImages = $images;
	let reshapedTo4D = false;
	if ($images.rank === 3) {
		reshapedTo4D = true;
		batchImages = reshape$1($images, [1, $images.shape[0], $images.shape[1], $images.shape[2]]);
	}
	const inputs = {images: batchImages};
	const attrs = {alignCorners, halfPixelCenters, size};
	// tslint:disable-next-line: no-unnecessary-type-assertion
	const res = ENGINE.runKernel(ResizeNearestNeighbor, inputs, attrs);
	if (reshapedTo4D) {
		return reshape$1(res, [res.shape[1], res.shape[2], res.shape[3]]);
	}
	return res;
}

const resizeNearestNeighbor = /* @__PURE__ */ op({resizeNearestNeighbor_});

/**
 * @license
 * Copyright 2021 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * https://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Performs image binarization with corresponding threshold
 * (depends on the method)value, which creates a binary image from a grayscale.
 * @param image 3d tensor of shape [imageHeight,imageWidth, depth],
 * where imageHeight and imageWidth must be positive.The image color
 * range should be [0, 255].
 * @param method Optional string from `'binary' | 'otsu'`
 * which specifies the method for thresholding. Defaults to 'binary'.
 * @param inverted Optional boolean whichspecifies
 * if colours should be inverted. Defaults to false.
 * @param threshValue Optional number which defines threshold value from 0 to 1.
 * Defaults to 0.5.
 * @return A 3d tensor of shape [imageHeight,imageWidth, depth], which
 * contains binarized image.
 */
function threshold_(image, method = 'binary', inverted = false, threshValue = 0.5) {
	const $image = convertToTensor(image, 'image', 'threshold');
	/* 0.2989, 0.5870, 0.1140 are represent luma coefficients in CCIR601.
    Reference for converting between RGB and grayscale: https://en.wikipedia.org/wiki/Luma_%28video%29  */
	const RED_INTENCITY_COEF = 0.2989;
	const GREEN_INTENCITY_COEF = 0.5870;
	const BLUE_INTENCITY_COEF = 0.1140;
	const totalPixelsInImage = $image.shape[0] * $image.shape[1];
	let $threshold = mul(tensor1d([threshValue]), 255);
	let r, g, b, grayscale;
	assert($image.rank === 3, () => 'Error in threshold: image must be rank 3,' +
		`but got rank ${$image.rank}.`);
	assert($image.shape[2] === 3 || $image.shape[2] === 1, () => 'Error in threshold: ' +
		'image color channel must be equal to 3 or 1' +
		`but got ${$image.shape[2]}.`);
	assert($image.dtype === 'int32' || $image.dtype === 'float32', () => 'Error in dtype: image dtype must be int32 or float32,' +
		`but got dtype ${$image.dtype}.`);
	assert(method === 'otsu' || method === 'binary', () => `Method must be binary or otsu, but was ${method}`);
	if ($image.shape[2] === 3) {
		[r, g, b] = split$1($image, [1, 1, 1], -1);
		const $r = mul(r, RED_INTENCITY_COEF);
		const $g = mul(g, GREEN_INTENCITY_COEF);
		const $b = mul(b, BLUE_INTENCITY_COEF);
		grayscale = add$1(add$1($r, $g), $b);
	} else {
		grayscale = image;
	}
	if (method === 'otsu') {
		const $histogram = bincount(cast$1(round(grayscale), 'int32'), tensor([]), 256);
		$threshold = otsu($histogram, totalPixelsInImage);
	}
	const invCondition = inverted ?
		lessEqual$1(grayscale, $threshold) : greater$1(grayscale, $threshold);
	const result = cast$1(mul(invCondition, 255), 'int32');
	return result;
}

function otsu(histogram, total) {
	let bestThresh = tensor1d([-1]);
	let bestInBetVar = tensor1d([0]);
	let cInBetVar = tensor1d([0]);
	let classFirst, classSecond, meanFirst, meanSec, weightForeground, weightBack;
	for (let index = 0; index < histogram.size - 1; index++) {
		classFirst = slice$1(histogram, 0, index + 1);
		classSecond = slice$1(histogram, index + 1);
		weightForeground = div$1(sum$1(classFirst), total);
		weightBack = div$1(sum$1(classSecond), total);
		const meanFirstDivA = sum$1(mul(classFirst, range(0, classFirst.size)));
		meanFirst = div$1(meanFirstDivA, sum$1(classFirst));
		const meanSecFill = fill$1(classSecond.shape, classFirst.size);
		const meanSecAdd = add$1(range(0, classSecond.size), meanSecFill);
		const meanSecMul = mul(classSecond, (meanSecAdd));
		meanSec = div$1(sum$1(meanSecMul), sum$1(classSecond));
		const cInBetVarSubA = sub$1(meanFirst, meanSec);
		const cInBetVarSubB = sub$1(meanFirst, meanSec);
		const cInBetVarMul = mul(weightForeground, weightBack);
		cInBetVar = mul(mul(cInBetVarMul, cInBetVarSubA), cInBetVarSubB);
		const condition = greater$1(cInBetVar, bestInBetVar);
		bestInBetVar = where(condition, cInBetVar, bestInBetVar);
		bestThresh = where(condition, tensor1d([index]), bestThresh);
	}
	return bestThresh;
}

const threshold = /* @__PURE__ */ op({threshold_});

/**
 * @license
 * Copyright 2021 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Applies the given transform(s) to the image(s).
 *
 * @param image 4d tensor of shape `[batch, imageHeight, imageWidth, depth]`.
 * @param transforms Projective transform matrix/matrices. A tensor1d of length
 *     8 or tensor of size N x 8. If one row of transforms is [a0, a1, a2, b0,
 *     b1, b2, c0, c1], then it maps the output point (x, y) to a transformed
 *     input point (x', y') = ((a0 x + a1 y + a2) / k, (b0 x + b1 y + b2) / k),
 *     where k = c0 x + c1 y + 1. The transforms are inverted compared to the
 *     transform mapping input points to output points.
 * @param interpolation Interpolation mode.
 *     Supported values: 'nearest', 'bilinear'. Default to 'nearest'.
 * @param fillMode Points outside the boundaries of the input are filled
 *     according to the given mode, one of 'constant', 'reflect', 'wrap',
 *     'nearest'. Default to 'constant'.
 *     'reflect': (d c b a | a b c d | d c b a ) The input is extended by
 *     reflecting about the edge of the last pixel.
 *     'constant': (k k k k | a b c d | k k k k) The input is extended by
 *     filling all values beyond the edge with the same constant value k.
 *     'wrap': (a b c d | a b c d | a b c d) The input is extended by
 *     wrapping around to the opposite edge.
 *     'nearest': (a a a a | a b c d | d d d d) The input is extended by
 *     the nearest pixel.
 * @param fillValue A float represents the value to be filled outside the
 *     boundaries when fillMode is 'constant'.
 * @param Output dimension after the transform, [height, width]. If undefined,
 *     output is the same size as input image.
 *
 * @doc {heading: 'Operations', subheading: 'Images', namespace: 'image'}
 */
function transform_(image, transforms, interpolation = 'nearest', fillMode = 'constant', fillValue = 0, outputShape) {
	const $image = convertToTensor(image, 'image', 'transform', 'float32');
	const $transforms = convertToTensor(transforms, 'transforms', 'transform', 'float32');
	assert($image.rank === 4, () => 'Error in transform: image must be rank 4,' +
		`but got rank ${$image.rank}.`);
	assert($transforms.rank === 2 &&
		($transforms.shape[0] === $image.shape[0] ||
			$transforms.shape[0] === 1) &&
		$transforms.shape[1] === 8, () => `Error in transform: Input transform should be batch x 8 or 1 x 8`);
	assert(outputShape == null || outputShape.length === 2, () => 'Error in transform: outputShape must be [height, width] or null, ' +
		`but got ${outputShape}.`);
	const inputs = {image: $image, transforms: $transforms};
	const attrs = {interpolation, fillMode, fillValue, outputShape};
	return ENGINE.runKernel(Transform, inputs, attrs);
}

const transform = /* @__PURE__ */ op({transform_});

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Copy a tensor setting everything outside a central band in each innermost
 * matrix to zero.
 *
 * The band part is computed as follows: Assume input has `k` dimensions
 * `[I, J, K, ..., M, N]`, then the output is a tensor with the same shape where
 * `band[i, j, k, ..., m, n] = in_band(m, n) * input[i, j, k, ..., m, n]`.
 * The indicator function
 * `in_band(m, n) = (num_lower < 0 || (m-n) <= num_lower)`
 * `&& (num_upper < 0 || (n-m) <= num_upper)`
 *
 * ```js
 * const x = tf.tensor2d([[ 0,  1,  2, 3],
 *                        [-1,  0,  1, 2],
 *                        [-2, -1,  0, 1],
 *                        [-3, -2, -1, 0]]);
 * let y = tf.linalg.bandPart(x, 1, -1);
 * y.print(); // [[ 0,  1,  2, 3],
 *            //  [-1,  0,  1, 2],
 *            //  [ 0, -1,  0, 1],
 *            //  [ 0, 0 , -1, 0]]
 * let z = tf.linalg.bandPart(x, 2, 1);
 * z.print(); // [[ 0,  1,  0, 0],
 *            //  [-1,  0,  1, 0],
 *            //  [-2, -1,  0, 1],
 *            //  [ 0, -2, -1, 0]]
 * ```
 *
 * @param x Rank `k` tensor
 * @param numLower Number of subdiagonals to keep.
 *   If negative, keep entire lower triangle.
 * @param numUpper Number of subdiagonals to keep.
 *   If negative, keep entire upper triangle.
 * @returns Rank `k` tensor of the same shape as input.
 *   The extracted banded tensor.
 *
 * @doc {heading:'Operations', subheading:'Linear Algebra', namespace:'linalg'}
 */
function bandPart_(a, numLower, numUpper) {
	const $a = convertToTensor(a, 'a', 'bandPart');
	assert($a.rank >= 2, () => `bandPart(): Rank must be at least 2, got ${$a.rank}.`);
	const shape = $a.shape;
	const [M, N] = $a.shape.slice(-2);
	let $numLower;
	let $numUpper;
	if (typeof numLower === 'number') {
		assert(numLower % 1 === 0, () => `bandPart(): numLower must be an integer, got ${numLower}.`);
		assert(numLower <= M, () => `bandPart(): numLower (${numLower})` +
			` must not be greater than the number of rows (${M}).`);
		$numLower =
			convertToTensor(numLower < 0 ? M : numLower, 'numLower', 'bandPart');
	} else {
		assert(numLower.dtype === 'int32', () => `bandPart(): numLower's dtype must be an int32.`);
		// If numLower is a Scalar, checking `numLower <= M` could hurt performance,
		// but minimum(numLower, M) could avoid unexpected results.
		$numLower = where(less(numLower, 0), M, minimum(numLower, M));
	}
	if (typeof numUpper === 'number') {
		assert(numUpper % 1 === 0, () => `bandPart(): numUpper must be an integer, got ${numUpper}.`);
		assert(numUpper <= N, () => `bandPart(): numUpper (${numUpper})` +
			` must not be greater than the number of columns (${N}).`);
		$numUpper =
			convertToTensor(numUpper < 0 ? N : numUpper, 'numUpper', 'bandPart');
	} else {
		assert(numUpper.dtype === 'int32', () => `bandPart(): numUpper's dtype must be an int32.`);
		$numUpper = where(less(numUpper, 0), N, minimum(numUpper, N));
	}
	const i = reshape$1(range(0, M, 1, 'int32'), [-1, 1]);
	const j = range(0, N, 1, 'int32');
	const ij = sub$1(i, j);
	const inBand = logicalAnd$1(lessEqual$1(ij, $numLower), greaterEqual$1(ij, neg$1($numUpper)));
	const zero = zeros$1([M, N], $a.dtype);
	return reshape$1(stack(unstack(reshape$1($a, [-1, M, N]))
		.map(mat => where(inBand, mat, zero))), shape);
}

const bandPart = /* @__PURE__ */ op({bandPart_});

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Gram-Schmidt orthogonalization.
 *
 * ```js
 * const x = tf.tensor2d([[1, 2], [3, 4]]);
 * let y = tf.linalg.gramSchmidt(x);
 * y.print();
 * console.log('Orthogonalized:');
 * y.dot(y.transpose()).print();  // should be nearly the identity matrix.
 * console.log('First row direction maintained:');
 * const data = await y.array();
 * console.log(data[0][1] / data[0][0]);  // should be nearly 2.
 * ```
 *
 * @param xs The vectors to be orthogonalized, in one of the two following
 *   formats:
 *   - An Array of `tf.Tensor1D`.
 *   - A `tf.Tensor2D`, i.e., a matrix, in which case the vectors are the rows
 *     of `xs`.
 *   In each case, all the vectors must have the same length and the length
 *   must be greater than or equal to the number of vectors.
 * @returns The orthogonalized and normalized vectors or matrix.
 *   Orthogonalization means that the vectors or the rows of the matrix
 *   are orthogonal (zero inner products). Normalization means that each
 *   vector or each row of the matrix has an L2 norm that equals `1`.
 *
 * @doc {heading:'Operations', subheading:'Linear Algebra', namespace:'linalg'}
 */
function gramSchmidt_(xs) {
	let inputIsTensor2D;
	if (Array.isArray(xs)) {
		inputIsTensor2D = false;
		assert(xs != null && xs.length > 0, () => 'Gram-Schmidt process: input must not be null, undefined, or ' +
			'empty');
		const dim = xs[0].shape[0];
		for (let i = 1; i < xs.length; ++i) {
			assert(xs[i].shape[0] === dim, () => 'Gram-Schmidt: Non-unique lengths found in the input vectors: ' +
				`(${xs[i].shape[0]} vs. ${dim})`);
		}
	} else {
		inputIsTensor2D = true;
		xs = split$1(xs, xs.shape[0], 0).map(x => squeeze(x, [0]));
	}
	assert(xs.length <= xs[0].shape[0], () => `Gram-Schmidt: Number of vectors (${xs.length}) exceeds ` +
		`number of dimensions (${xs[0].shape[0]}).`);
	const ys = [];
	const xs1d = xs;
	for (let i = 0; i < xs.length; ++i) {
		ys.push(ENGINE.tidy(() => {
			let x = xs1d[i];
			if (i > 0) {
				for (let j = 0; j < i; ++j) {
					const proj = mul(sum$1(mul(ys[j], x)), ys[j]);
					x = sub$1(x, proj);
				}
			}
			return div$1(x, norm(x, 'euclidean'));
		}));
	}
	if (inputIsTensor2D) {
		return stack(ys, 0);
	} else {
		return ys;
	}
}

const gramSchmidt = /* @__PURE__ */ op({gramSchmidt_});

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Compute QR decomposition of m-by-n matrix using Householder transformation.
 *
 * Implementation based on
 *   [http://www.cs.cornell.edu/~bindel/class/cs6210-f09/lec18.pdf]
 * (http://www.cs.cornell.edu/~bindel/class/cs6210-f09/lec18.pdf)
 *
 * ```js
 * const a = tf.tensor2d([[1, 2], [3, 4]]);
 * let [q, r] = tf.linalg.qr(a);
 * console.log('Q');
 * q.print();
 * console.log('R');
 * r.print();
 * console.log('Orthogonalized');
 * q.dot(q.transpose()).print()  // should be nearly the identity matrix.
 * console.log('Reconstructed');
 * q.dot(r).print(); // should be nearly [[1, 2], [3, 4]];
 * ```
 *
 * @param x The `tf.Tensor` to be QR-decomposed. Must have rank >= 2. Suppose
 *   it has the shape `[..., M, N]`.
 * @param fullMatrices An optional boolean parameter. Defaults to `false`.
 *   If `true`, compute full-sized `Q`. If `false` (the default),
 *   compute only the leading N columns of `Q` and `R`.
 * @returns An `Array` of two `tf.Tensor`s: `[Q, R]`. `Q` is a unitary matrix,
 *   i.e., its columns all have unit norm and are mutually orthogonal.
 *   If `M >= N`,
 *     If `fullMatrices` is `false` (default),
 *       - `Q` has a shape of `[..., M, N]`,
 *       - `R` has a shape of `[..., N, N]`.
 *     If `fullMatrices` is `true` (default),
 *       - `Q` has a shape of `[..., M, M]`,
 *       - `R` has a shape of `[..., M, N]`.
 *   If `M < N`,
 *     - `Q` has a shape of `[..., M, M]`,
 *     - `R` has a shape of `[..., M, N]`.
 * @throws If the rank of `x` is less than 2.
 *
 * @doc {heading:'Operations',
 *       subheading:'Linear Algebra',
 *       namespace:'linalg'}
 */
function qr_(x, fullMatrices = false) {
	assert(x.rank >= 2, () => `qr() requires input tensor to have a rank >= 2, but got rank ${x.rank}`);
	if (x.rank === 2) {
		return qr2d(x, fullMatrices);
	} else {
		// Rank > 2.
		// TODO(cais): Below we split the input into individual 2D tensors,
		//   perform QR decomposition on them and then stack the results back
		//   together. We should explore whether this can be parallelized.
		const outerDimsProd = x.shape.slice(0, x.shape.length - 2)
							   .reduce((value, prev) => value * prev);
		const x2ds = unstack(reshape$1(x, [
			outerDimsProd, x.shape[x.shape.length - 2],
			x.shape[x.shape.length - 1]
		]), 0);
		const q2ds = [];
		const r2ds = [];
		x2ds.forEach(x2d => {
			const [q2d, r2d] = qr2d(x2d, fullMatrices);
			q2ds.push(q2d);
			r2ds.push(r2d);
		});
		const q = reshape$1(stack(q2ds, 0), x.shape);
		const r = reshape$1(stack(r2ds, 0), x.shape);
		return [q, r];
	}
}

function qr2d(x, fullMatrices = false) {
	return ENGINE.tidy(() => {
		assert(x.shape.length === 2, () => `qr2d() requires a 2D Tensor, but got a ${x.shape.length}D Tensor.`);
		const m = x.shape[0];
		const n = x.shape[1];
		let q = eye(m); // Orthogonal transform so far.
		let r = clone(x); // Transformed matrix so far.
		const one2D = tensor2d([[1]], [1, 1]);
		let w = clone(one2D);
		const iters = m >= n ? n : m;
		for (let j = 0; j < iters; ++j) {
			// This tidy within the for-loop ensures we clean up temporary
			// tensors as soon as they are no longer needed.
			const rTemp = r;
			const wTemp = w;
			const qTemp = q;
			[w, r, q] = ENGINE.tidy(() => {
				// Find H = I - tau * w * w', to put zeros below R(j, j).
				const rjEnd1 = slice$1(r, [j, j], [m - j, 1]);
				const normX = norm(rjEnd1);
				const rjj = slice$1(r, [j, j], [1, 1]);
				// The sign() function returns 0 on 0, which causes division by zero.
				const s = where(greater$1(rjj, 0), tensor2d([[-1]]), tensor2d([[1]]));
				const u1 = sub$1(rjj, mul(s, normX));
				const wPre = div$1(rjEnd1, u1);
				if (wPre.shape[0] === 1) {
					w = clone(one2D);
				} else {
					w = concat([
						one2D,
						slice$1(wPre, [1, 0], [wPre.shape[0] - 1, wPre.shape[1]])
					], 0);
				}
				const tau = neg$1(div$1(matMul$1(s, u1), normX));
				// -- R := HR, Q := QH.
				const rjEndAll = slice$1(r, [j, 0], [m - j, n]);
				const tauTimesW = mul(tau, w);
				const wT = transpose$1(w);
				if (j === 0) {
					r = sub$1(rjEndAll, matMul$1(tauTimesW, matMul$1(wT, rjEndAll)));
				} else {
					const rTimesTau = sub$1(rjEndAll, matMul$1(tauTimesW, matMul$1(wT, rjEndAll)));
					r = concat([slice$1(r, [0, 0], [j, n]), rTimesTau], 0);
				}
				const tawTimesWT = transpose$1(tauTimesW);
				const qAllJEnd = slice$1(q, [0, j], [m, q.shape[1] - j]);
				if (j === 0) {
					q = sub$1(qAllJEnd, matMul$1(matMul$1(qAllJEnd, w), tawTimesWT));
				} else {
					const qTimesTau = sub$1(qAllJEnd, matMul$1(matMul$1(qAllJEnd, w), tawTimesWT));
					q = concat([slice$1(q, [0, 0], [m, j]), qTimesTau], 1);
				}
				return [w, r, q];
			});
			dispose([rTemp, wTemp, qTemp]);
		}
		if (!fullMatrices && m > n) {
			q = slice$1(q, [0, 0], [m, n]);
			r = slice$1(r, [0, 0], [n, n]);
		}
		return [q, r];
	});
}

const qr = /* @__PURE__ */ op({qr_});

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
var Reduction;
(function (Reduction) {
	Reduction[Reduction["NONE"] = 0] = "NONE";
	Reduction[Reduction["MEAN"] = 1] = "MEAN";
	Reduction[Reduction["SUM"] = 2] = "SUM";
	Reduction[Reduction["SUM_BY_NONZERO_WEIGHTS"] = 3] = "SUM_BY_NONZERO_WEIGHTS";
})(Reduction || (Reduction = {}));

/**
 * Computes the weighted loss between two tensors.
 *
 * @param losses Tensor of shape `[batch_size, d1, ..., dN]`.
 * @param weights Tensor whose rank is either 0, or the same rank as
 *    `losses`, and must be broadcastable to `losses` (i.e., all
 *    dimensions must be either `1`, or the same as the corresponding
 *    `losses` dimension).
 *
 * @doc {heading: 'Training', subheading: 'Losses', namespace: 'losses'}
 */
function computeWeightedLoss_(losses, weights, reduction = Reduction.SUM_BY_NONZERO_WEIGHTS) {
	const $losses = convertToTensor(losses, 'losses', 'computeWeightedLoss');
	let $weights = null;
	if (weights != null) {
		$weights = convertToTensor(weights, 'weights', 'computeWeightedLoss');
	}
	const weightedLoss = ($weights == null) ? $losses : mul($losses, $weights);
	if (reduction === Reduction.NONE) {
		return weightedLoss;
	}
	if (reduction === Reduction.SUM) {
		return sum$1(weightedLoss);
	}
	if (reduction === Reduction.MEAN) {
		if ($weights == null) {
			return mean$1(weightedLoss);
		} else {
			const broadcastFactor = $losses.size / $weights.size;
			const result = div$1(sum$1(weightedLoss), sum$1($weights));
			return broadcastFactor > 1 ? div$1(result, scalar(broadcastFactor)) :
				result;
		}
	}
	if (reduction === Reduction.SUM_BY_NONZERO_WEIGHTS) {
		if ($weights == null) {
			return div$1(sum$1(weightedLoss), scalar($losses.size));
		} else {
			const broadcastedWeights = mul($weights, ones($losses.shape));
			const numNonZeros = cast$1(sum$1(notEqual(broadcastedWeights, scalar(0))), 'float32');
			return div$1(sum$1(weightedLoss), numNonZeros);
		}
	}
	throw Error(`Unknown reduction: ${reduction}`);
}

const computeWeightedLoss = /* @__PURE__ */ op({computeWeightedLoss_});

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Computes the absolute difference loss between two tensors.
 *
 * @param labels The ground truth output tensor, same dimensions as
 *    'predictions'.
 * @param predictions The predicted outputs.
 * @param weights Tensor whose rank is either 0, or the same rank as
 *    `labels`, and must be broadcastable to `labels` (i.e., all dimensions
 *    must be either `1`, or the same as the corresponding `losses`
 *    dimension).
 * @param reduction Type of reduction to apply to loss. Should be of type
 *    `Reduction`
 *
 * @doc {heading: 'Training', subheading: 'Losses', namespace: 'losses'}
 */
function absoluteDifference_(labels, predictions, weights, reduction = Reduction.SUM_BY_NONZERO_WEIGHTS) {
	const $labels = convertToTensor(labels, 'labels', 'absoluteDifference');
	const $predictions = convertToTensor(predictions, 'predictions', 'absoluteDifference');
	let $weights = null;
	if (weights != null) {
		$weights = convertToTensor(weights, 'weights', 'absoluteDifference');
	}
	assertShapesMatch($labels.shape, $predictions.shape, 'Error in absoluteDifference: ');
	const losses = abs$1(sub$1($labels, $predictions));
	return computeWeightedLoss(losses, $weights, reduction);
}

const absoluteDifference = /* @__PURE__ */ op({absoluteDifference_});

/**
 * Computes the cosine distance loss between two tensors.
 *
 * @param labels The ground truth output tensor, same dimensions as
 *    'predictions'.
 * @param predictions The predicted outputs.
 * @param axis The dimension along which the cosine distance is computed.
 * @param weights Tensor whose rank is either 0, or the same rank as
 *    `labels`, and must be broadcastable to `labels` (i.e., all dimensions
 *    must be either `1`, or the same as the corresponding `losses`
 *    dimension).
 * @param reduction Type of reduction to apply to loss. Should be of type
 *    `Reduction`
 *
 * @doc {heading: 'Training', subheading: 'Losses', namespace: 'losses'}
 */
function cosineDistance_(labels, predictions, axis, weights, reduction = Reduction.SUM_BY_NONZERO_WEIGHTS) {
	const $labels = convertToTensor(labels, 'labels', 'cosineDistance');
	const $predictions = convertToTensor(predictions, 'predictions', 'cosineDistance');
	let $weights = null;
	if (weights != null) {
		$weights = convertToTensor(weights, 'weights', 'cosineDistance');
	}
	assertShapesMatch($labels.shape, $predictions.shape, 'Error in cosineDistance: ');
	const one = scalar(1);
	const losses = sub$1(one, sum$1(mul($labels, $predictions), axis, true));
	return computeWeightedLoss(losses, $weights, reduction);
}

const cosineDistance = /* @__PURE__ */ op({cosineDistance_});

/**
 * Computes the Hinge loss between two tensors.
 *
 * @param labels The ground truth output tensor, same dimensions as
 *    'predictions'.
 * @param predictions The predicted outputs.
 * @param weights Tensor whose rank is either 0, or the same rank as
 *    `labels`, and must be broadcastable to `labels` (i.e., all dimensions
 *    must be either `1`, or the same as the corresponding `losses`
 *    dimension).
 * @param reduction Type of reduction to apply to loss. Should be of type
 *    `Reduction`
 *
 * @doc {heading: 'Training', subheading: 'Losses', namespace: 'losses'}
 */
function hingeLoss_(labels, predictions, weights, reduction = Reduction.SUM_BY_NONZERO_WEIGHTS) {
	let $labels = convertToTensor(labels, 'labels', 'hingeLoss');
	const $predictions = convertToTensor(predictions, 'predictions', 'hingeLoss');
	let $weights = null;
	if (weights != null) {
		$weights = convertToTensor(weights, 'weights', 'hingeLoss');
	}
	assertShapesMatch($labels.shape, $predictions.shape, 'Error in hingeLoss: ');
	const one = scalar(1);
	// Convert binary labels to (-1, 1)
	$labels = sub$1(mul(scalar(2), $labels), one);
	const losses = relu$1(sub$1(one, mul($labels, $predictions)));
	return computeWeightedLoss(losses, $weights, reduction);
}

const hingeLoss = /* @__PURE__ */ op({hingeLoss_});

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Computes the Huber loss between two tensors.
 *
 * @param labels The ground truth output tensor, same dimensions as
 *    'predictions'.
 * @param predictions The predicted outputs.
 * @param weights Tensor whose rank is either 0, or the same rank as
 *    `labels`, and must be broadcastable to `labels` (i.e., all dimensions
 *    must be either `1`, or the same as the corresponding `losses`
 *    dimension).
 * @param delta Point where Huber loss changes from quadratic to linear.
 * @param reduction Type of reduction to apply to loss. Should be of type
 *    `Reduction`.
 *
 * @doc {heading: 'Training', subheading: 'Losses', namespace: 'losses'}
 */
function huberLoss_(labels, predictions, weights, delta = 1.0, reduction = Reduction.SUM_BY_NONZERO_WEIGHTS) {
	const $labels = convertToTensor(labels, 'labels', 'huberLoss');
	const $predictions = convertToTensor(predictions, 'predictions', 'huberLoss');
	let $weights = null;
	if (weights != null) {
		$weights = convertToTensor(weights, 'weights', 'huberLoss');
	}
	assertShapesMatch($labels.shape, $predictions.shape, 'Error in huberLoss: ');
	const deltaScalar = scalar(delta);
	const error = abs$1(sub$1($predictions, $labels));
	const quadratic = minimum(error, deltaScalar);
	const linear = sub$1(error, quadratic);
	const losses = add$1(mul(scalar(0.5), square(quadratic)), mul(deltaScalar, linear));
	return computeWeightedLoss(losses, $weights, reduction);
}

const huberLoss = /* @__PURE__ */ op({huberLoss_});

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Computes the log loss between two tensors.
 *
 * @param labels The ground truth output tensor, same dimensions as
 *    'predictions'.
 * @param predictions The predicted outputs.
 * @param weights Tensor whose rank is either 0, or the same rank as
 *    `labels`, and must be broadcastable to `labels` (i.e., all dimensions
 *    must be either `1`, or the same as the corresponding `losses`
 *    dimension).
 * @param epsilon A small increment to avoid taking log of zero
 * @param reduction Type of reduction to apply to loss. Should be of type
 *    `Reduction`
 *
 * @doc {heading: 'Training', subheading: 'Losses', namespace: 'losses'}
 */
function logLoss_(labels, predictions, weights, epsilon = 1e-7, reduction = Reduction.SUM_BY_NONZERO_WEIGHTS) {
	const $labels = convertToTensor(labels, 'labels', 'logLoss');
	const $predictions = convertToTensor(predictions, 'predictions', 'logLoss');
	let $weights = null;
	if (weights != null) {
		$weights = convertToTensor(weights, 'weights', 'logLoss');
	}
	assertShapesMatch($labels.shape, $predictions.shape, 'Error in logLoss: ');
	const one = scalar(1);
	const epsilonScalar = scalar(epsilon);
	const l1 = neg$1(mul($labels, log$1(add$1($predictions, epsilonScalar))));
	const l2 = mul(sub$1(one, $labels), log$1(add$1(sub$1(one, $predictions), epsilonScalar)));
	const losses = sub$1(l1, l2);
	return computeWeightedLoss(losses, $weights, reduction);
}

const logLoss = /* @__PURE__ */ op({logLoss_});

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Computes the mean squared error between two tensors.
 *
 * @param labels The ground truth output tensor, same dimensions as
 *    'predictions'.
 * @param predictions The predicted outputs.
 * @param weights Tensor whose rank is either 0, or the same rank as
 *    `labels`, and must be broadcastable to `labels` (i.e., all dimensions
 *    must be either `1`, or the same as the corresponding `losses`
 *    dimension).
 * @param reduction Type of reduction to apply to loss. Should be of type
 *    `Reduction`
 *
 * @doc {heading: 'Training', subheading: 'Losses', namespace: 'losses'}
 */
function meanSquaredError_(labels, predictions, weights, reduction = Reduction.SUM_BY_NONZERO_WEIGHTS) {
	const $labels = convertToTensor(labels, 'labels', 'meanSquaredError');
	const $predictions = convertToTensor(predictions, 'predictions', 'meanSquaredError');
	let $weights = null;
	if (weights != null) {
		$weights = convertToTensor(weights, 'weights', 'meanSquaredError');
	}
	assertShapesMatch($labels.shape, $predictions.shape, 'Error in meanSquaredError: ');
	const losses = squaredDifference($labels, $predictions);
	return computeWeightedLoss(losses, $weights, reduction);
}

const meanSquaredError = /* @__PURE__ */ op({meanSquaredError_});

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
function sigmoidCrossEntropyWithLogits_(labels, logits) {
	const $labels = convertToTensor(labels, 'labels', 'sigmoidCrossEntropyWithLogits');
	const $logits = convertToTensor(logits, 'logits', 'sigmoidCrossEntropyWithLogits');
	assertShapesMatch($labels.shape, $logits.shape, 'Error in sigmoidCrossEntropyWithLogits: ');
	/**
	 * Implementation Details:
	 *
	 * For brevity, let `x = logits`, `z = labels`.  The logistic loss is
	 *     z * -log(sigmoid(x)) + (1 - z) * -log(1 - sigmoid(x))
	 *   = z * -log(1 / (1 + exp(-x))) + (1 - z) * -log(exp(-x) / (1 + exp(-x)))
	 *   = z * log(1 + exp(-x)) + (1 - z) * (-log(exp(-x)) + log(1 + exp(-x)))
	 *   = z * log(1 + exp(-x)) + (1 - z) * (x + log(1 + exp(-x))
	 *   = (1 - z) * x + log(1 + exp(-x))
	 *   = x - x * z + log(1 + exp(-x))
	 *
	 *   For x < 0, to avoid overflow in exp(-x), we reformulate the above
	 *     x - x * z + log(1 + exp(-x))
	 *   = log(exp(x)) - x * z + log(1 + exp(-x))
	 *   = - x * z + log(1 + exp(x))
	 *
	 * Hence, to ensure stability and avoid overflow, the implementation uses
	 * this equivalent formulation:
	 *     max(x, 0) - x * z + log(1 + exp(-abs(x)))
	 */
	const maxOutput = relu$1($logits);
	const outputXTarget = mul($logits, $labels);
	const sigmoidOutput = log1p$1(exp$1(neg$1(abs$1($logits))));
	return add$1(sub$1(maxOutput, outputXTarget), sigmoidOutput);
}

/**
 * Computes the sigmoid cross entropy loss between two tensors.
 *
 * If labelSmoothing is nonzero, smooth the labels towards 1/2:
 *
 *   newMulticlassLabels = multiclassLabels * (1 - labelSmoothing)
 *                         + 0.5 * labelSmoothing
 *
 * @param multiClassLabels The ground truth output tensor of shape
 * [batch_size, num_classes], same dimensions as 'predictions'.
 * @param logits The predicted outputs.
 * @param weights Tensor whose rank is either 0, or the same rank as
 *    `labels`, and must be broadcastable to `labels` (i.e., all dimensions
 *    must be either `1`, or the same as the corresponding `losses`
 *    dimension).
 * @param labelSmoothing If greater than 0, then smooth the labels.
 * @param reduction Type of reduction to apply to loss. Should be of type
 *    `Reduction`
 *
 * @doc { heading: 'Training', subheading: 'Losses', namespace: 'losses' }
 */
function sigmoidCrossEntropy_(multiClassLabels, logits, weights, labelSmoothing = 0, reduction = Reduction.SUM_BY_NONZERO_WEIGHTS) {
	let $multiClassLabels = convertToTensor(multiClassLabels, 'multiClassLabels', 'sigmoidCrossEntropy');
	const $logits = convertToTensor(logits, 'logits', 'sigmoidCrossEntropy');
	let $weights = null;
	if (weights != null) {
		$weights = convertToTensor(weights, 'weights', 'sigmoidCrossEntropy');
	}
	assertShapesMatch($multiClassLabels.shape, $logits.shape, 'Error in sigmoidCrossEntropy: ');
	if (labelSmoothing > 0) {
		const labelSmoothingScalar = scalar(labelSmoothing);
		const one = scalar(1);
		const half = scalar(0.5);
		$multiClassLabels =
			add$1(mul($multiClassLabels, sub$1(one, labelSmoothingScalar)), mul(half, labelSmoothingScalar));
	}
	const losses = sigmoidCrossEntropyWithLogits_($multiClassLabels, $logits);
	return computeWeightedLoss(losses, $weights, reduction);
}

const sigmoidCrossEntropy = /* @__PURE__ */ op({sigmoidCrossEntropy_});

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Computes softmax cross entropy between logits and labels.
 *
 * Measures the probability error in discrete classification tasks in which
 * the classes are mutually exclusive (each entry is in exactly one class).
 * For example, each CIFAR-10 image is labeled with one and only one label: an
 * image can be a dog or a truck, but not both.
 *
 * `NOTE`: While the classes are mutually exclusive, their probabilities need
 * not be. All that is required is that each row of labels is a valid
 * probability distribution. If they are not, the computation of the gradient
 * will be incorrect.
 *
 * `WARNING`: This op expects unscaled logits, since it performs a softmax on
 * logits internally for efficiency. Do not call this op with the output of
 * softmax, as it will produce incorrect results.
 *
 * logits and labels must have the same shape, e.g. [batch_size, num_classes]
 * and the same dtype.
 * @param labels The labels array.
 * @param logits The logits array.
 * @param dim The dimension softmax would be performed on. Defaults to `-1`
 *     which indicates the last dimension.
 */
function softmaxCrossEntropyWithLogits_(labels, logits, dim = -1) {
	if (dim === -1) {
		dim = logits.rank - 1;
	}
	if (dim !== logits.rank - 1) {
		throw Error(`Softmax cross entropy along a non-last dimension is not yet ` +
			`supported. Labels / logits was rank ${logits.rank} ` +
			`and dim was ${dim}`);
	}
	// Use a custom gradient for numerical stability.
	const customOp = customGrad((labels, logits, save) => {
		// Reference:
		//   1. http://cs231n.github.io/linear-classify/#softmax
		//   2. https://blog.feedly.com/tricks-of-the-trade-logsumexp/
		const keepDims = true;
		const lse = logSumExp(logits, [dim], keepDims);
		const logResult = sub$1(cast$1(logits, 'float32'), lse);
		save([labels, logResult]);
		const costVector = neg$1(mul(logResult, labels));
		const value = sum$1(costVector, [dim]);
		const gradFunc = (dy, saved) => {
			const [labels, logResult] = saved;
			const dyShape = expandShapeToKeepDim(dy.shape, [dim]);
			return [
				mul(reshape$1(dy, dyShape), sub$1(cast$1(labels, 'float32'), exp$1(logResult))),
				mul(reshape$1(dy, dyShape), sub$1(exp$1(logResult), cast$1(labels, 'float32'))),
			];
		};
		return {value, gradFunc};
	});
	return customOp(labels, logits);
}

/**
 * Computes the softmax cross entropy loss between two tensors.
 *
 * If labelSmoothing is nonzero, smooth the labels towards 1/2:
 *
 *   newOnehotLabels = onehotLabels * (1 - labelSmoothing)
 *                         + labelSmoothing / numClasses
 *
 * @param onehotLabels One hot encoded labels
 *    [batch_size, num_classes], same dimensions as 'predictions'.
 * @param logits The predicted outputs.
 * @param weights Tensor whose rank is either 0, or 1, and must be
 *    broadcastable to `loss`  of shape [batch_size]
 * @param labelSmoothing If greater than 0, then smooth the labels.
 * @param reduction Type of reduction to apply to loss. Should be of type
 *    `Reduction`
 *
 * @doc { heading: 'Training', subheading: 'Losses', namespace: 'losses' }
 */
function softmaxCrossEntropy_(onehotLabels, logits, weights, labelSmoothing = 0, reduction = Reduction.SUM_BY_NONZERO_WEIGHTS) {
	let $onehotLabels = convertToTensor(onehotLabels, 'onehotLabels', 'softmaxCrossEntropy');
	const $logits = convertToTensor(logits, 'logits', 'softmaxCrossEntropy');
	let $weights = null;
	if (weights != null) {
		$weights = convertToTensor(weights, 'weights', 'softmaxCrossEntropy');
	}
	assertShapesMatch($onehotLabels.shape, $logits.shape, 'Error in softmaxCrossEntropy: ');
	if (labelSmoothing > 0) {
		const labelSmoothingScalar = scalar(labelSmoothing);
		const one = scalar(1);
		const numClasses = scalar($onehotLabels.shape[1]);
		$onehotLabels =
			add$1(mul($onehotLabels, sub$1(one, labelSmoothingScalar)), div$1(labelSmoothingScalar, numClasses));
	}
	const losses = softmaxCrossEntropyWithLogits_($onehotLabels, $logits);
	return computeWeightedLoss(losses, $weights, reduction);
}

const softmaxCrossEntropy = /* @__PURE__ */ op({softmaxCrossEntropy_});

/**
 * @license
 * Copyright 2021 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * The input SparseTensor is represented via the map of inputs {`indices`,
 * `values`, `denseShape`}. The output SparseTensor has the same `denseShape`
 * but with indices `outputIndices` and values `outputValues`. This op inserts a
 * single entry for every row that doesn't have any values. The index is created
 * as `[row, 0, ..., 0]` and the inserted value is `defaultValue`.
 *
 * For example, suppose `spInput` has shape [5, 6] and non-empty values:
 * [0, 1]: a
 * [0, 3]: b
 * [2, 0]: c
 * [3, 1]: d
 *
 * Rows 1 and 4 are empty, so the output will be of shape [5, 6] with values:
 * [0, 1]: a
 * [0, 3]: b
 * [1, 0]: `defaultValue`
 * [2, 0]: c
 * [3, 1]: d
 * [4, 0]: `defaultValue`
 *
 * The output SparseTensor will be in row-major order and will have the same
 * shape as the input.
 *
 * This op also returns an indicator vector shaped [dense_shape[0]] such that
 * emptyRowIndicator[i] = True iff row i was an empty row.
 *
 * And a reverse index map vector shaped [indices.shape[0]] that is used during
 * backpropagation, reverseIndexMap[i] = outi s.t. indices[i, j] ==
 * outputIndices[outi, j] for all j
 *
 * ```js
 * const result = tf.sparse.sparseFillEmptyRows(
 *   [[0, 0], [1, 0], [1, 3], [1, 4], [3, 2], [3, 3]],
 *   [0, 10, 13, 14, 32, 33], [5, 6], -1);
 * console.log(result);
 * result['outputIndices'].print(); // [[0, 0], [1, 0], [1, 3], [1, 4],
 *                                  //  [2, 0], [3, 2], [3, 3], [4, 0]]
 * result['outputValues'].print(); // [0, 10, 13, 14,-1, 32, 33, -1]
 * result['emptyRowIndicator'].print(); // [false, false, true, false, true]
 * result['reverseIndexMap'].print(); // [0, 1, 2, 3, 5, 6]
 * ```
 * @param indices: 2-D. The indices of the sparse tensor.
 * @param values: 1-D. The values of the sparse tensor.
 * @param denseShape: 1-D. The shape of the sparse tensor.
 * @param defaultValue: 0-D. Default value to insert into location [row, 0, ...,
 *     0] for rows missing from the input sparse tensor.
 * @return A map with the following properties:
 *     - outputIndices
 *     - outputValues: 1-D. The values of the filled sparse tensor.
 *     - emptyRowIndicator: 1-D. Whether the dense row was missing in the input
 * sparse tensor.
 *     - reverseIndexMap: 1-D. A map from the input indices to the output
 * indices.
 * @doc {heading: 'Operations', subheading: 'Sparse'}
 */
function sparseFillEmptyRows_(indices, values, denseShape, defaultValue) {
	const $indices = convertToTensor(indices, 'indices', 'sparseFillEmptyRows', 'int32');
	const $values = convertToTensor(values, 'values', 'sparseFillEmptyRows');
	const $denseShape = convertToTensor(denseShape, 'denseShape', 'sparseFillEmptyRows', 'int32');
	const $defaultValue = convertToTensor(defaultValue, 'defaultValue', 'sparseFillEmptyRows', $values.dtype);
	if ($indices.rank !== 2) {
		throw new Error(`Indices should be Tensor2D but received shape
        ${$indices.shape}`);
	}
	if ($values.rank !== 1) {
		throw new Error(`Values should be Tensor1D but received shape ${$values.shape}`);
	}
	if ($denseShape.rank !== 1) {
		throw new Error(`Dense shape should be Tensor1D but received shape ${$denseShape.shape}`);
	}
	if ($defaultValue.rank !== 0) {
		throw new Error(`Default value should be a scalar but received shape ${$defaultValue.shape}`);
	}
	const inputs = {
		indices: $indices,
		values: $values,
		denseShape: $denseShape,
		defaultValue: $defaultValue
	};
	const result = ENGINE.runKernel(SparseFillEmptyRows, inputs);
	return {
		outputIndices: result[0],
		outputValues: result[1],
		emptyRowIndicator: result[2],
		reverseIndexMap: result[3]
	};
}

const sparseFillEmptyRows = /* @__PURE__ */ op({sparseFillEmptyRows_});

/**
 * @license
 * Copyright 2021 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * This operation has the same semantics as reshape on the represented dense
 * tensor. The `inputIndices` are recomputed based on the requested `newShape`.
 * If one component of `newShape` is the special value -1, the size of that
 * dimension is computed so that the total dense size remains constant. At most
 * one component of `newShape` can be -1. The number of dense elements implied
 * by `newShape` must be the same as the number of dense elements originally
 * implied by `inputShape`. Reshaping does not affect the order of values in the
 * SparseTensor. If the input tensor has rank R_in and N non-empty values, and
 * `newShape` has length R_out, then `inputIndices` has shape [N, R_in],
 * `inputShape` has length R_in, `outputIndices` has shape [N, R_out], and
 * `outputShape` has length R_out.
 *
 * ```js
 * const result = tf.sparse.sparseReshape(
 *   [[0, 0, 0], [0, 0, 1], [0, 1, 0], [1, 0, 0], [1, 2, 3]],
 *   [2, 3, 6], [9, -1]);
 * console.log(result);
 * result['outputIndices'].print(); //[[0, 0], [0, 1], [1, 2], [4, 2], [8, 1]]
 * result['outputShape'].print(); // [9, 4]
 * ```
 * @param inputIndices: 2-D. N x R_in matrix with the indices of non-empty
 * values in a SparseTensor.
 * @param inputShape: 1-D. R_in Tensor1D with the input SparseTensor's dense
 * shape.
 * @param newShape: 1-D. R_out Tensor1D with the requested new dense shape.
 * @return A map with the following properties:
 *     - outputIndices: 2-D. N x R_out matrix with the updated indices of
 *       non-empty values in the output SparseTensor.
 *     - outputShape: 1-D. R_out vector with the full dense shape of the output
 *       SparseTensor. This is the same as newShape but with any -1 dimensions
 *        filled in.
 * @doc {heading: 'Operations', subheading: 'Sparse'}
 */
function sparseReshape_(inputIndices, inputShape, newShape) {
	const $inputIndices = convertToTensor(inputIndices, 'inputIndices', 'sparseReshape', 'int32');
	const $inputShape = convertToTensor(inputShape, 'inputShape', 'sparseReshape', 'int32');
	const $newShape = convertToTensor(newShape, 'newShape', 'sparseReshape', 'int32');
	if ($inputIndices.rank !== 2) {
		throw new Error(`Input indices should be Tensor2D but received shape
        ${$inputIndices.shape}`);
	}
	if ($inputShape.rank !== 1) {
		throw new Error(`Input shape should be Tensor1D but received shape ${$inputShape.shape}`);
	}
	if ($newShape.rank !== 1) {
		throw new Error(`New shape should be Tensor1D but received shape ${$newShape.shape}`);
	}
	const inputs = {
		inputIndices: $inputIndices,
		inputShape: $inputShape,
		newShape: $newShape
	};
	const result = ENGINE.runKernel(SparseReshape, inputs);
	return {outputIndices: result[0], outputShape: result[1]};
}

const sparseReshape = /* @__PURE__ */ op({sparseReshape_});

/**
 * @license
 * Copyright 2021 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Computes the mean along sparse segments of a tensor.
 *
 * ```js
 * const c = tf.tensor2d([[1,2,3,4], [-1,-2,-3,-4], [6,7,8,9]]);
 * // Select two rows, one segment.
 * const result1 = tf.sparse.sparseSegmentMean(c,
 *                                           tf.tensor1d([0, 1], 'int32'),
 *                                           tf.tensor1d([0, 0], 'int32'));
 * result1.print(); // [[0, 0, 0, 0]]
 *
 * // Select two rows, two segments.
 * const result2 = tf.sparse.sparseSegmentMean(c,
 *                                             tf.tensor1d([0, 1], 'int32'),
 *                                             tf.tensor1d([0, 1], 'int32'));
 * result2.print(); // [[1, 2, 3, 4], [-1, -2, -3, -4]]
 *
 * // Select all rows, two segments.
 * const result3 = tf.sparse.sparseSegmentMean(c,
 *                                             tf.tensor1d([0, 1, 2], 'int32'),
 *                                             tf.tensor1d([0, 1, 1], 'int32'));
 * result3.print(); // [[1.0, 2.0, 3.0, 4.0], [2.5, 2.5, 2.5, 2.5]]
 * ```
 * @param data: A Tensor of at least one dimension with data that will be
 *     assembled in the output.
 * @param indices: A 1-D Tensor with indices into data. Has same rank as
 *     segmentIds.
 * @param segmentIds: A 1-D Tensor with indices into the output Tensor. Values
 *     should be sorted and can be repeated.
 * @return Has same shape as data, except for dimension 0 which has equal to
 *         the number of segments.
 *
 * @doc {heading: 'Operations', subheading: 'Sparse'}
 */
function sparseSegmentMean_(data, indices, segmentIds) {
	const $data = convertToTensor(data, 'data', 'sparseSegmentMean');
	const $indices = convertToTensor(indices, 'indices', 'sparseSegmentMean', 'int32');
	const $segmentIds = convertToTensor(segmentIds, 'segmentIds', 'sparseSegmentMean', 'int32');
	if ($data.rank < 1) {
		throw new Error(`Data should be at least 1 dimensional but received scalar`);
	}
	if ($indices.rank !== 1) {
		throw new Error(`Indices should be Tensor1D but received shape
          ${$indices.shape}`);
	}
	if ($segmentIds.rank !== 1) {
		throw new Error(`Segment ids should be Tensor1D but received shape
          ${$segmentIds.shape}`);
	}
	const inputs = {
		data: $data,
		indices: $indices,
		segmentIds: $segmentIds
	};
	return ENGINE.runKernel(SparseSegmentMean, inputs);
}

const sparseSegmentMean = /* @__PURE__ */ op({sparseSegmentMean_});

/**
 * @license
 * Copyright 2021 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Computes the sum along sparse segments of a tensor.
 *
 * ```js
 * const c = tf.tensor2d([[1,2,3,4], [-1,-2,-3,-4], [5,6,7,8]]);
 * // Select two rows, one segment.
 * const result1 = tf.sparse.sparseSegmentSum(c,
 *                                           tf.tensor1d([0, 1], 'int32'),
 *                                           tf.tensor1d([0, 0], 'int32'));
 * result1.print(); // [[0, 0, 0, 0]]
 *
 * // Select two rows, two segments.
 * const result2 = tf.sparse.sparseSegmentSum(c,
 *                                           tf.tensor1d([0, 1], 'int32'),
 *                                           tf.tensor1d([0, 1], 'int32'));
 * result2.print(); // [[1, 2, 3, 4], [-1, -2, -3, -4]]
 *
 * // Select all rows, two segments.
 * const result3 = tf.sparse.sparseSegmentSum(c,
 *                                           tf.tensor1d([0, 1, 2], 'int32'),
 *                                           tf.tensor1d([0, 0, 1], 'int32'));
 * result3.print(); // [[0, 0, 0, 0], [5, 6, 7, 8]]
 * ```
 * @param data: A Tensor of at least one dimension with data that will be
 *     assembled in the output.
 * @param indices: A 1-D Tensor with indices into data. Has same rank as
 *     segmentIds.
 * @param segmentIds: A 1-D Tensor with indices into the output Tensor. Values
 *     should be sorted and can be repeated.
 * @return Has same shape as data, except for dimension 0 which has equal to
 *         the number of segments.
 *
 * @doc {heading: 'Operations', subheading: 'Sparse'}
 */
function sparseSegmentSum_(data, indices, segmentIds) {
	const $data = convertToTensor(data, 'data', 'sparseSegmentSum');
	const $indices = convertToTensor(indices, 'indices', 'sparseSegmentSum', 'int32');
	const $segmentIds = convertToTensor(segmentIds, 'segmentIds', 'sparseSegmentSum', 'int32');
	if ($data.rank < 1) {
		throw new Error(`Data should be at least 1 dimensional but received scalar`);
	}
	if ($indices.rank !== 1) {
		throw new Error(`Indices should be Tensor1D but received shape
         ${$indices.shape}`);
	}
	if ($segmentIds.rank !== 1) {
		throw new Error(`Segment ids should be Tensor1D but received shape
         ${$segmentIds.shape}`);
	}
	const inputs = {
		data: $data,
		indices: $indices,
		segmentIds: $segmentIds
	};
	return ENGINE.runKernel(SparseSegmentSum, inputs);
}

const sparseSegmentSum = /* @__PURE__ */ op({sparseSegmentSum_});

/**
 * @license
 * Copyright 2021 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Creates ngrams from ragged string data.
 *
 * This op accepts a ragged tensor with 1 ragged dimension containing only
 * strings and outputs a ragged tensor with 1 ragged dimension containing ngrams
 * of that string, joined along the innermost axis.
 *
 * ```js
 * const result = tf.string.stringNGrams(
 *   ['a', 'b', 'c', 'd'], tf.tensor1d([0, 2, 4], 'int32'),
 *   '|', [1, 2], 'LP', 'RP', -1, false);
 * result['nGrams'].print(); // ['a', 'b', 'LP|a', 'a|b', 'b|RP',
 *                           //  'c', 'd', 'LP|c', 'c|d', 'd|RP']
 * result['nGramsSplits'].print(); // [0, 5, 10]
 * ```
 * @param data: The values tensor of the ragged string tensor to make ngrams out
 *     of. Must be a 1D string tensor.
 * @param dataSplits: The splits tensor of the ragged string tensor to make
 *     ngrams out of.
 * @param separator: The string to append between elements of the token. Use ""
 *     for no separator.
 * @param nGramWidths: The sizes of the ngrams to create.
 * @param leftPad: The string to use to pad the left side of the ngram sequence.
 *     Only used if pad_width !== 0.
 * @param rightPad: The string to use to pad the right side of the ngram
 *     sequence. Only used if pad_width !== 0.
 * @param padWidth: The number of padding elements to add to each side of each
 *     sequence. Note that padding will never be greater than `nGramWidths`-1
 *     regardless of this value. If `padWidth`=-1, then add max(`nGramWidths`)-1
 *     elements.
 * @param preserveShortSequences: If true, then ensure that at least one ngram
 *     is generated for each input sequence. In particular, if an input sequence
 *     is shorter than min(ngramWidth) + 2*padWidth, then generate a single
 *     ngram containing the entire sequence. If false, then no ngrams are
 *     generated for these short input sequences.
 * @return A map with the following properties:
 *     - nGrams: The values tensor of the output ngrams ragged tensor.
 *     - nGramsSplits: The splits tensor of the output ngrams ragged tensor.
 *
 * @doc {heading: 'Operations', subheading: 'String'}
 */
function stringNGrams_(data, dataSplits, separator, nGramWidths, leftPad, rightPad, padWidth, preserveShortSequences) {
	const $data = convertToTensor(data, 'data', 'stringNGrams', 'string');
	if ($data.dtype !== 'string') {
		throw new Error('Data must be of datatype string');
	}
	if ($data.shape.length !== 1) {
		throw new Error(`Data must be a vector, saw: ${$data.shape}`);
	}
	const $dataSplits = convertToTensor(dataSplits, 'dataSplits', 'stringNGrams');
	if ($dataSplits.dtype !== 'int32') {
		throw new Error('Data splits must be of datatype int32');
	}
	const attrs = {
		separator,
		nGramWidths,
		leftPad,
		rightPad,
		padWidth,
		preserveShortSequences
	};
	const inputs = {data: $data, dataSplits: $dataSplits};
	const result = ENGINE.runKernel(StringNGrams, inputs, attrs);
	return {nGrams: result[0], nGramsSplits: result[1]};
}

const stringNGrams = /* @__PURE__ */ op({stringNGrams_});

/**
 * @license
 * Copyright 2021 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Split elements of `input` based on `delimiter` into a SparseTensor .
 *
 * Let N be the size of source (typically N will be the batch size). Split each
 * element of `input` based on `delimiter` and return a SparseTensor containing
 * the splitted tokens. Empty tokens are ignored if `skipEmpty` is set to True.
 *
 * `delimiter` can be empty, or a string of split characters. If `delimiter` is
 * an empty string, each element of `input` is split into individual
 * character strings. Otherwise every character of `delimiter` is a potential
 * split point.
 *
 * ```js
 * const result = tf.string.stringSplit(['hello world',  'a b c'], ' ');
 * result['indices'].print(); // [[0, 0], [0, 1], [1, 0], [1, 1], [1, 2]]
 * result['values'].print(); // ['hello', 'world', 'a', 'b', 'c']
 * result['shape'].print(); // [2, 3]
 * ```
 * @param input: 1-D. Strings to split.
 * @param delimiter: 0-D. Delimiter characters, or empty string.
 * @param skipEmpty: Optional. If true, skip the empty strings from the result.
 *     Defaults to true.
 * @return A map with the following properties:
 *     - indices: A dense matrix of int32 representing the indices of the sparse
 *       tensor.
 *     - values: A vector of strings corresponding to the splited values.
 *     - shape: a length-2 vector of int32 representing the shape of the sparse
 * tensor, where the first value is N and the second value is the maximum number
 * of tokens in a single input entry.
 *
 * @doc {heading: 'Operations', subheading: 'String'}
 */
function stringSplit_(input, delimiter, skipEmpty = true) {
	const $input = convertToTensor(input, 'input', 'stringSplit', 'string');
	const $delimiter = convertToTensor(delimiter, 'delimiter', 'stringSplit', 'string');
	if ($input.rank !== 1) {
		throw new Error(`Input should be Tensor1D but received shape ${$input.shape}`);
	}
	if ($delimiter.rank !== 0) {
		throw new Error(`Delimiter should be a scalar but received shape ${$delimiter.shape}`);
	}
	const attrs = {skipEmpty};
	const inputs = {input: $input, delimiter: $delimiter};
	const result = ENGINE.runKernel(StringSplit, inputs, attrs);
	return {indices: result[0], values: result[1], shape: result[2]};
}

const stringSplit = /* @__PURE__ */ op({stringSplit_});

/**
 * @license
 * Copyright 2021 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Converts each string in the input Tensor to its hash mod by a number of
 * buckets.
 *
 * The hash function is deterministic on the content of the string within the
 * process and will never change. However, it is not suitable for cryptography.
 * This function may be used when CPU time is scarce and inputs are trusted or
 * unimportant. There is a risk of adversaries constructing inputs that all hash
 * to the same bucket.
 *
 * ```js
 * const result = tf.string.stringToHashBucketFast(
 *   ['Hello', 'TensorFlow', '2.x'], 3);
 * result.print(); // [0, 2, 2]
 * ```
 * @param input: The strings to assign a hash bucket.
 * @param numBuckets: The number of buckets.
 * @return A Tensor of the same shape as the input tensor.
 *
 * @doc {heading: 'Operations', subheading: 'String'}
 */
function stringToHashBucketFast_(input, numBuckets) {
	const $input = convertToTensor(input, 'input', 'stringToHashBucketFast', 'string');
	const attrs = {numBuckets};
	if (numBuckets <= 0) {
		throw new Error(`Number of buckets must be at least 1`);
	}
	const inputs = {input: $input};
	return ENGINE.runKernel(StringToHashBucketFast, inputs, attrs);
}

const stringToHashBucketFast$1 = /* @__PURE__ */ op({stringToHashBucketFast_});

/**
 * @license
 * Copyright 2023 Google LLC.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Replace the match of a `pattern` in `input` with `rewrite`.
 *
 * ```js
 * const result = tf.string.staticRegexReplace(
 *     ['format       this   spacing      better'], ' +', ' ');
 * result.print(); // ['format this spacing better']
 * ```
 * @param input: A Tensor of type string. The text to be processed.
 * @param pattern: A string. The regular expression to match the input.
 * @param rewrite: A string. The rewrite to be applied to the matched
 *     expression.
 * @param replaceGlobal: An optional bool. Defaults to True. If True, the
 *     replacement is global, otherwise the replacement is done only on the
 *     first match.
 * @return A Tensor of type string.
 *
 * @doc {heading: 'Operations', subheading: 'String'}
 */
function staticRegexReplace_(input, pattern, rewrite, replaceGlobal = true) {
	const $input = convertToTensor(input, 'input', 'staticRegexReplace', 'string');
	const attrs = {pattern, rewrite, replaceGlobal};
	return ENGINE.runKernel(StaticRegexReplace, {x: $input}, attrs);
}

const staticRegexReplace = /* @__PURE__ */ op({staticRegexReplace_});

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
// Modularized ops.
const spectral = {
	fft,
	ifft,
	rfft,
	irfft
};
const signal = {
	hammingWindow,
	hannWindow,
	frame,
	stft,
};
const image = {
	flipLeftRight,
	grayscaleToRGB,
	resizeNearestNeighbor,
	resizeBilinear,
	rgbToGrayscale,
	rotateWithOffset,
	cropAndResize,
	nonMaxSuppression,
	nonMaxSuppressionAsync,
	nonMaxSuppressionWithScore,
	nonMaxSuppressionWithScoreAsync,
	nonMaxSuppressionPadded,
	nonMaxSuppressionPaddedAsync,
	threshold,
	transform
};
const linalg = {
	bandPart,
	gramSchmidt,
	qr
};
const losses = {
	absoluteDifference,
	computeWeightedLoss,
	cosineDistance,
	hingeLoss,
	huberLoss,
	logLoss,
	meanSquaredError,
	sigmoidCrossEntropy,
	softmaxCrossEntropy
};
const sparse = {
	sparseFillEmptyRows,
	sparseReshape,
	sparseSegmentMean,
	sparseSegmentSum
};
// tslint:disable-next-line:variable-name
const string = {
	stringNGrams,
	stringSplit,
	stringToHashBucketFast: stringToHashBucketFast$1,
	staticRegexReplace,
};

/**
 * @license
 * Copyright 2018 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/** @doc {heading: 'Training', subheading: 'Classes', namespace: 'train'} */
class Optimizer extends Serializable {
	/**
	 * Executes `f()` and minimizes the scalar output of `f()` by computing
	 * gradients of y with respect to the list of trainable variables provided by
	 * `varList`. If no list is provided, it defaults to all trainable variables.
	 *
	 * @param f The function to execute and whose output to minimize.
	 * @param returnCost Whether to return the scalar cost value produced by
	 * executing `f()`.
	 * @param varList An optional list of variables to update. If specified, only
	 * the trainable variables in varList will be updated by minimize. Defaults to
	 * all trainable variables.
	 *
	 * @doc {heading: 'Training', subheading: 'Optimizers'}
	 */
	minimize(f, returnCost = false, varList) {
		const {value, grads} = this.computeGradients(f, varList);
		if (varList != null) {
			const gradArray = varList.map(v => ({name: v.name, tensor: grads[v.name]}));
			this.applyGradients(gradArray);
		} else {
			this.applyGradients(grads);
		}
		// Dispose gradients.
		dispose(grads);
		if (returnCost) {
			return value;
		} else {
			value.dispose();
			return null;
		}
	}

	/**
	 * The number of iterations that this optimizer instance has been invoked for.
	 */
	get iterations() {
		if (this.iterations_ == null) {
			this.iterations_ = 0;
		}
		return this.iterations_;
	}

	incrementIterations() {
		this.iterations_ = this.iterations + 1;
	}

	/**
	 * Executes f() and computes the gradient of the scalar output of f() with
	 * respect to the list of trainable variables provided by `varList`. If no
	 * list is provided, it defaults to all trainable variables.
	 *
	 * @param f The function to execute and whose output to use for computing
	 * gradients with respect to variables.
	 * @param varList An optional list of variables to compute gradients with
	 * respect to. If specified, only the trainable variables in varList will have
	 * gradients computed with respect to. Defaults to all trainable variables.
	 *
	 * @doc {heading: 'Training', subheading: 'Optimizers'}
	 */
	computeGradients(f, varList) {
		return variableGrads(f, varList);
	}

	/**
	 * Dispose the variables (if any) owned by this optimizer instance.
	 */
	dispose() {
		if (this.iterations_ != null) {
			dispose(this.iterations_);
		}
	}

	async saveIterations() {
		if (this.iterations_ == null) {
			this.iterations_ = 0;
		}
		return {
			name: 'iter',
			// TODO(cais): Use 'int64' type when available.
			tensor: scalar(this.iterations_, 'int32')
		};
	}

	async getWeights() {
		throw new Error('getWeights() is not implemented for this optimizer yet.');
	}

	async setWeights(weightValues) {
		throw new Error(`setWeights() is not implemented for this optimizer class ` +
			`${this.getClassName()}`);
	}

	/**
	 * Extract the first element of the weight values and set it
	 * as the iterations counter variable of this instance of optimizer.
	 *
	 * @param weightValues
	 * @returns Weight values with the first element consumed and excluded.
	 */
	async extractIterations(weightValues) {
		this.iterations_ = (await weightValues[0].tensor.data())[0];
		return weightValues.slice(1);
	}
}

Object.defineProperty(Optimizer, Symbol.hasInstance, {
	value: (instance) => {
		return instance.minimize != null && instance.computeGradients != null &&
			instance.applyGradients != null;
	}
});

/**
 * @license
 * Copyright 2018 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/** @doclink Optimizer */
class AdadeltaOptimizer extends Optimizer {
	/** @nocollapse */
	static get className() {
		// Name matters for Python compatibility.
		// This is a getter instead of a property because when it's a property, it
		// prevents the entire class from being tree-shaken.
		return 'Adadelta';
	}

	constructor(learningRate, rho, epsilon = null) {
		super();
		this.learningRate = learningRate;
		this.rho = rho;
		this.epsilon = epsilon;
		this.accumulatedGrads = [];
		this.accumulatedUpdates = [];
		if (epsilon == null) {
			this.epsilon = ENGINE.backend.epsilon();
		}
	}

	applyGradients(variableGradients) {
		const variableNames = Array.isArray(variableGradients) ?
			variableGradients.map(item => item.name) :
			Object.keys(variableGradients);
		variableNames.forEach((name, i) => {
			const value = ENGINE.registeredVariables[name];
			const trainable = false;
			if (this.accumulatedGrads[i] == null) {
				this.accumulatedGrads[i] = {
					originalName: `${name}/accum_grad`,
					variable: tidy(() => zerosLike$1(value).variable(trainable))
				};
			}
			if (this.accumulatedUpdates[i] == null) {
				this.accumulatedUpdates[i] = {
					originalName: `${name}/accum_var`,
					variable: tidy(() => zerosLike$1(value).variable(trainable))
				};
			}
			const gradient = Array.isArray(variableGradients) ?
				variableGradients[i].tensor :
				variableGradients[name];
			if (gradient == null) {
				return;
			}
			const accumulatedGrad = this.accumulatedGrads[i].variable;
			const accumulatedUpdate = this.accumulatedUpdates[i].variable;
			tidy(() => {
				const newAccumulatedGrad = add$1(mul(accumulatedGrad, this.rho), mul(square(gradient), 1 - this.rho));
				const updates = mul(div$1(sqrt$1(add$1(accumulatedUpdate, this.epsilon)), sqrt$1(add$1(accumulatedGrad, this.epsilon))), gradient);
				const newAccumulatedUpdate = add$1(mul(accumulatedUpdate, this.rho), mul(square(updates), 1 - this.rho));
				accumulatedGrad.assign(newAccumulatedGrad);
				accumulatedUpdate.assign(newAccumulatedUpdate);
				const newValue = add$1(mul(updates, -this.learningRate), value);
				value.assign(newValue);
			});
		});
		this.incrementIterations();
	}

	dispose() {
		if (this.accumulatedUpdates != null) {
			dispose(this.accumulatedGrads.map(v => v.variable));
			dispose(this.accumulatedUpdates.map(v => v.variable));
		}
	}

	async getWeights() {
		// Order matters for Python compatibility.
		const variables = [...this.accumulatedGrads, ...this.accumulatedUpdates];
		return [await this.saveIterations()].concat(variables.map(v => ({name: v.originalName, tensor: v.variable})));
	}

	async setWeights(weightValues) {
		weightValues = await this.extractIterations(weightValues);
		const variableCount = weightValues.length / 2;
		const trainable = false;
		this.accumulatedGrads =
			weightValues.slice(0, variableCount).map(v => ({
				originalName: v.name,
				variable: v.tensor.variable(trainable)
			}));
		this.accumulatedUpdates =
			weightValues.slice(variableCount, variableCount * 2)
						.map(v => ({
							originalName: v.name,
							variable: v.tensor.variable(trainable)
						}));
	}

	getConfig() {
		return {
			'learningRate': this.learningRate,
			'rho': this.rho,
			'epsilon': this.epsilon
		};
	}

	/** @nocollapse */
	static fromConfig(cls, config) {
		return new cls(config['learningRate'], config['rho'], config['epsilon']);
	}
}

/**
 * @license
 * Copyright 2018 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/** @doclink Optimizer */
class AdagradOptimizer extends Optimizer {
	/** @nocollapse */
	static get className() {
		// Name matters for Python compatibility.
		// This is a getter instead of a property because when it's a property, it
		// prevents the entire class from being tree-shaken.
		return 'Adagrad';
	}

	constructor(learningRate, initialAccumulatorValue = 0.1) {
		super();
		this.learningRate = learningRate;
		this.initialAccumulatorValue = initialAccumulatorValue;
		this.accumulatedGrads = [];
	}

	applyGradients(variableGradients) {
		const variableNames = Array.isArray(variableGradients) ?
			variableGradients.map(item => item.name) :
			Object.keys(variableGradients);
		variableNames.forEach((name, i) => {
			const value = ENGINE.registeredVariables[name];
			if (this.accumulatedGrads[i] == null) {
				const trainable = false;
				this.accumulatedGrads[i] = {
					originalName: `${name}/accumulator`,
					variable: tidy(() => fill$1(value.shape, this.initialAccumulatorValue)
						.variable(trainable))
				};
			}
			const gradient = Array.isArray(variableGradients) ?
				variableGradients[i].tensor :
				variableGradients[name];
			if (gradient == null) {
				return;
			}
			const accumulatedGrad = this.accumulatedGrads[i].variable;
			tidy(() => {
				const newAccumulatedGrad = add$1(accumulatedGrad, square(gradient));
				accumulatedGrad.assign(newAccumulatedGrad);
				const newValue = add$1(mul(div$1(gradient, sqrt$1(add$1(newAccumulatedGrad, ENGINE.backend.epsilon()))), -this.learningRate), value);
				value.assign(newValue);
			});
		});
		this.incrementIterations();
	}

	dispose() {
		if (this.accumulatedGrads != null) {
			dispose(this.accumulatedGrads.map(v => v.variable));
		}
	}

	async getWeights() {
		// Order matters for Python compatibility.
		return [await this.saveIterations()].concat(this.accumulatedGrads.map(v => ({name: v.originalName, tensor: v.variable})));
	}

	async setWeights(weightValues) {
		weightValues = await this.extractIterations(weightValues);
		const trainable = false;
		this.accumulatedGrads = weightValues.map(v => ({originalName: v.name, variable: v.tensor.variable(trainable)}));
	}

	getConfig() {
		return {
			'learningRate': this.learningRate,
			'initialAccumulatorValue': this.initialAccumulatorValue,
		};
	}

	/** @nocollapse */
	static fromConfig(cls, config) {
		return new cls(config['learningRate'], config['initialAccumulatorValue']);
	}
}

/**
 * @license
 * Copyright 2018 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
class AdamOptimizer extends Optimizer {
	/** @nocollapse */
	static get className() {
		// Name matters for Python compatibility.
		// This is a getter instead of a property because when it's a property, it
		// prevents the entire class from being tree-shaken.
		return 'Adam';
	}

	constructor(learningRate, beta1, beta2, epsilon = null) {
		super();
		this.learningRate = learningRate;
		this.beta1 = beta1;
		this.beta2 = beta2;
		this.epsilon = epsilon;
		this.accumulatedFirstMoment = [];
		this.accumulatedSecondMoment = [];
		tidy(() => {
			// accB* will be updated by batch.
			this.accBeta1 = scalar(beta1).variable();
			this.accBeta2 = scalar(beta2).variable();
		});
		if (epsilon == null) {
			this.epsilon = ENGINE.backend.epsilon();
		}
	}

	applyGradients(variableGradients) {
		const varNames = Array.isArray(variableGradients) ?
			variableGradients.map(v => v.name) :
			Object.keys(variableGradients);
		tidy(() => {
			const oneMinusAccBeta1 = sub$1(1, this.accBeta1);
			const oneMinusAccBeta2 = sub$1(1, this.accBeta2);
			varNames.forEach((name, i) => {
				const value = ENGINE.registeredVariables[name];
				const trainable = false;
				if (this.accumulatedFirstMoment[i] == null) {
					this.accumulatedFirstMoment[i] = {
						originalName: `${name}/m`,
						variable: tidy(() => zerosLike$1(value).variable(trainable))
					};
				}
				if (this.accumulatedSecondMoment[i] == null) {
					this.accumulatedSecondMoment[i] = {
						originalName: `${name}/v`,
						variable: tidy(() => zerosLike$1(value).variable(trainable))
					};
				}
				const gradient = Array.isArray(variableGradients) ?
					variableGradients[i].tensor :
					variableGradients[name];
				if (gradient == null) {
					return;
				}
				const firstMoment = this.accumulatedFirstMoment[i].variable;
				const secondMoment = this.accumulatedSecondMoment[i].variable;
				const newFirstMoment = add$1(mul(firstMoment, this.beta1), mul(gradient, 1 - this.beta1));
				const newSecondMoment = add$1(mul(secondMoment, this.beta2), mul(square(gradient), 1 - this.beta2));
				const biasCorrectedFirstMoment = div$1(newFirstMoment, oneMinusAccBeta1);
				const biasCorrectedSecondMoment = div$1(newSecondMoment, oneMinusAccBeta2);
				firstMoment.assign(newFirstMoment);
				secondMoment.assign(newSecondMoment);
				const newValue = add$1(mul(div$1(biasCorrectedFirstMoment, add$1(sqrt$1(biasCorrectedSecondMoment), this.epsilon)), -this.learningRate), value);
				value.assign(newValue);
			});
			this.accBeta1.assign(mul(this.accBeta1, this.beta1));
			this.accBeta2.assign(mul(this.accBeta2, this.beta2));
		});
		this.incrementIterations();
	}

	dispose() {
		this.accBeta1.dispose();
		this.accBeta2.dispose();
		if (this.accumulatedFirstMoment != null) {
			dispose(this.accumulatedFirstMoment.map(v => v.variable));
		}
		if (this.accumulatedSecondMoment != null) {
			dispose(this.accumulatedSecondMoment.map(v => v.variable));
		}
	}

	async getWeights() {
		// Order matters for Python compatibility.
		const variables = [...this.accumulatedFirstMoment, ...this.accumulatedSecondMoment];
		return [await this.saveIterations()].concat(variables.map(v => ({name: v.originalName, tensor: v.variable})));
	}

	async setWeights(weightValues) {
		weightValues = await this.extractIterations(weightValues);
		tidy(() => {
			this.accBeta1.assign(pow(this.beta1, this.iterations_ + 1));
			this.accBeta2.assign(pow(this.beta2, this.iterations_ + 1));
		});
		const variableCount = weightValues.length / 2;
		const trainable = false;
		this.accumulatedFirstMoment =
			weightValues.slice(0, variableCount).map(v => ({
				originalName: v.name,
				variable: v.tensor.variable(trainable)
			}));
		this.accumulatedSecondMoment =
			weightValues.slice(variableCount, variableCount * 2)
						.map(v => ({
							originalName: v.name,
							variable: v.tensor.variable(trainable)
						}));
	}

	getConfig() {
		return {
			'learningRate': this.learningRate,
			'beta1': this.beta1,
			'beta2': this.beta2,
			'epsilon': this.epsilon,
		};
	}

	/** @nocollapse */
	static fromConfig(cls, config) {
		return new cls(config['learningRate'], config['beta1'], config['beta2'], config['epsilon']);
	}
}

/**
 * @license
 * Copyright 2018 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
class AdamaxOptimizer extends Optimizer {
	/** @nocollapse */
	static get className() {
		// Name matters for Python compatibility.
		// This is a getter instead of a property because when it's a property, it
		// prevents the entire class from being tree-shaken.
		return 'Adamax';
	}

	constructor(learningRate, beta1, beta2, epsilon = null, decay = 0.0) {
		super();
		this.learningRate = learningRate;
		this.beta1 = beta1;
		this.beta2 = beta2;
		this.epsilon = epsilon;
		this.decay = decay;
		this.accumulatedFirstMoment = [];
		this.accumulatedWeightedInfNorm = [];
		tidy(() => {
			this.iteration = scalar(0).variable();
			this.accBeta1 = scalar(beta1).variable();
		});
		if (epsilon == null) {
			this.epsilon = ENGINE.backend.epsilon();
		}
	}

	applyGradients(variableGradients) {
		const variableNames = Array.isArray(variableGradients) ?
			variableGradients.map(item => item.name) :
			Object.keys(variableGradients);
		tidy(() => {
			const oneMinusAccBeta1 = sub$1(1, this.accBeta1);
			const lr = div$1(-this.learningRate, add$1(mul(this.iteration, this.decay), 1));
			variableNames.forEach((name, i) => {
				const value = ENGINE.registeredVariables[name];
				const trainable = false;
				if (this.accumulatedFirstMoment[i] == null) {
					this.accumulatedFirstMoment[i] = {
						originalName: `${name}/m`,
						variable: zerosLike$1(value).variable(trainable)
					};
				}
				if (this.accumulatedWeightedInfNorm[i] == null) {
					this.accumulatedWeightedInfNorm[i] = {
						originalName: `${name}/v`,
						variable: zerosLike$1(value).variable(trainable)
					};
				}
				const gradient = Array.isArray(variableGradients) ?
					variableGradients[i].tensor :
					variableGradients[name];
				if (gradient == null) {
					return;
				}
				const firstMoment = this.accumulatedFirstMoment[i].variable;
				const weightedInfNorm = this.accumulatedWeightedInfNorm[i].variable;
				const newFirstMoment = add$1(mul(firstMoment, this.beta1), mul(gradient, 1 - this.beta1));
				const ut0 = mul(weightedInfNorm, this.beta2);
				const ut1 = abs$1(gradient);
				const newWeightedInfNorm = maximum(ut0, ut1);
				firstMoment.assign(newFirstMoment);
				weightedInfNorm.assign(newWeightedInfNorm);
				const newValue = add$1(mul(div$1(lr, oneMinusAccBeta1), div$1(newFirstMoment, add$1(newWeightedInfNorm, this.epsilon))), value);
				value.assign(newValue);
			});
			this.iteration.assign(add$1(this.iteration, 1));
			this.accBeta1.assign(mul(this.accBeta1, this.beta1));
		});
		this.incrementIterations();
	}

	dispose() {
		this.accBeta1.dispose();
		this.iteration.dispose();
		if (this.accumulatedFirstMoment != null) {
			dispose(this.accumulatedFirstMoment.map(v => v.variable));
		}
		if (this.accumulatedWeightedInfNorm != null) {
			dispose(this.accumulatedWeightedInfNorm.map(v => v.variable));
		}
	}

	async getWeights() {
		throw new Error('getWeights() is not implemented for Adamax yet.');
	}

	async setWeights(weightValues) {
		throw new Error('setWeights() is not implemented for Adamax yet.');
	}

	getConfig() {
		return {
			'learningRate': this.learningRate,
			'beta1': this.beta1,
			'beta2': this.beta2,
			'epsilon': this.epsilon,
			'decay': this.decay
		};
	}

	/** @nocollapse */
	static fromConfig(cls, config) {
		return new cls(config['learningRate'], config['beta1'], config['beta2'], config['epsilon'], config['decay']);
	}
}

/**
 * @license
 * Copyright 2018 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/** @doclink Optimizer */
class SGDOptimizer extends Optimizer {
	/** @nocollapse */
	static get className() {
		// Name matters for Python compatibility.
		// This is a getter instead of a property because when it's a property, it
		// prevents the entire class from being tree-shaken.
		return 'SGD';
	}

	constructor(learningRate) {
		super();
		this.learningRate = learningRate;
		this.setLearningRate(learningRate);
	}

	applyGradients(variableGradients) {
		const varNames = Array.isArray(variableGradients) ?
			variableGradients.map(v => v.name) :
			Object.keys(variableGradients);
		varNames.forEach((name, i) => {
			const gradient = Array.isArray(variableGradients) ?
				variableGradients[i].tensor :
				variableGradients[name];
			if (gradient == null) {
				return;
			}
			const value = ENGINE.registeredVariables[name];
			tidy(() => {
				const newValue = add$1(mul(this.c, gradient), value);
				value.assign(newValue);
			});
		});
		this.incrementIterations();
	}

	/**
	 * Sets the learning rate of the optimizer.
	 */
	setLearningRate(learningRate) {
		this.learningRate = learningRate;
		if (this.c != null) {
			this.c.dispose();
		}
		this.c = keep(scalar(-learningRate));
	}

	dispose() {
		this.c.dispose();
	}

	async getWeights() {
		return [await this.saveIterations()];
	}

	async setWeights(weightValues) {
		weightValues = await this.extractIterations(weightValues);
		if (weightValues.length !== 0) {
			throw new Error('SGD optimizer does not have settable weights.');
		}
	}

	getConfig() {
		return {'learningRate': this.learningRate};
	}

	/** @nocollapse */
	static fromConfig(cls, config) {
		return new cls(config['learningRate']);
	}
}

/**
 * @license
 * Copyright 2018 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/** @doclink Optimizer */
class MomentumOptimizer extends SGDOptimizer {
	/** @nocollapse */
	// Name matters for Python compatibility.
	static get className() {
		// Name matters for Python compatibility.
		// This is a getter instead of a property because when it's a property, it
		// prevents the entire class from being tree-shaken.
		return 'Momentum';
	}

	constructor(learningRate, momentum, useNesterov = false) {
		super(learningRate);
		this.learningRate = learningRate;
		this.momentum = momentum;
		this.useNesterov = useNesterov;
		this.accumulations = [];
		this.m = scalar(this.momentum);
	}

	applyGradients(variableGradients) {
		const variableNames = Array.isArray(variableGradients) ?
			variableGradients.map(item => item.name) :
			Object.keys(variableGradients);
		variableNames.forEach((name, i) => {
			const value = ENGINE.registeredVariables[name];
			if (this.accumulations[i] == null) {
				const trainable = false;
				this.accumulations[i] = {
					originalName: `${name}/momentum`,
					variable: tidy(() => zerosLike$1(value).variable(trainable))
				};
			}
			const accumulation = this.accumulations[i].variable;
			const gradient = Array.isArray(variableGradients) ?
				variableGradients[i].tensor :
				variableGradients[name];
			if (gradient == null) {
				return;
			}
			tidy(() => {
				let newValue;
				const newAccumulation = add$1(mul(this.m, accumulation), gradient);
				if (this.useNesterov) {
					newValue = add$1(mul(this.c, add$1(gradient, mul(newAccumulation, this.m))), value);
				} else {
					newValue = add$1(mul(this.c, newAccumulation), value);
				}
				accumulation.assign(newAccumulation);
				value.assign(newValue);
			});
		});
		this.incrementIterations();
	}

	dispose() {
		this.m.dispose();
		if (this.accumulations != null) {
			dispose(this.accumulations.map(v => v.variable));
		}
	}

	/**
	 * Sets the momentum of the optimizer.
	 *
	 * @param momentum
	 */
	setMomentum(momentum) {
		this.momentum = momentum;
	}

	async getWeights() {
		// Order matters for Python compatibility.
		return [await this.saveIterations()].concat(this.accumulations.map(v => ({name: v.originalName, tensor: v.variable})));
	}

	async setWeights(weightValues) {
		weightValues = await this.extractIterations(weightValues);
		const trainable = false;
		this.accumulations = weightValues.map(v => ({originalName: v.name, variable: v.tensor.variable(trainable)}));
	}

	getConfig() {
		return {
			'learningRate': this.learningRate,
			'momentum': this.momentum,
			'useNesterov': this.useNesterov
		};
	}

	/** @nocollapse */
	static fromConfig(cls, config) {
		return new cls(config['learningRate'], config['momentum'], config['useNesterov']);
	}
}

/**
 * @license
 * Copyright 2018 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/** @doclink Optimizer */
class RMSPropOptimizer extends Optimizer {
	/** @nocollapse */
	static get className() {
		// Name matters for Python compatibility.
		// This is a getter instead of a property because when it's a property, it
		// prevents the entire class from being tree-shaken.
		return 'RMSProp';
	}

	constructor(learningRate, decay = 0.9, momentum = 0.0, epsilon = null, centered = false) {
		super();
		this.learningRate = learningRate;
		this.decay = decay;
		this.momentum = momentum;
		this.epsilon = epsilon;
		this.accumulatedMeanSquares = [];
		this.accumulatedMoments = [];
		this.accumulatedMeanGrads = [];
		this.centered = centered;
		if (epsilon == null) {
			this.epsilon = ENGINE.backend.epsilon();
		}
		if (learningRate == null) {
			throw new Error(`learningRate for RMSPropOptimizer must be defined.`);
		}
	}

	applyGradients(variableGradients) {
		const variableNames = Array.isArray(variableGradients) ?
			variableGradients.map(item => item.name) :
			Object.keys(variableGradients);
		variableNames.forEach((name, i) => {
			const value = ENGINE.registeredVariables[name];
			const trainable = false;
			if (this.accumulatedMeanSquares[i] == null) {
				this.accumulatedMeanSquares[i] = {
					originalName: `${name}/rms`,
					variable: tidy(() => zerosLike$1(value).variable(trainable))
				};
			}
			if (this.accumulatedMoments[i] == null) {
				this.accumulatedMoments[i] = {
					originalName: `${name}/momentum`,
					variable: tidy(() => zerosLike$1(value).variable(trainable))
				};
			}
			if (this.accumulatedMeanGrads[i] == null && this.centered) {
				this.accumulatedMeanGrads[i] = {
					originalName: `${name}/mg`,
					variable: tidy(() => zerosLike$1(value).variable(trainable))
				};
			}
			const gradient = Array.isArray(variableGradients) ?
				variableGradients[i].tensor :
				variableGradients[name];
			if (gradient == null) {
				return;
			}
			const accumulatedMeanSquare = this.accumulatedMeanSquares[i].variable;
			const accumulatedMoments = this.accumulatedMoments[i].variable;
			tidy(() => {
				const newAccumulatedMeanSquare = add$1(mul(accumulatedMeanSquare, this.decay), mul(square(gradient), 1 - this.decay));
				if (this.centered) {
					const accumulatedMeanGrad = this.accumulatedMeanGrads[i].variable;
					// Centered gradient
					const newAccumulatedMeanGrad = add$1(mul(accumulatedMeanGrad, this.decay), mul(gradient, 1 - this.decay));
					const gradContribution = div$1(mul(gradient, this.learningRate), sqrt$1(sub$1(newAccumulatedMeanSquare, add$1(square(newAccumulatedMeanGrad), this.epsilon))));
					const newAccumulatedMoments = add$1(mul(accumulatedMoments, this.momentum), gradContribution);
					accumulatedMeanSquare.assign(newAccumulatedMeanSquare);
					accumulatedMeanGrad.assign(newAccumulatedMeanGrad);
					accumulatedMoments.assign(newAccumulatedMoments);
					const newValue = sub$1(value, newAccumulatedMoments);
					value.assign(newValue);
				} else {
					// Plain gradient
					const newAccumulatedMeanSquare = add$1(mul(accumulatedMeanSquare, this.decay), mul(square(gradient), 1 - this.decay));
					const newAccumulatedMoments = add$1(mul(accumulatedMoments, this.momentum), div$1(mul(gradient, this.learningRate), sqrt$1(add$1(newAccumulatedMeanSquare, this.epsilon))));
					accumulatedMeanSquare.assign(newAccumulatedMeanSquare);
					accumulatedMoments.assign(newAccumulatedMoments);
					const newValue = sub$1(value, newAccumulatedMoments);
					value.assign(newValue);
				}
			});
		});
		this.incrementIterations();
	}

	dispose() {
		if (this.accumulatedMeanSquares != null) {
			dispose(this.accumulatedMeanSquares.map(v => v.variable));
		}
		if (this.accumulatedMeanGrads != null && this.centered) {
			dispose(this.accumulatedMeanGrads.map(v => v.variable));
		}
		if (this.accumulatedMoments != null) {
			dispose(this.accumulatedMoments.map(v => v.variable));
		}
	}

	async getWeights() {
		// Order matters for Python compatibility.
		const variables = [...this.accumulatedMeanSquares, ...this.accumulatedMoments];
		if (this.centered) {
			variables.push(...this.accumulatedMeanGrads);
		}
		return [await this.saveIterations()].concat(variables.map(v => ({name: v.originalName, tensor: v.variable})));
	}

	async setWeights(weightValues) {
		weightValues = await this.extractIterations(weightValues);
		const variableCount = this.centered ? weightValues.length / 3 : weightValues.length / 2;
		const trainable = false;
		this.accumulatedMeanSquares =
			weightValues.slice(0, variableCount).map(v => ({
				originalName: v.name,
				variable: v.tensor.variable(trainable)
			}));
		this.accumulatedMoments =
			weightValues.slice(variableCount, variableCount * 2)
						.map(v => ({
							originalName: v.name,
							variable: v.tensor.variable(trainable)
						}));
		if (this.centered) {
			this.accumulatedMeanGrads =
				weightValues.slice(variableCount * 2, variableCount * 3)
							.map(v => ({
								originalName: v.name,
								variable: v.tensor.variable(trainable)
							}));
		}
	}

	getConfig() {
		return {
			'learningRate': this.learningRate,
			'decay': this.decay,
			'momentum': this.momentum,
			'epsilon': this.epsilon,
			'centered': this.centered
		};
	}

	/** @nocollapse */
	static fromConfig(cls, config) {
		return new cls(config['learningRate'], config['decay'], config['momentum'], config['epsilon'], config['centered']);
	}
}

/**
 * @license
 * Copyright 2018 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
class OptimizerConstructors {
	/**
	 * Constructs a `tf.SGDOptimizer` that uses stochastic gradient descent.
	 *
	 * ```js
	 * // Fit a quadratic function by learning the coefficients a, b, c.
	 * const xs = tf.tensor1d([0, 1, 2, 3]);
	 * const ys = tf.tensor1d([1.1, 5.9, 16.8, 33.9]);
	 *
	 * const a = tf.scalar(Math.random()).variable();
	 * const b = tf.scalar(Math.random()).variable();
	 * const c = tf.scalar(Math.random()).variable();
	 *
	 * // y = a * x^2 + b * x + c.
	 * const f = x => a.mul(x.square()).add(b.mul(x)).add(c);
	 * const loss = (pred, label) => pred.sub(label).square().mean();
	 *
	 * const learningRate = 0.01;
	 * const optimizer = tf.train.sgd(learningRate);
	 *
	 * // Train the model.
	 * for (let i = 0; i < 10; i++) {
	 *   optimizer.minimize(() => loss(f(xs), ys));
	 * }
	 *
	 * // Make predictions.
	 * console.log(
	 *     `a: ${a.dataSync()}, b: ${b.dataSync()}, c: ${c.dataSync()}`);
	 * const preds = f(xs).dataSync();
	 * preds.forEach((pred, i) => {
	 *   console.log(`x: ${i}, pred: ${pred}`);
	 * });
	 * ```
	 *
	 * @param learningRate The learning rate to use for the SGD algorithm.
	 *
	 * @doc {heading: 'Training', subheading: 'Optimizers', namespace: 'train'}
	 */
	static sgd(learningRate) {
		return new SGDOptimizer(learningRate);
	}

	/**
	 * Constructs a `tf.MomentumOptimizer` that uses momentum gradient
	 * descent.
	 *
	 * See
	 * [http://proceedings.mlr.press/v28/sutskever13.pdf](
	 * http://proceedings.mlr.press/v28/sutskever13.pdf)
	 *
	 * @param learningRate The learning rate to use for the Momentum gradient
	 * descent algorithm.
	 * @param momentum The momentum to use for the momentum gradient descent
	 * algorithm.
	 *
	 * @doc {heading: 'Training', subheading: 'Optimizers', namespace: 'train'}
	 */
	static momentum(learningRate, momentum, useNesterov = false) {
		return new MomentumOptimizer(learningRate, momentum, useNesterov);
	}

	/**
	 * Constructs a `tf.RMSPropOptimizer` that uses RMSProp gradient
	 * descent. This implementation uses plain momentum and is not centered
	 * version of RMSProp.
	 *
	 * See
	 * [http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf](
	 * http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf)
	 *
	 * @param learningRate The learning rate to use for the RMSProp gradient
	 * descent algorithm.
	 * @param decay The discounting factor for the history/coming gradient.
	 * @param momentum The momentum to use for the RMSProp gradient descent
	 * algorithm.
	 * @param epsilon Small value to avoid zero denominator.
	 * @param centered If true, gradients are normalized by the estimated
	 * variance of the gradient.
	 *
	 * @doc {heading: 'Training', subheading: 'Optimizers', namespace: 'train'}
	 */
	static rmsprop(learningRate, decay = .9, momentum = 0.0, epsilon = null, centered = false) {
		return new RMSPropOptimizer(learningRate, decay, momentum, epsilon, centered);
	}

	/**
	 * Constructs a `tf.AdamOptimizer` that uses the Adam algorithm.
	 * See [https://arxiv.org/abs/1412.6980](https://arxiv.org/abs/1412.6980)
	 *
	 * @param learningRate The learning rate to use for the Adam gradient
	 * descent algorithm.
	 * @param beta1 The exponential decay rate for the 1st moment estimates.
	 * @param beta2 The exponential decay rate for the 2nd moment estimates.
	 * @param epsilon A small constant for numerical stability.
	 *
	 * @doc {heading: 'Training', subheading: 'Optimizers', namespace: 'train'}
	 */
	static adam(learningRate = 0.001, beta1 = 0.9, beta2 = 0.999, epsilon = null) {
		return new AdamOptimizer(learningRate, beta1, beta2, epsilon);
	}

	/**
	 * Constructs a `tf.AdadeltaOptimizer` that uses the Adadelta algorithm.
	 * See [https://arxiv.org/abs/1212.5701](https://arxiv.org/abs/1212.5701)
	 *
	 * @param learningRate The learning rate to use for the Adadelta gradient
	 * descent algorithm.
	 * @param rho The learning rate decay over each update.
	 * @param epsilon A constant epsilon used to better condition the grad
	 * update.
	 *
	 * @doc {heading: 'Training', subheading: 'Optimizers', namespace: 'train'}
	 */
	static adadelta(learningRate = .001, rho = .95, epsilon = null) {
		return new AdadeltaOptimizer(learningRate, rho, epsilon);
	}

	/**
	 * Constructs a `tf.AdamaxOptimizer` that uses the Adamax algorithm.
	 * See [https://arxiv.org/abs/1412.6980](https://arxiv.org/abs/1412.6980)
	 *
	 * @param learningRate The learning rate to use for the Adamax gradient
	 * descent algorithm.
	 * @param beta1 The exponential decay rate for the 1st moment estimates.
	 * @param beta2 The exponential decay rate for the 2nd moment estimates.
	 * @param epsilon A small constant for numerical stability.
	 * @param decay The learning rate decay over each update.
	 *
	 * @doc {heading: 'Training', subheading: 'Optimizers', namespace: 'train'}
	 */
	static adamax(learningRate = 0.002, beta1 = 0.9, beta2 = 0.999, epsilon = null, decay = 0.0) {
		return new AdamaxOptimizer(learningRate, beta1, beta2, epsilon, decay);
	}

	/**
	 * Constructs a `tf.AdagradOptimizer` that uses the Adagrad algorithm.
	 * See
	 * [http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf](
	 * http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf)
	 * or
	 * [http://ruder.io/optimizing-gradient-descent/index.html#adagrad](
	 * http://ruder.io/optimizing-gradient-descent/index.html#adagrad)
	 *
	 * @param learningRate The learning rate to use for the Adagrad gradient
	 * descent algorithm.
	 * @param initialAccumulatorValue Starting value for the accumulators, must be
	 * positive.
	 *
	 * @doc {heading: 'Training', subheading: 'Optimizers', namespace: 'train'}
	 */
	static adagrad(learningRate, initialAccumulatorValue = 0.1) {
		return new AdagradOptimizer(learningRate, initialAccumulatorValue);
	}
}

/**
 * @license
 * Copyright 2018 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
const train = OptimizerConstructors;

/**
 * @license
 * Copyright 2017 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
const delayCallback = (() => {
	if (typeof requestAnimationFrame !== 'undefined') {
		return requestAnimationFrame;
	} else if (typeof setImmediate !== 'undefined') {
		return setImmediate;
	}
	return (f) => f(); // no delays
})();

/**
 * Returns a promise that resolves when a requestAnimationFrame has completed.
 *
 * On Node.js this uses setImmediate instead of requestAnimationFrame.
 *
 * This is simply a sugar method so that users can do the following:
 * `await tf.nextFrame();`
 *
 * @doc {heading: 'Performance', subheading: 'Timing'}
 */
function nextFrame() {
	return new Promise(resolve => delayCallback(() => resolve()));
}

/**
 * @license
 * Copyright 2017 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
function assertParamsConsistent(shapes, axis) {
	const rank = shapes[0].length;
	shapes.forEach((shape, i) => {
		assert(shape.length === rank, () => `Error in concat${rank}D: rank of tensors[${i}] must be the same ` +
			`as the rank of the rest (${rank})`);
	});
	assert(axis >= 0 && axis < rank, () => `Error in concat${rank}D: axis must be between 0 and ${rank - 1}.`);
	const firstShape = shapes[0];
	shapes.forEach((shape, i) => {
		for (let r = 0; r < rank; r++) {
			assert((r === axis) || (shape[r] === firstShape[r]), () => `Error in concat${rank}D: Shape of tensors[${i}] (${shape}) ` +
				`does not match the shape of the rest (${firstShape}) ` +
				`along the non-concatenated axis ${i}.`);
		}
	});
}

function computeOutShape$1(shapes, axis) {
	const outputShape = shapes[0].slice();
	for (let i = 1; i < shapes.length; i++) {
		outputShape[axis] += shapes[i][axis];
	}
	return outputShape;
}

/**
 * @license
 * Copyright 2022 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
var RowPartitionType$1;
(function (RowPartitionType) {
	RowPartitionType[RowPartitionType["FIRST_DIM_SIZE"] = 0] = "FIRST_DIM_SIZE";
	RowPartitionType[RowPartitionType["VALUE_ROWIDS"] = 1] = "VALUE_ROWIDS";
	RowPartitionType[RowPartitionType["ROW_LENGTHS"] = 2] = "ROW_LENGTHS";
	RowPartitionType[RowPartitionType["ROW_SPLITS"] = 3] = "ROW_SPLITS";
	RowPartitionType[RowPartitionType["ROW_LIMITS"] = 4] = "ROW_LIMITS";
	RowPartitionType[RowPartitionType["ROW_STARTS"] = 5] = "ROW_STARTS";
})(RowPartitionType$1 || (RowPartitionType$1 = {}));

function combineRaggedTensorToTensorShapes(raggedRank, shape, valueShape) {
	// Test for consistency of valueShape and shape specified.
	// If shape is unspecified and valueShape is specified, then copy
	// over the size from the valueShape dimension.
	let outputShape = new Array();
	if (valueShape == null && shape == null) {
		return outputShape;
	}
	if (shape == null) {
		// Here, value_shape must be of known size.
		while (outputShape.length < raggedRank + valueShape.length) {
			outputShape.push(-1);
		}
	} else {
		outputShape = shape.slice();
	}
	if (valueShape == null) {
		return outputShape;
	}
	// At this point, valueShape and output_shape have known ranks.
	if (raggedRank + valueShape.length !== outputShape.length) {
		throw new Error(`rt input.shape and shape=${shape} are incompatible: rt input.rank = ${raggedRank +
		valueShape.length}, but shape.rank = ${outputShape.length}`);
	}
	for (let i = 1; i < valueShape.length; ++i) {
		const valueDim = valueShape[i];
		const outputShapeDimIndex = outputShape[outputShape.length - valueShape.length + i];
		const outputShapeDim = outputShape[outputShapeDimIndex];
		if (valueDim >= 0) {
			if (outputShapeDim >= 0) {
				if (outputShapeDim !== valueDim) {
					throw new Error(`rt input.shape and shape=${shape} are incompatible: rt input.shape[${i + raggedRank}] = ${valueDim} but shape[${i + raggedRank}] = ${outputShapeDim}`);
				}
			} else {
				outputShape[outputShapeDimIndex] = valueDim;
			}
		}
	}
	return outputShape;
}

function getRowPartitionTypesHelper(rowPartitionTypeStrings) {
	const stringToType = {
		'FIRST_DIM_SIZE': RowPartitionType$1.FIRST_DIM_SIZE,
		'VALUE_ROWIDS': RowPartitionType$1.VALUE_ROWIDS,
		'ROW_LENGTHS': RowPartitionType$1.ROW_LENGTHS,
		'ROW_SPLITS': RowPartitionType$1.ROW_SPLITS,
		'ROW_LIMITS': RowPartitionType$1.ROW_LIMITS,
		'ROW_STARTS': RowPartitionType$1.ROW_STARTS
	};
	const result = [];
	for (const typeStr of rowPartitionTypeStrings) {
		if (typeStr in stringToType) {
			result.push(stringToType[typeStr]);
		} else {
			break;
		}
	}
	return result;
}

function getRaggedRank(rowPartitionTypes) {
	if (rowPartitionTypes.length === 0) {
		return 0;
	}
	if (rowPartitionTypes[0] === RowPartitionType$1.FIRST_DIM_SIZE) {
		return rowPartitionTypes.length - 1;
	}
	return rowPartitionTypes.length;
}

function validateDefaultValueShape(defaultValueShape, valueShape) {
	if (defaultValueShape == null || valueShape == null) {
		return;
	}
	const defaultNDims = defaultValueShape.length;
	const valuesNDims = valueShape.length;
	if (defaultNDims >= valuesNDims) {
		throw new Error(`defaultValue.shape=${defaultValueShape} and ragged tensor flatValues.shape=${valueShape}, are incompatible: defaultValue.rank = ${defaultNDims} must be less than ragged tensor input flatValues.rank = ${valuesNDims})`);
	}
	for (let i = 0; i < Math.min(defaultNDims, valuesNDims - 1); ++i) {
		const defaultDim = defaultValueShape[i];
		const valueDim = valueShape[i + 1];
		if (defaultDim >= 0 && valueDim >= 0 && defaultDim !== 1 &&
			defaultDim !== valueDim) {
			throw new Error(`defaultValue.shape=${defaultValueShape}, and ragged tensor input flatValues.shape=${valueShape} are incompatible: defaultValue.shape[${i - defaultValueShape.length}] = ${defaultDim} but ragged tensor input.flatValues.shape[${i - defaultValueShape.length}] = ${valueDim}`);
		}
	}
}

/**
 * @license
 * Copyright 2017 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Inputs of size above this threshold will be parallelized by calling multiple
 * shader programs.
 */
const PARALLELIZE_THRESHOLD = 30;

function computeOptimalWindowSize(inSize) {
	if (inSize <= PARALLELIZE_THRESHOLD) {
		return inSize;
	}
	return nearestDivisor(inSize, Math.floor(Math.sqrt(inSize)));
}

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
// Returns the image center in pixels.
function getImageCenter(center, imageHeight, imageWidth) {
	const centerX = imageWidth * (typeof center === 'number' ? center : center[0]);
	const centerY = imageHeight * (typeof center === 'number' ? center : center[1]);
	return [centerX, centerY];
}

/**
 * @license
 * Copyright 2018 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Gets the new shape of the input Tensor after it's been reshaped
 * to:
 * [blockShape[0], ..., blockShape[M-1], batch / prod(blockShape),
 * inputShape[1], ..., inputShape[N-1]]
 *
 * See step 1: https://www.tensorflow.org/api_docs/python/tf/batch_to_space_nd
 */
function getReshaped(inputShape, blockShape, prod, batchToSpace = true) {
	let reshaped = [];
	if (batchToSpace) {
		reshaped = reshaped.concat(blockShape.slice(0));
		reshaped.push(inputShape[0] / prod);
		reshaped = reshaped.concat(inputShape.slice(1));
	} else {
		reshaped = reshaped.concat(inputShape[0]);
		const spatialLength = blockShape.length;
		for (let i = 0; i < spatialLength; ++i) {
			reshaped =
				reshaped.concat([inputShape[i + 1] / blockShape[i], blockShape[i]]);
		}
		reshaped = reshaped.concat(inputShape.slice(spatialLength + 1));
	}
	return reshaped;
}

/**
 * Gets the permutation that will transpose the dimensions of the
 * reshaped tensor to shape:
 *
 * [batch / prod(block_shape),inputShape[1], blockShape[0], ...,
 * inputShape[M], blockShape[M-1],inputShape[M+1], ..., inputShape[N-1]]
 *
 * see step 2: https://www.tensorflow.org/api_docs/python/tf/batch_to_space_nd
 */
function getPermuted(reshapedRank, blockShapeRank, batchToSpace = true) {
	const permuted = [];
	if (batchToSpace) {
		permuted.push(blockShapeRank);
		for (let i = blockShapeRank + 1; i < reshapedRank; ++i) {
			if (i <= 2 * blockShapeRank) {
				permuted.push(i);
				permuted.push(i - (blockShapeRank + 1));
			} else {
				permuted.push(i);
			}
		}
	} else {
		const permutedBeforeBatch = [];
		const permutedAfterBatch = [];
		for (let i = 1; i < reshapedRank; ++i) {
			if (i >= blockShapeRank * 2 + 1 || i % 2 === 1) {
				permutedAfterBatch.push(i);
			} else {
				permutedBeforeBatch.push(i);
			}
		}
		permuted.push(...permutedBeforeBatch);
		permuted.push(0);
		permuted.push(...permutedAfterBatch);
	}
	return permuted;
}

/**
 * Gets the shape of the reshaped and permuted input Tensor before any cropping
 * is applied.  The new shape will be:
 *
 * [batch / prod(blockShape),inputShape[1] * blockShape[0], ...,
 * inputShape[M] * blockShape[M-1],inputShape[M+1], ..., inputShape[N-1]]
 *
 * See step 3: https://www.tensorflow.org/api_docs/python/tf/batch_to_space_nd
 */
function getReshapedPermuted(inputShape, blockShape, prod, batchToSpace = true) {
	const reshapedPermuted = [];
	if (batchToSpace) {
		reshapedPermuted.push(inputShape[0] / prod);
	} else {
		reshapedPermuted.push(inputShape[0] * prod);
	}
	for (let i = 1; i < inputShape.length; ++i) {
		if (i <= blockShape.length) {
			if (batchToSpace) {
				reshapedPermuted.push(blockShape[i - 1] * inputShape[i]);
			} else {
				reshapedPermuted.push(inputShape[i] / blockShape[i - 1]);
			}
		} else {
			reshapedPermuted.push(inputShape[i]);
		}
	}
	return reshapedPermuted;
}

/**
 * Converts the crops argument into the beginning coordinates of a slice
 * operation.
 */
function getSliceBeginCoords(crops, blockShape) {
	const sliceBeginCoords = [0];
	for (let i = 0; i < blockShape; ++i) {
		sliceBeginCoords.push(crops[i][0]);
	}
	return sliceBeginCoords;
}

/**
 * Converts the crops argument into the size of a slice operation.  When
 * combined with getSliceBeginCoords this function allows the reshaped and
 * permuted Tensor to be cropped to its final output shape of:
 *
 * inputShape[1] * blockShape[0] - crops[0,0] - crops[0,1], ...,
 * inputShape[M] * blockShape[M-1] -crops[M-1,0] -
 * crops[M-1,1],inputShape[M+1], ..., inputShape[N-1]]
 *
 * See step 4: https://www.tensorflow.org/api_docs/python/tf/batch_to_space_nd
 */
function getSliceSize(uncroppedShape, crops, blockShape) {
	const sliceSize = uncroppedShape.slice(0, 1);
	for (let i = 0; i < blockShape; ++i) {
		sliceSize.push(uncroppedShape[i + 1] - crops[i][0] - crops[i][1]);
	}
	return sliceSize;
}

/**
 * @license
 * Copyright 2018 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
const SELU_SCALEALPHA = 1.7580993408473768599402175208123;
const SELU_SCALE = 1.0507009873554804934193349852946;

/**
 * @license
 * Copyright 2018 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
const ERF_P = 0.3275911;
const ERF_A1 = 0.254829592;
const ERF_A2 = -0.284496736;
const ERF_A3 = 1.421413741;
const ERF_A4 = -1.453152027;
const ERF_A5 = 1.061405429;

/**
 * @license
 * Copyright 2018 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Merges real and imaginary Float32Arrays into a single complex Float32Array.
 *
 * The memory layout is interleaved as follows:
 * real: [r0, r1, r2]
 * imag: [i0, i1, i2]
 * complex: [r0, i0, r1, i1, r2, i2]
 *
 * This is the inverse of splitRealAndImagArrays.
 *
 * @param real The real values of the complex tensor values.
 * @param imag The imag values of the complex tensor values.
 * @returns A complex tensor as a Float32Array with merged values.
 */
function mergeRealAndImagArrays(real, imag) {
	if (real.length !== imag.length) {
		throw new Error(`Cannot merge real and imag arrays of different lengths. real:` +
			`${real.length}, imag: ${imag.length}.`);
	}
	const result = new Float32Array(real.length * 2);
	for (let i = 0; i < result.length; i += 2) {
		result[i] = real[i / 2];
		result[i + 1] = imag[i / 2];
	}
	return result;
}

/**
 * Splits a complex Float32Array into real and imag parts.
 *
 * The memory layout is interleaved as follows:
 * complex: [r0, i0, r1, i1, r2, i2]
 * real: [r0, r1, r2]
 * imag: [i0, i1, i2]
 *
 * This is the inverse of mergeRealAndImagArrays.
 *
 * @param complex The complex tensor values.
 * @returns An object with real and imag Float32Array components of the complex
 *     tensor.
 */
function splitRealAndImagArrays(complex) {
	const real = new Float32Array(complex.length / 2);
	const imag = new Float32Array(complex.length / 2);
	for (let i = 0; i < complex.length; i += 2) {
		real[i / 2] = complex[i];
		imag[i / 2] = complex[i + 1];
	}
	return {real, imag};
}

/**
 * Extracts even indexed complex values in the given array.
 * @param complex The complex tensor values
 */
function complexWithEvenIndex(complex) {
	const len = Math.ceil(complex.length / 4);
	const real = new Float32Array(len);
	const imag = new Float32Array(len);
	for (let i = 0; i < complex.length; i += 4) {
		real[Math.floor(i / 4)] = complex[i];
		imag[Math.floor(i / 4)] = complex[i + 1];
	}
	return {real, imag};
}

/**
 * Extracts odd indexed complete values in the given array.
 * @param complex The complex tensor values
 */
function complexWithOddIndex(complex) {
	const len = Math.floor(complex.length / 4);
	const real = new Float32Array(len);
	const imag = new Float32Array(len);
	for (let i = 2; i < complex.length; i += 4) {
		real[Math.floor(i / 4)] = complex[i];
		imag[Math.floor(i / 4)] = complex[i + 1];
	}
	return {real, imag};
}

/**
 * Get the map representing a complex value in the given array.
 * @param complex The complex tensor values.
 * @param index An index of the target complex value.
 */
function getComplexWithIndex(complex, index) {
	const real = complex[index * 2];
	const imag = complex[index * 2 + 1];
	return {real, imag};
}

/**
 * Insert a given complex value into the TypedArray.
 * @param data The array in which the complex value is inserted.
 * @param c The complex value to be inserted.
 * @param index An index of the target complex value.
 */
function assignToTypedArray(data, real, imag, index) {
	data[index * 2] = real;
	data[index * 2 + 1] = imag;
}

/**
 * Make the list of exponent terms used by FFT.
 */
function exponents(n, inverse) {
	const real = new Float32Array(n / 2);
	const imag = new Float32Array(n / 2);
	for (let i = 0; i < Math.ceil(n / 2); i++) {
		const x = (inverse ? 2 : -2) * Math.PI * (i / n);
		real[i] = Math.cos(x);
		imag[i] = Math.sin(x);
	}
	return {real, imag};
}

/**
 * Make the exponent term used by FFT.
 */
function exponent(k, n, inverse) {
	const x = (inverse ? 2 : -2) * Math.PI * (k / n);
	const real = Math.cos(x);
	const imag = Math.sin(x);
	return {real, imag};
}

/**
 * @license
 * Copyright 2021 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
const ARROW = '->';
const ARROW_REGEX = /->/g;
const COMMA = ',';
const ELLIPSIS = '...';

/**
 * Parse an equation for einsum.
 *
 * @param equation The einsum equation (e.g., "ij,jk->ik").
 * @param numTensors Number of tensors provided along with `equation`. Used to
 *   check matching number of input tensors.
 * @returns An object consisting of the following fields:
 *   - allDims: all dimension names as strings.
 *   - summedDims: a list of all dimensions being summed over, as indices to
 *     the elements of `allDims`.
 *   - idDims: indices of the dimensions in each input tensor, as indices to
 *     the elements of `allDims.
 */
function decodeEinsumEquation(equation, numTensors) {
	equation = equation.replace(/\s/g, ''); // Remove witespace in equation.
	const numArrows = (equation.length - equation.replace(ARROW_REGEX, '').length) /
		ARROW.length;
	if (numArrows < 1) {
		throw new Error('Equations without an arrow are not supported.');
	} else if (numArrows > 1) {
		throw new Error(`Equation must contain exactly one arrow ("${ARROW}").`);
	}
	const [inputString, outputString] = equation.split(ARROW);
	assert(inputString.indexOf(ELLIPSIS) === -1, () => `The ellipsis notation ("${ELLIPSIS}") is not supported yet.`);
	const inputTerms = inputString.split(COMMA);
	const numInputs = inputTerms.length;
	if (numTensors !== numInputs) {
		throw new Error(`Expected ${numInputs} input tensors, received ${numTensors}`);
	}
	if (numInputs > 2) {
		throw new Error('Support for more than 2 input tensors is not implemented yet.');
	}
	const allDims = [];
	for (let i = 0; i < outputString.length; ++i) {
		const dimName = outputString[i];
		if (!inputTerms.some(inputTerm => inputTerm.indexOf(dimName) !== -1)) {
			throw new Error(`Output subscripts contain the label ${dimName} ` +
				`not present in the input subscripts.`);
		}
		if (allDims.indexOf(dimName) === -1) {
			allDims.push(dimName);
		}
	}
	for (let i = 0; i < inputString.length; ++i) {
		const dimName = inputString[i];
		if (allDims.indexOf(dimName) === -1 && dimName !== COMMA) {
			allDims.push(dimName);
		}
	}
	const idDims = new Array(inputTerms.length);
	for (let i = 0; i < numInputs; ++i) {
		if (new Set(inputTerms[i].split('')).size !== inputTerms[i].length) {
			throw new Error(`Found duplicate axes in input component ${inputTerms[i]}. ` +
				`Support for duplicate axes in input is not implemented yet.`);
		}
		idDims[i] = [];
		for (let j = 0; j < inputTerms[i].length; ++j) {
			idDims[i].push(allDims.indexOf(inputTerms[i][j]));
		}
	}
	const numDims = allDims.length; // Number of unique dimensions.
	const numOutDims = outputString.length; // Number of output dimensions.
	const summedDims = []; // Dimensions being summed over.
	for (let i = numOutDims; i < numDims; ++i) {
		summedDims.push(i);
	}
	return {allDims, summedDims, idDims};
}

/**
 * Get the permutation for a given input tensor.
 *
 * @param nDims Total number of dimension of all tensors involved in the einsum
 *   operation.
 * @param idDims Dimension indices involve in the tensor in question.
 * @returns An object consisting of the following fields:
 *   - permutationIndices: Indices to permute the axes of the tensor with.
 *   - expandDims: Indices to the dimension that need to be expanded from the
 *     tensor after permutation.
 */
function getEinsumPermutation(nDims, idDims) {
	let permutationIndices = new Array(nDims);
	permutationIndices.fill(-1);
	for (let i = 0; i < idDims.length; ++i) {
		permutationIndices[idDims[i]] = i;
	}
	const expandDims = [];
	for (let i = 0; i < nDims; ++i) {
		if (permutationIndices[i] === -1) {
			expandDims.push(i);
		}
	}
	permutationIndices = permutationIndices.filter(d => d !== -1);
	return {permutationIndices, expandDims};
}

/**
 * Checks that the dimension sizes from different input tensors match the
 * equation.
 */
function checkEinsumDimSizes(nDims, idDims, tensors) {
	const dimSizes = new Array(nDims);
	for (let i = 0; i < tensors.length; ++i) {
		const shape = tensors[i].shape;
		for (let j = 0; j < idDims[i].length; ++j) {
			if (dimSizes[idDims[i][j]] === undefined) {
				dimSizes[idDims[i][j]] = shape[j];
			} else {
				assert(dimSizes[idDims[i][j]] === shape[j], () => `Expected dimension ${dimSizes[idDims[i][j]]} at axis ${j} ` +
					`of input shaped ${JSON.stringify(shape)}, ` +
					`but got dimension ${shape[j]}`);
			}
		}
	}
}

/**
 * Gets path of computation for einsum.
 *
 * @param summedDims indices to the dimensions being summed over.
 * @param idDims A look up table for the dimensions present in each input
 *     tensor.Each constituent array contains indices for the dimensions in the
 *     corresponding input tensor.
 *
 * @return A map with two fields:
 *   - path: The path of computation, with each element indicating the dimension
 *     being summed over after the element-wise multiplication in that step.
 *   - steps: With the same length as `path`. Each element contains the indices
 *     to the input tensors being used for element-wise multiplication in the
 *     corresponding step.
 */
function getEinsumComputePath(summedDims, idDims) {
	const path = summedDims;
	const steps = [];
	let nSteps = 0;
	if (summedDims.length === 0) {
		// Einsum that involes no summing: e.g., transpose and outer product.
		path.push(-1);
	}
	nSteps = summedDims.length + 1;
	for (let i = 0; i < nSteps; ++i) {
		steps.push([]);
	}
	const computedTermIndices = [];
	for (let i = 0; i < path.length; ++i) {
		const summedDim = path[i];
		const termIndices = findTermsWithDim(idDims, summedDim);
		for (const termIndex of termIndices) {
			if (computedTermIndices.indexOf(termIndex) === -1) {
				steps[i].push(termIndex);
				computedTermIndices.push(termIndex);
			}
		}
	}
	return {path, steps};
}

/** Determines if an axes permutation is the identity permutation. */
function isIdentityPermutation(perm) {
	return perm.every((dim, index) => dim === index);
}

function findTermsWithDim(idDims, dim) {
	const termIndices = [];
	for (let i = 0; i < idDims.length; ++i) {
		if (idDims[i].length === 0 || idDims[i].indexOf(dim) !== -1 || dim === -1) {
			termIndices.push(i);
		}
	}
	return termIndices;
}

/**
 * Prepare the split size array. When the input is a number, the axis is evenly
 * divided among the split size. When the input contains the negative value, the
 * rest of the axis is allocated toward that.
 */
function prepareSplitSize(x, numOrSizeSplits, axis = 0) {
	let splitSizes = [];
	if (typeof (numOrSizeSplits) === 'number') {
		assert(x.shape[axis] % numOrSizeSplits === 0, () => 'Number of splits must evenly divide the axis.');
		splitSizes =
			new Array(numOrSizeSplits).fill(x.shape[axis] / numOrSizeSplits);
	} else {
		const numOfNegs = numOrSizeSplits.reduce((count, value) => {
			if (value === -1) {
				count += 1;
			}
			return count;
		}, 0);
		assert(numOfNegs <= 1, () => 'There should be only one negative value in split array.');
		const negIndex = numOrSizeSplits.indexOf(-1);
		// Allow the number of split array to be -1, which indicates the rest
		// of dimension is allocated to that split.
		if (negIndex !== -1) {
			const total = numOrSizeSplits.reduce((a, b) => b > 0 ? a + b : a);
			numOrSizeSplits[negIndex] = x.shape[axis] - total;
		}
		assert(x.shape[axis] === numOrSizeSplits.reduce((a, b) => a + b), () => 'The sum of sizes must match the size of the axis dimension.');
		splitSizes = numOrSizeSplits;
	}
	return splitSizes;
}

/**
 * @license
 * Copyright 2021 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Generates sparse fill empty rows indices, dense shape mismatch error message.
 *
 * @param indicesLength The first dimension of indices.
 */
function getSparseFillEmptyRowsIndicesDenseShapeMismatch(indicesLength) {
	return `Received SparseTensor with denseShape[0] = 0 but
  indices.shape[0] = ${indicesLength}`;
}

/**
 * Generates sparse fill empty rows negative index error message.
 *
 * @param index The index with a negative value.
 * @param value The negative value.
 */
function getSparseFillEmptyRowsNegativeIndexErrorMessage(index, value) {
	return `indices(${index}, 0) is invalid: ${value} < 0`;
}

/**
 * Generates sparse fill empty rows out of range index error message.
 *
 * @param index The index with an out of range value.
 * @param value The out of range value.
 * @param limit The upper limit for indices.
 */
function getSparseFillEmptyRowsOutOfRangeIndexErrorMessage(index, value, limit) {
	return `indices(${index}, 0) is invalid: ${value} >= ${limit}`;
}

/**
 * @license
 * Copyright 2021 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Generates sparse reshape multiple negative 1 output dimension error message.
 *
 * @param dim1 The first dimension with a negative 1 value.
 * @param dim2 The second dimension with a negative 1 value.
 */
function getSparseReshapeMultipleNegativeOneOutputDimErrorMessage(dim1, dim2) {
	return `only one output dimension may be -1, not both ${dim1} and ${dim2}`;
}

/**
 * Generates sparse reshape negative output dimension error message.
 *
 * @param dim The dimension with a negative value.
 * @param value The negative value.
 */
function getSparseReshapeNegativeOutputDimErrorMessage(dim, value) {
	return `size ${dim} must be non-negative, not ${value}`;
}

/**
 * Generates sparse reshape empty tensor zero output dimension error message.
 *
 */
function getSparseReshapeEmptyTensorZeroOutputDimErrorMessage() {
	return 'reshape cannot infer the missing input size for an empty tensor ' +
		'unless all specified input sizes are non-zero';
}

/**
 * Generates sparse reshape input output multiple mismatch error message.
 *
 * @param inputShape the input shape.
 * @param outputShape the requested output shape.
 */
function getSparseReshapeInputOutputMultipleErrorMessage(inputShape, outputShape) {
	const inputSize = sizeFromShape(inputShape);
	const outputSize = sizeFromShape(outputShape);
	return `Input to reshape is a SparseTensor with ${inputSize}
  dense values, but the requested shape requires a multiple of ${outputSize}. inputShape=${inputShape} outputShape= ${outputShape}`;
}

/**
 * Generates sparse reshape input output inequality error message.
 *
 * @param inputShape the input shape.
 * @param outputShape the requested output shape.
 */
function getSparseReshapeInputOutputMismatchErrorMessage(inputShape, outputShape) {
	const inputSize = sizeFromShape(inputShape);
	const outputSize = sizeFromShape(outputShape);
	return `Input to reshape is a tensor with ${inputSize} dense values, but the requested shape has ${outputSize}. inputShape=${inputShape} outputShape=${outputShape}`;
}

/**
 * @license
 * Copyright 2021 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Generates sparse segment reduction negative segment ids error message.
 *
 */
function getSparseSegmentReductionNegativeSegmentIdsErrorMessage() {
	return `segment ids must be >= 0`;
}

/**
 * Generates sparse segment reduction non increasing segment ids error message.
 *
 */
function getSparseSegmentReductionNonIncreasingSegmentIdsErrorMessage() {
	return `segment ids are not increasing`;
}

/**
 * Generates sparse segment reduction segment id out of range error message.
 *
 * @param segmentId The segment id index that is out of range.
 * @param outputRows Upper bound of valid segment id values.
 */
function getSparseSegmentReductionSegmentIdOutOfRangeErrorMessage(segmentId, outputRows) {
	return `Segment id ${segmentId} out of range [0, ${outputRows}), possibly because segmentIds input is not sorted.`;
}

/**
 * Generates sparse segment reduction input indice out of range error message.
 *
 * @param index The index that holds the out of range value.
 * @param indexValue The value that is out of range.
 * @param inputRows Upper bound of valid index values.
 */
function getSparseSegmentReductionIndicesOutOfRangeErrorMessage(index, indexValue, inputRows) {
	return `Bad: indices[${index}] == ${indexValue} out of range [0, ${inputRows})`;
}

/**
 * @license
 * Copyright 2018 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
function segOpComputeOptimalWindowSize(inSize, numSegments) {
	let done = false;
	let res;
	if (inSize <= PARALLELIZE_THRESHOLD) {
		res = inSize;
		done = true;
	} else {
		res = nearestDivisor(inSize, Math.floor(Math.sqrt(inSize)));
	}
	while (!done) {
		if (res > numSegments || res === inSize) {
			done = true;
		} else {
			res = nearestDivisor(inSize, res + 1);
		}
	}
	return res;
}

function computeOutShape(aShape, axis, numSegments) {
	const outShape = [];
	const rank = aShape.length;
	for (let dim = 0; dim < rank; dim++) {
		if (dim !== axis) {
			outShape.push(aShape[dim]);
		} else {
			outShape.push(numSegments);
		}
	}
	return outShape;
}

function collectGatherOpShapeInfo(x, indices, axis, batchDims) {
	const indicesRank = indices.shape.length;
	const xRank = x.shape.length;
	if (batchDims !== 0) {
		if (batchDims < -indicesRank || batchDims > indicesRank) {
			throw new Error(`Expect batchDims in the range of [-${indicesRank}, ${indicesRank}], but got ${batchDims}`);
		}
	}
	if (batchDims < 0) {
		batchDims += indicesRank;
	}
	if (batchDims > xRank) {
		throw new Error(`batchDims (${batchDims}) must be less than rank(x) (
    ${xRank}).`);
	}
	if (axis < batchDims) {
		throw new Error(`batchDims (${batchDims}) must be less than or equal to axis (${axis}).`);
	}
	for (let i = 0; i < batchDims; ++i) {
		if (x.shape[i] !== indices.shape[i]) {
			throw new Error(`x.shape[${i}]: ${x.shape[i]} should be equal to indices.shape[${i}]: ${indices.shape[i]}.`);
		}
	}
	const dimSize = x.shape[axis];
	const outputShape = [];
	let batchSize = 1;
	let outerSize = 1;
	let sliceSize = 1;
	for (let i = 0; i < batchDims; ++i) {
		outputShape.push(x.shape[i]);
		batchSize *= x.shape[i];
	}
	for (let i = batchDims; i < axis; i++) {
		outputShape.push(x.shape[i]);
		outerSize *= x.shape[i];
	}
	for (let i = batchDims; i < indicesRank; i++) {
		outputShape.push(indices.shape[i]);
	}
	for (let i = axis + 1; i < xRank; i++) {
		outputShape.push(x.shape[i]);
		sliceSize *= x.shape[i];
	}
	return {batchSize, sliceSize, outerSize, dimSize, outputShape};
}

var segment_util = /*#__PURE__*/Object.freeze({
	__proto__: null,
	collectGatherOpShapeInfo: collectGatherOpShapeInfo,
	computeOutShape: computeOutShape,
	segOpComputeOptimalWindowSize: segOpComputeOptimalWindowSize
});

/**
 * @license
 * Copyright 2018 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
function fromUint8ToStringArray(vals) {
	try {
		// Decode the bytes into string.
		return vals.map(val => decodeString(val));
	} catch (err) {
		throw new Error(`Failed to decode encoded string bytes into utf-8, error: ${err}`);
	}
}

function fromStringArrayToUint8(strings) {
	return strings.map(s => encodeString(s));
}

var backend_util = /*#__PURE__*/Object.freeze({
	__proto__: null,
	ERF_A1: ERF_A1,
	ERF_A2: ERF_A2,
	ERF_A3: ERF_A3,
	ERF_A4: ERF_A4,
	ERF_A5: ERF_A5,
	ERF_P: ERF_P,
	PARALLELIZE_THRESHOLD: PARALLELIZE_THRESHOLD,
	get RowPartitionType() {
		return RowPartitionType$1;
	},
	SELU_SCALE: SELU_SCALE,
	SELU_SCALEALPHA: SELU_SCALEALPHA,
	applyActivation: applyActivation$1,
	assertAndGetBroadcastShape: assertAndGetBroadcastShape,
	assertAxesAreInnerMostDims: assertAxesAreInnerMostDims,
	assertParamsConsistent: assertParamsConsistent,
	assignToTypedArray: assignToTypedArray,
	axesAreInnerMostDims: axesAreInnerMostDims,
	calculateShapes: calculateShapes,
	checkEinsumDimSizes: checkEinsumDimSizes,
	checkPadOnDimRoundingMode: checkPadOnDimRoundingMode,
	combineLocations: combineLocations,
	combineRaggedTensorToTensorShapes: combineRaggedTensorToTensorShapes,
	complexWithEvenIndex: complexWithEvenIndex,
	complexWithOddIndex: complexWithOddIndex,
	computeConv2DInfo: computeConv2DInfo,
	computeConv3DInfo: computeConv3DInfo,
	computeDefaultPad: computeDefaultPad,
	computeDilation2DInfo: computeDilation2DInfo,
	computeOptimalWindowSize: computeOptimalWindowSize,
	computeOutAndReduceShapes: computeOutAndReduceShapes,
	computeOutShape: computeOutShape$1,
	computePool2DInfo: computePool2DInfo,
	computePool3DInfo: computePool3DInfo,
	convertConv2DDataFormat: convertConv2DDataFormat,
	decodeEinsumEquation: decodeEinsumEquation,
	eitherStridesOrDilationsAreOne: eitherStridesOrDilationsAreOne,
	expandShapeToKeepDim: expandShapeToKeepDim,
	exponent: exponent,
	exponents: exponents,
	fromStringArrayToUint8: fromStringArrayToUint8,
	fromUint8ToStringArray: fromUint8ToStringArray,
	getAxesPermutation: getAxesPermutation,
	getBroadcastDims: getBroadcastDims,
	getComplexWithIndex: getComplexWithIndex,
	getEinsumComputePath: getEinsumComputePath,
	getEinsumPermutation: getEinsumPermutation,
	getFusedBiasGradient: getFusedBiasGradient,
	getFusedDyActivation: getFusedDyActivation,
	getImageCenter: getImageCenter,
	getInnerMostAxes: getInnerMostAxes,
	getPermuted: getPermuted,
	getRaggedRank: getRaggedRank,
	getReductionAxes: getReductionAxes,
	getReshaped: getReshaped,
	getReshapedPermuted: getReshapedPermuted,
	getRowPartitionTypesHelper: getRowPartitionTypesHelper,
	getSliceBeginCoords: getSliceBeginCoords,
	getSliceSize: getSliceSize,
	getSparseFillEmptyRowsIndicesDenseShapeMismatch: getSparseFillEmptyRowsIndicesDenseShapeMismatch,
	getSparseFillEmptyRowsNegativeIndexErrorMessage: getSparseFillEmptyRowsNegativeIndexErrorMessage,
	getSparseFillEmptyRowsOutOfRangeIndexErrorMessage: getSparseFillEmptyRowsOutOfRangeIndexErrorMessage,
	getSparseReshapeEmptyTensorZeroOutputDimErrorMessage: getSparseReshapeEmptyTensorZeroOutputDimErrorMessage,
	getSparseReshapeInputOutputMismatchErrorMessage: getSparseReshapeInputOutputMismatchErrorMessage,
	getSparseReshapeInputOutputMultipleErrorMessage: getSparseReshapeInputOutputMultipleErrorMessage,
	getSparseReshapeMultipleNegativeOneOutputDimErrorMessage: getSparseReshapeMultipleNegativeOneOutputDimErrorMessage,
	getSparseReshapeNegativeOutputDimErrorMessage: getSparseReshapeNegativeOutputDimErrorMessage,
	getSparseSegmentReductionIndicesOutOfRangeErrorMessage: getSparseSegmentReductionIndicesOutOfRangeErrorMessage,
	getSparseSegmentReductionNegativeSegmentIdsErrorMessage: getSparseSegmentReductionNegativeSegmentIdsErrorMessage,
	getSparseSegmentReductionNonIncreasingSegmentIdsErrorMessage: getSparseSegmentReductionNonIncreasingSegmentIdsErrorMessage,
	getSparseSegmentReductionSegmentIdOutOfRangeErrorMessage: getSparseSegmentReductionSegmentIdOutOfRangeErrorMessage,
	getUndoAxesPermutation: getUndoAxesPermutation,
	isIdentityPermutation: isIdentityPermutation,
	log: log$2,
	mergeRealAndImagArrays: mergeRealAndImagArrays,
	prepareAndValidate: prepareAndValidate,
	prepareSplitSize: prepareSplitSize,
	segment_util: segment_util,
	shouldFuse: shouldFuse,
	slice_util: slice_util,
	splitRealAndImagArrays: splitRealAndImagArrays,
	stridesOrDilationsArePositive: stridesOrDilationsArePositive,
	tupleValuesAreOne: tupleValuesAreOne,
	upcastType: upcastType,
	validateDefaultValueShape: validateDefaultValueShape,
	validateInput: validateInput$1,
	validateUpdateShape: validateUpdateShape,
	warn: warn
});

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */

var kernel_impls = /*#__PURE__*/Object.freeze({
	__proto__: null,
	nonMaxSuppressionV3Impl: nonMaxSuppressionV3Impl,
	nonMaxSuppressionV4Impl: nonMaxSuppressionV4Impl,
	nonMaxSuppressionV5Impl: nonMaxSuppressionV5Impl,
	whereImpl: whereImpl$1
});

/**
 * @license
 * Copyright 2019 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
class PlatformBrowser {
	constructor() {
		// For setTimeoutCustom
		this.messageName = 'setTimeoutCustom';
		this.functionRefs = [];
		this.handledMessageCount = 0;
		this.hasEventListener = false;
	}

	fetch(path, init) {
		return fetch(path, init);
	}

	now() {
		return performance.now();
	}

	encode(text, encoding) {
		if (encoding !== 'utf-8' && encoding !== 'utf8') {
			throw new Error(`Browser's encoder only supports utf-8, but got ${encoding}`);
		}
		if (this.textEncoder == null) {
			this.textEncoder = new TextEncoder();
		}
		return this.textEncoder.encode(text);
	}

	decode(bytes, encoding) {
		return new TextDecoder(encoding).decode(bytes);
	}

	// If the setTimeout nesting level is greater than 5 and timeout is less
	// than 4ms, timeout will be clamped to 4ms, which hurts the perf.
	// Interleaving window.postMessage and setTimeout will trick the browser and
	// avoid the clamp.
	setTimeoutCustom(functionRef, delay) {
		if (typeof window === 'undefined' ||
			!env().getBool('USE_SETTIMEOUTCUSTOM')) {
			setTimeout(functionRef, delay);
			return;
		}
		this.functionRefs.push(functionRef);
		setTimeout(() => {
			window.postMessage({name: this.messageName, index: this.functionRefs.length - 1}, '*');
		}, delay);
		if (!this.hasEventListener) {
			this.hasEventListener = true;
			window.addEventListener('message', (event) => {
				if (event.source === window && event.data.name === this.messageName) {
					event.stopPropagation();
					const functionRef = this.functionRefs[event.data.index];
					functionRef();
					this.handledMessageCount++;
					if (this.handledMessageCount === this.functionRefs.length) {
						this.functionRefs = [];
						this.handledMessageCount = 0;
					}
				}
			}, true);
		}
	}

	isTypedArray(a) {
		return isTypedArrayBrowser(a);
	}
}

if (env().get('IS_BROWSER')) {
	env().setPlatform('browser', new PlatformBrowser());
	// Register LocalStorage IOHandler
	try {
		ModelStoreManagerRegistry.registerManager(BrowserLocalStorage.URL_SCHEME, new BrowserLocalStorageManager());
	} catch (err) {
	}
	// Register IndexedDB IOHandler
	try {
		ModelStoreManagerRegistry.registerManager(BrowserIndexedDB.URL_SCHEME, new BrowserIndexedDBManager());
	} catch (err) {
	}
}

/**
 * @license
 * Copyright 2019 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
// We are wrapping this within an object so it can be stubbed by Jasmine.
const getNodeFetch = {
	// tslint:disable-next-line:no-require-imports
	importFetch: () => require('node-fetch')
};
let systemFetch;

class PlatformNode {
	constructor() {
		// tslint:disable-next-line:no-require-imports
		this.util = require('util');
		// According to the spec, the built-in encoder can do only UTF-8 encoding.
		// https://developer.mozilla.org/en-US/docs/Web/API/TextEncoder/TextEncoder
		this.textEncoder = new this.util.TextEncoder();
	}

	fetch(path, requestInits) {
		if (env().global.fetch != null) {
			return env().global.fetch(path, requestInits);
		}
		if (systemFetch == null) {
			systemFetch = getNodeFetch.importFetch();
		}
		return systemFetch(path, requestInits);
	}

	now() {
		const time = process.hrtime();
		return time[0] * 1000 + time[1] / 1000000;
	}

	encode(text, encoding) {
		if (encoding !== 'utf-8' && encoding !== 'utf8') {
			throw new Error(`Node built-in encoder only supports utf-8, but got ${encoding}`);
		}
		return this.textEncoder.encode(text);
	}

	decode(bytes, encoding) {
		if (bytes.length === 0) {
			return '';
		}
		return new this.util.TextDecoder(encoding).decode(bytes);
	}

	isTypedArray(a) {
		return this.util.types.isFloat32Array(a)
			|| this.util.types.isInt32Array(a)
			|| this.util.types.isUint8Array(a)
			|| this.util.types.isUint8ClampedArray(a);
	}
}

if (env().get('IS_NODE') && !env().get('IS_BROWSER')) {
	env().setPlatform('node', new PlatformNode());
}

/**
 * @license
 * Copyright 2020 Google Inc. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
// Required side effectful code for tfjs-core
// Set up Engine and ENV
getOrMakeEngine();
const opHandler = {
	buffer,
	cast: cast$1,
	clone,
	print
};
setOpHandler(opHandler);

/**
 * @license
 * Copyright 2022 Google LLC.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
const OPTIMIZERS = [
	AdadeltaOptimizer,
	AdagradOptimizer,
	AdamOptimizer,
	AdamaxOptimizer,
	MomentumOptimizer,
	RMSPropOptimizer,
	SGDOptimizer,
];

function registerOptimizers() {
	for (const optimizer of OPTIMIZERS) {
		registerClass(optimizer);
	}
}

/**
 * @license
 * Copyright 2017 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
// Required side effectful code.
registerOptimizers();

/**
 * @license
 * Copyright 2019 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
function assertNotComplex(tensor, opName) {
	if (!Array.isArray(tensor)) {
		tensor = [tensor];
	}
	tensor.forEach(t => {
		if (t != null) {
			assert(t.dtype !== 'complex64', () => `${opName} does not support complex64 tensors in the CPU backend.`);
		}
	});
}

/**
 * @license
 * Copyright 2021 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
const whereImpl = whereImpl$1;

class MathBackendCPU extends KernelBackend {
	nextDataId() {
		return MathBackendCPU.nextDataId++;
	}

	constructor() {
		super();
		this.blockSize = 48;
		this.firstUse = true;
		this.data = new DataStorage(this, engine());
	}

	write(values, shape, dtype) {
		if (this.firstUse) {
			this.firstUse = false;
			if (env().get('IS_NODE')) {
				warn('\n============================\n' +
					'Hi, looks like you are running TensorFlow.js in ' +
					'Node.js. To speed things up dramatically, install our node ' +
					'backend, visit https://github.com/tensorflow/tfjs-node for more details. ' +
					'\n============================');
			}
		}
		const dataId = {id: this.nextDataId()};
		this.data.set(dataId, {values, dtype, refCount: 1});
		return dataId;
	}

	/**
	 * Create a data bucket in cpu backend.
	 * @param shape Shape of the `TensorInfo`.
	 * @param dtype DType of the `TensorInfo`.
	 * @param values The value of the `TensorInfo` stored as a flattened array.
	 */
	makeTensorInfo(shape, dtype, values) {
		let outId;
		if (dtype === 'string' && values != null && values.length > 0 &&
			isString(values[0])) {
			const encodedValues = values.map(d => encodeString(d));
			outId = this.write(encodedValues, shape, dtype);
		} else {
			outId = this.write(values, shape, dtype);
		}
		return {dataId: outId, shape, dtype};
	}

	/** Return refCount of a `TensorData`. */
	refCount(dataId) {
		if (this.data.has(dataId)) {
			const tensorData = this.data.get(dataId);
			return tensorData.refCount;
		}
		return 0;
	}

	/** Increase refCount of a `TensorData`. */
	incRef(dataId) {
		const tensorData = this.data.get(dataId);
		tensorData.refCount++;
	}

	/** Decrease refCount of a `TensorData`. */
	decRef(dataId) {
		if (this.data.has(dataId)) {
			const tensorData = this.data.get(dataId);
			tensorData.refCount--;
		}
	}

	move(dataId, values, shape, dtype, refCount) {
		this.data.set(dataId, {values, dtype, refCount});
	}

	numDataIds() {
		return this.data.numDataIds();
	}

	async read(dataId) {
		return this.readSync(dataId);
	}

	readSync(dataId) {
		const {dtype, complexTensorInfos} = this.data.get(dataId);
		if (dtype === 'complex64') {
			const realValues = this.readSync(complexTensorInfos.real.dataId);
			const imagValues = this.readSync(complexTensorInfos.imag.dataId);
			return mergeRealAndImagArrays(realValues, imagValues);
		}
		return convertBackendValuesAndArrayBuffer(this.data.get(dataId).values, dtype);
	}

	bufferSync(t) {
		const data = this.readSync(t.dataId);
		if (t.dtype === 'string') {
			try {
				// Decode the bytes into string.
				const strings = data.map(d => decodeString(d));
				return buffer(t.shape, t.dtype, strings);
			} catch (_a) {
				throw new Error('Failed to decode encoded string bytes into utf-8');
			}
		}
		return buffer(t.shape, t.dtype, data);
	}

	makeOutput(values, shape, dtype) {
		return engine().makeTensorFromTensorInfo(this.makeTensorInfo(shape, dtype, values), this);
	}

	/**
	 * Dispose the memory if the dataId has 0 refCount. Return true if the memory
	 * is released or memory is not managed in this backend, false if memory is
	 * not cleared.
	 * @param dataId
	 * @oaram force Optional, remove the data regardless of refCount
	 */
	disposeData(dataId, force = false) {
		if (this.data.has(dataId)) {
			this.data.get(dataId).refCount--;
			if (!force && this.data.get(dataId).refCount > 0) {
				return false;
			}
			const {complexTensorInfos} = this.data.get(dataId);
			if (complexTensorInfos != null) {
				this.disposeData(complexTensorInfos.real.dataId, true);
				this.disposeData(complexTensorInfos.imag.dataId, true);
			}
			this.data.delete(dataId);
		}
		return true;
	}

	disposeIntermediateTensorInfo(tensorInfo) {
		this.disposeData(tensorInfo.dataId);
	}

	async time(f) {
		const start = now();
		f();
		const kernelMs = now() - start;
		return {kernelMs};
	}

	memory() {
		return {
			// Unreliable due to automatic gc. The numbers above are cumulative.
			unreliable: true,
			reasons: ['The reported memory is an upper bound. Due to automatic garbage ' +
			'collection, the true allocated memory may be less.']
		};
	}

	where(condition) {
		assertNotComplex([condition], 'where');
		const condVals = this.readSync(condition.dataId);
		return whereImpl(condition.shape, condVals);
	}

	dispose() {
	}

	floatPrecision() {
		return 32;
	}

	/** Returns the smallest representable number.  */
	epsilon() {
		return super.epsilon();
	}
}

MathBackendCPU.nextDataId = 0;

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the License);
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an AS IS BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
function simpleAbsImpl(vals) {
	const resultValues = new Float32Array(vals.length);
	for (let i = 0; i < vals.length; ++i) {
		resultValues[i] = Math.abs(vals[i]);
	}
	return resultValues;
}

const abs = (args) => {
	const {x} = args.inputs;
	const cpuBackend = args.backend;
	assertNotComplex(x, 'abs');
	let resultValues = new Float32Array(sizeFromShape(x.shape));
	const values = cpuBackend.data.get(x.dataId).values;
	resultValues = simpleAbsImpl(values);
	return cpuBackend.makeOutput(resultValues, x.shape, x.dtype);
};
const absConfig = {
	kernelName: Abs,
	backendName: 'cpu',
	kernelFunc: abs,
};

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Template that creates implementation for binary ops. Supports broadcast.
 */
function createSimpleBinaryKernelImpl(op) {
	return (aShape, bShape, aVals, bVals, dtype) => {
		const newShape = assertAndGetBroadcastShape(aShape, bShape);
		const resultRank = newShape.length;
		const resultStrides = computeStrides(newShape);
		const resultSize = sizeFromShape(newShape);
		const result = getTypedArrayFromDType(dtype, resultSize);
		const aRank = aShape.length;
		const bRank = bShape.length;
		const aStrides = computeStrides(aShape);
		const bStrides = computeStrides(bShape);
		const aBroadcastDims = getBroadcastDims(aShape, newShape);
		const bBroadcastDims = getBroadcastDims(bShape, newShape);
		if (aBroadcastDims.length + bBroadcastDims.length === 0) {
			for (let i = 0; i < result.length; ++i) {
				result[i] = op(aVals[i % aVals.length], bVals[i % bVals.length]);
			}
		} else {
			for (let i = 0; i < result.length; ++i) {
				const loc = indexToLoc(i, resultRank, resultStrides);
				const aLoc = loc.slice(-aRank);
				aBroadcastDims.forEach(d => aLoc[d] = 0);
				const aIndex = locToIndex(aLoc, aRank, aStrides);
				const bLoc = loc.slice(-bRank);
				bBroadcastDims.forEach(d => bLoc[d] = 0);
				const bIndex = locToIndex(bLoc, bRank, bStrides);
				result[i] = op(aVals[aIndex], bVals[bIndex]);
			}
		}
		return [result, newShape];
	};
}

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
function complex(args) {
	const {inputs, backend} = args;
	const {real, imag} = inputs;
	const realVals = backend.data.get(real.dataId).values;
	const imagVals = backend.data.get(imag.dataId).values;
	const complexInfo = backend.makeTensorInfo(real.shape, 'complex64');
	const complex = backend.data.get(complexInfo.dataId);
	// The complex tensor owns the underlying real and imag tensorInfos, only the
	// complex tensor tracks refCount, when complexData is disposed the
	// underlying tensorData will be disposed.
	complex.complexTensorInfos = {
		real: backend.makeTensorInfo(real.shape, 'float32', realVals),
		imag: backend.makeTensorInfo(imag.shape, 'float32', imagVals)
	};
	return complexInfo;
}

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Generates a tensorInfo with all zeros value.
 * @param backend cpu backend.
 * @param shape Shape for the zeros tensor.
 * @param dtype Optional. If set, the result has this dtype.
 */
function zeros(backend, shape, dtype = 'float32') {
	if (dtype === 'complex64') {
		const real = zeros(backend, shape, 'float32');
		const imag = zeros(backend, shape, 'float32');
		return complex({inputs: {real, imag}, backend});
	}
	const values = makeZerosTypedArray(sizeFromShape(shape), dtype);
	return backend.makeTensorInfo(shape, dtype, values);
}

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
function identity(args) {
	const {inputs, backend} = args;
	const {x} = inputs;
	backend.incRef(x.dataId);
	return {dataId: x.dataId, shape: x.shape, dtype: x.dtype};
}

const identityConfig = {
	kernelName: Identity,
	backendName: 'cpu',
	kernelFunc: identity
};

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
function real(args) {
	const {inputs, backend} = args;
	const {input} = inputs;
	const real = backend.data.get(input.dataId).complexTensorInfos.real;
	const realVal = backend.data.get(real.dataId).values;
	// When complex tensor is disposed, its underlying parts will be disposed too.
	// Make new tensor out of the real value of the complex. This makes sure the
	// value is still accessible even if complex tensor is disposed.
	return backend.makeTensorInfo(real.shape, real.dtype, realVal);
}

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
function castImpl(values, shape, inputType, dtype) {
	if (dtype === 'int32') {
		const resultValues = Int32Array.from(values);
		return [shape, 'int32', resultValues];
	}
	if (dtype === 'bool') {
		// This is essentially the result of notEqual(x, 0). We avoid using
		// kernel notEqual to avoid circular dependency, i.e. binary_utils ->
		// cast -> notEqual -> binary_utils.
		const zero = toTypedArray([0], inputType);
		const [resultData, resultShape] = createSimpleBinaryKernelImpl((a, b) => (a !== b) ? 1 : 0)(shape, [], values, zero, 'bool');
		return [resultShape, 'bool', resultData];
	}
	throw new Error(`Error in Cast: failed to cast ${inputType} to ${dtype}`);
}

function cast(args) {
	const {inputs, backend, attrs} = args;
	const {x} = inputs;
	const {dtype} = attrs;
	// Casting to complex64.
	if (dtype === 'complex64') {
		if (x.dtype === 'complex64') {
			return identity({inputs: {x}, backend});
		}
		const zerosTensorInfo = zeros(backend, x.shape, x.dtype);
		const floatX = cast({inputs: {x}, backend, attrs: {dtype: 'float32'}});
		const result = complex({inputs: {real: floatX, imag: zerosTensorInfo}, backend});
		backend.disposeIntermediateTensorInfo(zerosTensorInfo);
		backend.disposeIntermediateTensorInfo(floatX);
		return result;
	}
	// Casting from complex64
	if (x.dtype === 'complex64') {
		const realPart = real({inputs: {input: x}, backend});
		const result = cast({inputs: {x: realPart}, backend, attrs: {dtype}});
		backend.disposeIntermediateTensorInfo(realPart);
		return result;
	}
	if (!hasEncodingLoss(x.dtype, dtype)) {
		// We don't change the underlying data, since we cast to higher
		// precision.
		const result = identity({inputs: {x}, backend});
		return {dataId: result.dataId, shape: result.shape, dtype};
	}
	const values = backend.data.get(x.dataId).values;
	const [resultShape, resultType, resultData] = castImpl(values, x.shape, x.dtype, dtype);
	return backend.makeTensorInfo(resultShape, resultType, resultData);
}

const castConfig = {
	kernelName: Cast,
	backendName: 'cpu',
	kernelFunc: cast
};

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Template that creates a `KernelFunc` for binary ops.
 * @param name Kernel name.
 * @param binaryKernelImpl A `SimpleBinaryKernelImpl` for the kernel.
 * @param binaryKernelComplexImpl Optional. If exists, represents a
 *     `ComplexBinaryKernelImpl` for the kernel, will be used when input dtype
 *     is `complex64`.
 * @param dtype Optional. If set, the result has this dtype. Otherwise, the
 *     result has the same dtype as the first input. This is mainly used in
 *     comparison kernels, such as Equal, Less, Greater, etc.
 */
function binaryKernelFunc(name, simpleImpl, complexImpl, dtype) {
	if (complexImpl == null) {
		return ({inputs, backend}) => {
			const {a, b} = inputs;
			const cpuBackend = backend;
			assertNotComplex([a, b], name);
			const aVals = cpuBackend.data.get(a.dataId).values;
			const bVals = cpuBackend.data.get(b.dataId).values;
			const decodedAVals = a.dtype === 'string' ?
				// tslint:disable-next-line: no-any
				fromUint8ToStringArray(aVals) :
				aVals;
			const decodedBVals = a.dtype === 'string' ?
				// tslint:disable-next-line: no-any
				fromUint8ToStringArray(bVals) :
				bVals;
			const $dtype = dtype || a.dtype;
			const [resultData, resultShape] = simpleImpl(a.shape, b.shape, decodedAVals, decodedBVals, $dtype);
			return cpuBackend.makeTensorInfo(resultShape, $dtype, resultData);
		};
	}
	return ({inputs, backend}) => {
		const {a, b} = inputs;
		const cpuBackend = backend;
		if (a.dtype === 'complex64' || b.dtype === 'complex64') {
			const $aComplex = cast({inputs: {x: a}, backend: cpuBackend, attrs: {dtype: 'complex64'}});
			const $aComplexVals = cpuBackend.data.get($aComplex.dataId);
			const aReal = $aComplexVals.complexTensorInfos.real;
			const aImag = $aComplexVals.complexTensorInfos.imag;
			const aRealVals = cpuBackend.data.get(aReal.dataId).values;
			const aImagVals = cpuBackend.data.get(aImag.dataId).values;
			const $bComplex = cast({inputs: {x: b}, backend: cpuBackend, attrs: {dtype: 'complex64'}});
			const $bComplexVals = cpuBackend.data.get($bComplex.dataId);
			const bReal = $bComplexVals.complexTensorInfos.real;
			const bImag = $bComplexVals.complexTensorInfos.imag;
			const bRealVals = cpuBackend.data.get(bReal.dataId).values;
			const bImagVals = cpuBackend.data.get(bImag.dataId).values;
			const [resultRealData, resultImagData, resultShape] = complexImpl(a.shape, b.shape, aRealVals, aImagVals, bRealVals, bImagVals);
			const resultReal = cpuBackend.makeTensorInfo(resultShape, 'float32', resultRealData);
			const resultImag = cpuBackend.makeTensorInfo(resultShape, 'float32', resultImagData);
			const result = complex({inputs: {real: resultReal, imag: resultImag}, backend: cpuBackend});
			cpuBackend.disposeIntermediateTensorInfo($aComplex);
			cpuBackend.disposeIntermediateTensorInfo($bComplex);
			cpuBackend.disposeIntermediateTensorInfo(resultReal);
			cpuBackend.disposeIntermediateTensorInfo(resultImag);
			return result;
		} else {
			const aVals = cpuBackend.data.get(a.dataId).values;
			const bVals = cpuBackend.data.get(b.dataId).values;
			const $dtype = dtype || a.dtype;
			const [resultData, resultShape] = simpleImpl(a.shape, b.shape, aVals, bVals, $dtype);
			return cpuBackend.makeTensorInfo(resultShape, $dtype, resultData);
		}
	};
}

/**
 * Template that creates the complex type implementation for binary ops.
 * Supports broadcast.
 */
function createComplexBinaryKernelImpl(op) {
	return (aShape, bShape, aRealVals, aImagVals, bRealVals, bImagVals) => {
		const resultShape = assertAndGetBroadcastShape(aShape, bShape);
		const resultSize = sizeFromShape(resultShape);
		const resultRank = resultShape.length;
		const resultStrides = computeStrides(resultShape);
		const resultRealVals = getTypedArrayFromDType('float32', resultSize);
		const resultImagVals = getTypedArrayFromDType('float32', resultSize);
		const aBroadcastDims = getBroadcastDims(aShape, resultShape);
		const bBroadcastDims = getBroadcastDims(bShape, resultShape);
		const aVals = mergeRealAndImagArrays(aRealVals, aImagVals);
		const bVals = mergeRealAndImagArrays(bRealVals, bImagVals);
		const aRank = aShape.length;
		const aStrides = computeStrides(aShape);
		const bRank = bShape.length;
		const bStrides = computeStrides(bShape);
		if (aBroadcastDims.length + bBroadcastDims.length === 0) {
			for (let i = 0; i < resultRealVals.length; i++) {
				const aIdx = i % aVals.length;
				const bIdx = i % bVals.length;
				const result = op(aVals[aIdx * 2], aVals[aIdx * 2 + 1], bVals[bIdx * 2], bVals[bIdx * 2 + 1]);
				resultRealVals[i] = result.real;
				resultImagVals[i] = result.imag;
			}
		} else {
			for (let i = 0; i < resultRealVals.length; i++) {
				const loc = indexToLoc(i, resultRank, resultStrides);
				const aLoc = loc.slice(-aRank);
				aBroadcastDims.forEach(d => aLoc[d] = 0);
				const aIndex = locToIndex(aLoc, aRank, aStrides);
				const bLoc = loc.slice(-bRank);
				bBroadcastDims.forEach(d => bLoc[d] = 0);
				const bIndex = locToIndex(bLoc, bRank, bStrides);
				const opResult = op(aVals[aIndex * 2], aVals[aIndex * 2 + 1], bVals[bIndex * 2], bVals[bIndex * 2 + 1]);
				resultRealVals[i] = opResult.real;
				resultImagVals[i] = opResult.imag;
			}
		}
		return [resultRealVals, resultImagVals, resultShape];
	};
}

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
const addImpl = createSimpleBinaryKernelImpl(((a, b) => a + b));
const addComplexImpl = createComplexBinaryKernelImpl(((aReal, aImag, bReal, bImag) => {
	return {real: aReal + bReal, imag: aImag + bImag};
}));
const add = binaryKernelFunc(Add, addImpl, addComplexImpl);
const addConfig = {
	kernelName: Add,
	backendName: 'cpu',
	kernelFunc: add
};

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
function bincountImpl(xVals, weightsVals, weightsDtype, weightsShape, size) {
	const weightsSize = sizeFromShape(weightsShape);
	const outVals = makeZerosTypedArray(size, weightsDtype);
	for (let i = 0; i < xVals.length; i++) {
		const value = xVals[i];
		if (value < 0) {
			throw new Error('Input x must be non-negative!');
		}
		if (value >= size) {
			continue;
		}
		if (weightsSize > 0) {
			outVals[value] += weightsVals[i];
		} else {
			outVals[value] += 1;
		}
	}
	return outVals;
}

function bincountReduceImpl(xBuf, weightsBuf, size, binaryOutput = false) {
	const numRows = xBuf.shape[0];
	const numCols = xBuf.shape[1];
	const outBuf = buffer([numRows, size], weightsBuf.dtype);
	for (let i = 0; i < numRows; i++) {
		for (let j = 0; j < numCols; j++) {
			const value = xBuf.get(i, j);
			if (value < 0) {
				throw new Error('Input x must be non-negative!');
			}
			if (value >= size) {
				continue;
			}
			if (binaryOutput) {
				outBuf.set(1, i, value);
			} else {
				if (weightsBuf.size > 0) {
					outBuf.set(outBuf.get(i, value) + weightsBuf.get(i, j), i, value);
				} else {
					outBuf.set(outBuf.get(i, value) + 1, i, value);
				}
			}
		}
	}
	return outBuf;
}

/**
 * @license
 * Copyright 2023 Google LLC.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
const bitwiseAndImpl = createSimpleBinaryKernelImpl(((a, b) => a & b));

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Template that creates implementation for unary op.
 */
function createSimpleUnaryImpl(op) {
	return (values, dtype, attrs) => {
		const newValues = getArrayFromDType(dtype, values.length);
		for (let i = 0; i < values.length; ++i) {
			newValues[i] = op(values[i], attrs);
		}
		return newValues;
	};
}

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * Template that creates a `KernelFunc` for unary ops.
 * @param name Kernel name.
 * @param op A `SimpleUnaryOperation` for the kernel.
 * @param dtype Optional. If set, the result has this dtype. Otherwise, the
 *     result has the same dtype as the input. This is mainly used in certain
 *     kernels that return bool type, such as isFinite, isInf, etc.
 */
function unaryKernelFunc(name, op, dtype) {
	const impl = createSimpleUnaryImpl(op);
	return unaryKernelFuncFromImpl(name, impl, dtype);
}

/**
 * Template that creates a `KernelFunc` for unary ops from the given
 * `SimpleUnaryImpl`..
 * @param name Kernel name.
 * @param unaryImpl A `SimpleUnaryImpl` that implements the op.
 * @param dtype Optional. If set, the result has this dtype. Otherwise, the
 *     result has the same dtype as the input. This is mainly used in certain
 *     kernels that return bool type, such as isFinite, isInf, etc.
 */
function unaryKernelFuncFromImpl(name, unaryImpl, dtype) {
	return ({inputs, attrs, backend}) => {
		const {x} = inputs;
		assertNotComplex(x, name);
		const cpuBackend = backend;
		const values = cpuBackend.data.get(x.dataId).values;
		let decoded;
		if (x.dtype === 'string') {
			if (!Array.isArray(values)) {
				throw new Error('String tensor\'s value was not an instance of Array');
			}
			decoded = fromUint8ToStringArray(values);
		} else {
			decoded = values;
		}
		const $dtype = dtype || x.dtype;
		const newValues = unaryImpl(decoded, $dtype, attrs);
		return cpuBackend.makeTensorInfo(x.shape, $dtype, newValues);
	};
}

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the License);
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an AS IS BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
const ceilImpl = createSimpleUnaryImpl((xi) => Math.ceil(xi));

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
function concatImpl(inputs, outShape, dtype, simplyConcat) {
	const outVals = getArrayFromDType(dtype, sizeFromShape(outShape));
	if (simplyConcat && dtype !== 'string') {
		// Use built-in TypedArray.set() method for speed.
		let offset = 0;
		inputs.forEach(input => {
			const size = sizeFromShape(input.shape);
			outVals.set(input.vals, offset);
			offset += size;
		});
	} else {
		let colOffset = 0;
		inputs.forEach(input => {
			const decodedData = dtype === 'string' ?
				fromUint8ToStringArray(input.vals) :
				input.vals;
			let tIdx = 0;
			for (let row = 0; row < input.shape[0]; ++row) {
				const resIdx = row * outShape[1] + colOffset;
				for (let col = 0; col < input.shape[1]; ++col) {
					outVals[resIdx + col] = decodedData[tIdx++];
				}
			}
			colOffset += input.shape[1];
		});
	}
	return outVals;
}

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
const equalImpl = createSimpleBinaryKernelImpl((a, b) => (a === b) ? 1 : 0);
const equal = binaryKernelFunc(Equal, equalImpl, null /* complexImpl */, 'bool');
const equalConfig = {
	kernelName: Equal,
	backendName: 'cpu',
	kernelFunc: equal
};

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the License);
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an AS IS BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
const expImpl = createSimpleUnaryImpl((xi) => Math.exp(xi));
const exp = unaryKernelFuncFromImpl(Exp, expImpl, 'float32');
const expConfig = {
	kernelName: Exp,
	backendName: 'cpu',
	kernelFunc: exp,
};

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the License);
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an AS IS BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
const expm1Impl = createSimpleUnaryImpl((xi) => Math.expm1(xi));

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the License);
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an AS IS BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
const floorImpl = createSimpleUnaryImpl((xi) => Math.floor(xi));
const floor = unaryKernelFuncFromImpl(Floor, floorImpl);
const floorConfig = {
	kernelName: Floor,
	backendName: 'cpu',
	kernelFunc: floor,
};

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
const floorDivImpl = createSimpleBinaryKernelImpl((a, b) => Math.floor(a / b));

/**
 * @license
 * Copyright 2021 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
function gatherNdImpl(indicesData, paramsBuf, dtype, numSlices, sliceRank, sliceSize, strides, paramsShape, paramsSize) {
	const outBuf = buffer([numSlices, sliceSize], dtype);
	for (let i = 0; i < numSlices; i++) {
		const index = [];
		let flattenIndex = 0;
		for (let j = 0; j < sliceRank; j++) {
			const dim = indicesData[i * sliceRank + j];
			flattenIndex += dim * strides[j];
			index.push(dim);
		}
		if (flattenIndex < 0 || flattenIndex >= paramsSize / sliceSize) {
			throw new Error(`Invalid indices: ${index} does not index into ${paramsShape}`);
		}
		for (let k = 0; k < sliceSize; k++) {
			outBuf.values[i * sliceSize + k] =
				paramsBuf.get(...paramsBuf.indexToLoc(flattenIndex * sliceSize + k));
		}
	}
	return outBuf;
}

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
function gatherV2Impl(xBuf, indicesBuf, flattenOutputShape) {
	const outBuf = buffer(flattenOutputShape, xBuf.dtype);
	for (let i = 0; i < outBuf.size; ++i) {
		const newLoc = outBuf.indexToLoc(i);
		const originalLoc = newLoc.slice();
		const batchIdx = originalLoc[0];
		const indicesIdx = originalLoc[2];
		const indicesIndex = indicesBuf.locToIndex([batchIdx, indicesIdx]);
		originalLoc[2] = indicesBuf.values[indicesIndex];
		const originalIndex = xBuf.locToIndex(originalLoc);
		if (0 <= originalIndex && originalIndex < xBuf.values.length) {
			outBuf.values[i] = xBuf.values[originalIndex];
		} // Else, index is out of bounds, so leave the default zero val in outBuf.
	}
	return outBuf;
}

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
const greaterImpl = createSimpleBinaryKernelImpl((a, b) => (a > b) ? 1 : 0);
const greater = binaryKernelFunc(Greater, greaterImpl, null /* complexImpl */, 'bool');
const greaterConfig = {
	kernelName: Greater,
	backendName: 'cpu',
	kernelFunc: greater
};

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
const greaterEqualImpl = createSimpleBinaryKernelImpl((a, b) => (a >= b) ? 1 : 0);
const greaterEqual = binaryKernelFunc(GreaterEqual, greaterEqualImpl, null /* complexImpl */, 'bool');
const greaterEqualConfig = {
	kernelName: GreaterEqual,
	backendName: 'cpu',
	kernelFunc: greaterEqual
};

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
const lessImpl = createSimpleBinaryKernelImpl((a, b) => (a < b) ? 1 : 0);

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
const lessEqualImpl = createSimpleBinaryKernelImpl((a, b) => (a <= b) ? 1 : 0);
const lessEqual = binaryKernelFunc(LessEqual, lessEqualImpl, null /* complexImpl */, 'bool');
const lessEqualConfig = {
	kernelName: LessEqual,
	backendName: 'cpu',
	kernelFunc: lessEqual
};

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
function linSpaceImpl(start, stop, num) {
	const step = (stop - start) / (num - 1);
	const values = makeZerosTypedArray(num, 'float32');
	values[0] = start;
	for (let i = 1; i < values.length; i++) {
		values[i] = values[i - 1] + step;
	}
	return values;
}

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the License);
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an AS IS BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
const logImpl = createSimpleUnaryImpl((xi) => Math.log(xi));
const log = unaryKernelFuncFromImpl(Log, logImpl);
const logConfig = {
	kernelName: Log,
	backendName: 'cpu',
	kernelFunc: log,
};

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
function maxImpl(aVals, reduceSize, outShape, dtype) {
	const vals = getTypedArrayFromDType(dtype, sizeFromShape(outShape));
	for (let i = 0; i < vals.length; ++i) {
		const offset = i * reduceSize;
		let max = aVals[offset];
		for (let j = 0; j < reduceSize; ++j) {
			const value = aVals[offset + j];
			if (Number.isNaN(value) ||
				value > max) { // comparison with NaN always return false
				max = value;
			}
		}
		vals[i] = max;
	}
	return vals;
}

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
const maximumImpl = createSimpleBinaryKernelImpl(((aValue, bValue) => Math.max(aValue, bValue)));

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
const minimumImpl = createSimpleBinaryKernelImpl(((aValue, bValue) => Math.min(aValue, bValue)));

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
const multiplyImpl = createSimpleBinaryKernelImpl(((aValue, bValue) => aValue * bValue));
const multiplyComplexImpl = createComplexBinaryKernelImpl(((aReal, aImag, bReal, bImag) => {
	return {
		real: aReal * bReal - aImag * bImag,
		imag: aReal * bImag + aImag * bReal
	};
}));
const multiply = binaryKernelFunc(Multiply, multiplyImpl, multiplyComplexImpl);
const multiplyConfig = {
	kernelName: Multiply,
	backendName: 'cpu',
	kernelFunc: multiply
};

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
function negImpl(xVals, xShape, xDtype) {
	const minusOne = createScalarValue(-1, xDtype);
	return multiplyImpl([], xShape, minusOne, xVals, xDtype);
}

function neg(args) {
	const {inputs, backend} = args;
	const {x} = inputs;
	assertNotComplex(x, 'neg');
	const xVals = backend.data.get(x.dataId).values;
	const [res, newShape] = negImpl(xVals, x.shape, x.dtype);
	return backend.makeTensorInfo(newShape, x.dtype, res);
}

const negConfig = {
	kernelName: Neg,
	backendName: 'cpu',
	kernelFunc: neg
};

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
const notEqualImpl = createSimpleBinaryKernelImpl(((a, b) => (a !== b) ? 1 : 0));

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
function transposeImpl(xVals, xShape, dtype, perm, newShape) {
	const xRank = xShape.length;
	const xSize = sizeFromShape(xShape);
	const xStrides = computeStrides(xShape);
	const newStrides = computeStrides(newShape);
	const result = getTypedArrayFromDType(dtype, sizeFromShape(newShape));
	for (let i = 0; i < xSize; ++i) {
		const loc = indexToLoc(i, xRank, xStrides);
		// Permute location.
		const newLoc = new Array(loc.length);
		for (let i = 0; i < newLoc.length; i++) {
			newLoc[i] = loc[perm[i]];
		}
		const newIndex = locToIndex(newLoc, xRank, newStrides);
		result[newIndex] = xVals[i];
	}
	return result;
}

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
function transpose(args) {
	const {inputs, attrs, backend} = args;
	const {x} = inputs;
	const {perm} = attrs;
	assertNotComplex(x, 'transpose');
	const xRank = x.shape.length;
	const newShape = new Array(xRank);
	for (let i = 0; i < newShape.length; i++) {
		newShape[i] = x.shape[perm[i]];
	}
	const values = backend.data.get(x.dataId).values;
	const result = transposeImpl(values, x.shape, x.dtype, perm, newShape);
	const dataId = backend.write(result, newShape, x.dtype);
	return {dataId, shape: newShape, dtype: x.dtype};
}

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
function prodImpl(xShape, xDtype, xVals, reductionAxes) {
	const [outShape, reduceShape] = computeOutAndReduceShapes(xShape, reductionAxes);
	const outDtype = upcastType(xDtype, 'int32');
	const outVals = makeZerosTypedArray(sizeFromShape(outShape), outDtype);
	const reduceSize = sizeFromShape(reduceShape);
	for (let i = 0; i < outVals.length; ++i) {
		const offset = i * reduceSize;
		let prod = 1;
		for (let j = 0; j < reduceSize; ++j) {
			prod *= xVals[offset + j];
		}
		outVals[i] = prod;
	}
	return {outVals, outShape, outDtype};
}

/**
 * @license
 * Copyright 2022 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
function validateIndices(indices, indicesShape, numParams) {
	indices.forEach((index, i) => {
		if (index < 0 || index >= numParams) {
			const locString = indexToLoc(i, indicesShape.length, computeStrides(indicesShape))
				.join(',');
			throw new Error(`indices[${locString}] = ${index} is not in [0, ${numParams})`);
		}
	});
}

function validateSplits(paramsNestedSplits, numParamsDenseValues) {
	// Validate
	for (let dim = 0; dim < paramsNestedSplits.length; ++dim) {
		const splits = paramsNestedSplits[dim];
		const lastSplit = (dim === paramsNestedSplits.length - 1) ?
			numParamsDenseValues :
			paramsNestedSplits[dim + 1].length;
		if (splits.length === 0) {
			throw new Error('Ragged splits may not be empty');
		}
		if (splits[0] < 0) {
			throw new Error('Ragged splits must be non-negative');
		}
		if (splits[splits.length - 1] > lastSplit) {
			throw new Error('Ragged splits must not point past values');
		}
		for (let i = 1; i < splits.length; ++i) {
			if (splits[i - 1] > splits[i]) {
				throw new Error('Ragged splits must be sorted in ascending order');
			}
		}
	}
}

// Construct the `splits` output tensors, encoded using a nested vector.
// Also find the slices of values that need to be copied, and store them
// in `valueSlices`.  The total number of values that will be copied (which
// we need for allocating the output values tensor) is stored in `numValues`.
function makeSplits(indices, indicesShape, paramsNestedSplits, numParamsDenseValues) {
	const valueSlices = [];
	let numValues = 0;
	const numSplits = indicesShape.length - 1 + paramsNestedSplits.length;
	const outSplits = new Array(numSplits).fill(null).map(() => [0]);
	validateSplits(paramsNestedSplits, numParamsDenseValues);
	// Add `splits` that come from all but the last dimension of the dense
	// Tensor `indices`.  In particular, for each dimension D, we add a
	// splits tensor whose values are:
	//   range(reduceProd(splits.shape[:D]) + 1) * splits.shape[D+1]
	// E.g., if indices.shape=[2, 3, 4] then we will add splits tensors:
	//   [0, 3, 6]                    # length=2+1, stride=3
	//   [0, 4, 8, 12, 16, 20, 24]    # length=2*3+1, stride=4
	let nrows = 1;
	for (let dim = 0; dim < indicesShape.length - 1; ++dim) {
		nrows *= indicesShape[dim];
		const rowLength = indicesShape[dim + 1];
		for (let i = 1; i < nrows + 1; ++i) {
			outSplits[dim].push(i * rowLength);
		}
	}
	// Add `splits` that come from `paramsNestedSplits`.  Starting with the
	// outermost ragged dimension (i.e., the first `splits` tensor), we work
	// our way in, finding the range of values that should be copied.  As we
	// go, we update the output `splits` for each dimension with the appropriate
	// values.  In particular, the *lengths* of the slices from `param_splits`
	// should be copied to generate corresponding slice lengths in the output
	// splits.  E.g., if we are copying a ragged row with length 4, then we
	// should add a new split point to outSplits that is 4 greater than the
	// previous split point in outSplits.
	for (let i = 0; i < indices.length; ++i) {
		let start = indices[i];
		let limit = indices[i] + 1;
		// Copy splits.
		for (let dim = 0; dim < paramsNestedSplits.length; ++dim) {
			const splits = paramsNestedSplits[dim];
			const outDim = dim + indicesShape.length - 1;
			if (outDim >= 0) {
				const outSplitsOutDim = outSplits[outDim];
				const delta = outSplitsOutDim[outSplitsOutDim.length - 1] - splits[start];
				for (let j = start; j < limit; ++j) {
					outSplits[outDim].push(splits[j + 1] + delta);
				}
			}
			start = splits[start];
			limit = splits[limit];
		}
		if (limit !== start) {
			valueSlices.push([start, limit]);
			numValues += limit - start;
		}
	}
	return {outSplits, valueSlices, numValues};
}

function getSplits(outSplits) {
	const splitsOut = [];
	for (let i = 0; i < outSplits.length; ++i) {
		const numSplits = outSplits[i].length;
		const splits = getArrayFromDType('int32', numSplits);
		splitsOut.push(splits);
		outSplits[i].forEach((value, j) => splits[j] = value);
	}
	return splitsOut;
}

function computeFlatOuterDims(orig, numOutDims) {
	const outDims = orig.slice(0, numOutDims);
	while (outDims.length < numOutDims) {
		outDims.push(1);
	}
	for (let inDim = numOutDims; inDim < orig.length; inDim++) {
		outDims[numOutDims - 1] *= orig[inDim];
	}
	return outDims;
}

// For each slice in `(start, limit)` in `valueSlices`, append
// `paramsDenseValues[start,...,limit] to `values`.  `valueSize` indicates
// the number of scalars contained in each value paramsDenseValues[i].
function writeValueSlices(paramsDenseValues, paramsDenseValuesShape, valueSlices, valueSize, values, valuesShape) {
	const denseM = computeFlatOuterDims(paramsDenseValuesShape, 2)[1];
	const valuesM = computeFlatOuterDims(valuesShape, 2)[1];
	let outPos = 0;
	for (const slice of valueSlices) {
		for (let i = slice[0]; i < slice[1]; ++i) {
			for (let j = 0; j < valueSize; ++j) {
				values[outPos * valuesM + j] = paramsDenseValues[i * denseM + j];
			}
			++outPos;
		}
	}
}

function getValues(paramsDenseValues, paramsDenseValuesShape, paramsDenseValuesDType, valueSlices, numValues) {
	const valuesShape = paramsDenseValuesShape.slice();
	valuesShape[0] = numValues;
	const valuesOut = getArrayFromDType(paramsDenseValuesDType, sizeFromShape(valuesShape));
	const numElements = paramsDenseValues.length;
	const valueSize = numElements === 0 ? 0 : (numElements / paramsDenseValuesShape[0]);
	writeValueSlices(paramsDenseValues, paramsDenseValuesShape, valueSlices, valueSize, valuesOut, valuesShape);
	return [valuesOut, valuesShape];
}

function raggedGatherImpl(paramsNestedSplits, paramsNestedSplitsShapes, paramsDenseValues, paramsDenseValuesShape, paramsDenseValuesDType, indices, indicesShape, outputRaggedRank) {
	if (paramsNestedSplits.length === 0) {
		throw new Error('paramsNestedSplits must be non empty');
	}
	if (paramsNestedSplitsShapes[0].length === 0) {
		throw new Error('Split tensors must not be scalars');
	}
	const numParams = paramsNestedSplitsShapes[0][0] - 1;
	validateIndices(indices, indicesShape, numParams);
	if (paramsDenseValuesShape.length === 0) {
		throw new Error('params.rank must be nonzero');
	}
	const numParamsDenseValues = paramsDenseValuesShape[0];
	// Calculate the `splits`, and store the value slices that we need to
	// copy in `valueSlices`.
	const {outSplits, valueSlices, numValues} = makeSplits(indices, indicesShape, paramsNestedSplits, numParamsDenseValues);
	// Write the output tensors.
	const outputNestedSplits = getSplits(outSplits);
	const outputDenseValues = getValues(paramsDenseValues, paramsDenseValuesShape, paramsDenseValuesDType, valueSlices, numValues);
	return [outputNestedSplits, outputDenseValues[0], outputDenseValues[1]];
}

/**
 * @license
 * Copyright 2022 Google LLC.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
const INT32_MAX = 2147483647;

function raggedRangeImpl(starts, startsShape, startsDType, limits, limitsShape, deltas, deltasShape) {
	// Check input tensor shapes.
	if (startsShape.length > 1) {
		throw new Error('starts must be a scalar or vector');
	}
	if (limitsShape.length > 1) {
		throw new Error('limits must be a scalar or vector');
	}
	if (deltasShape.length > 1) {
		throw new Error('deltas must be a scalar or vector');
	}
	// Determine which tensors we need to broadcast.
	const broadcastStarts = startsShape.length === 0;
	const broadcastLimits = limitsShape.length === 0;
	const broadcastDeltas = deltasShape.length === 0;
	// nRows (number of output rows) is the size of the non-broadcast inputs,
	// or 1 if all inputs are scalars.
	const inSizes = [];
	if (!broadcastStarts) {
		inSizes.push(startsShape[0]);
	}
	if (!broadcastLimits) {
		inSizes.push(limitsShape[0]);
	}
	if (!broadcastDeltas) {
		inSizes.push(deltasShape[0]);
	}
	for (let i = 1; i < inSizes.length; ++i) {
		if (inSizes[i] !== inSizes[i - 1]) {
			throw new Error('starts, limits, and deltas must have the same shape');
		}
	}
	const nRows = inSizes.length === 0 ? 1 : inSizes[0];
	// Construct the rtNestedSplits tensor.
	const rtNestedSplits = getArrayFromDType('int32', nRows + 1);
	rtNestedSplits[0] = 0;
	for (let row = 0; row < nRows; ++row) {
		const start = broadcastStarts ? starts[0] : starts[row];
		const limit = broadcastLimits ? limits[0] : limits[row];
		const delta = broadcastDeltas ? deltas[0] : deltas[row];
		if (delta === 0) {
			throw new Error('Requires delta != 0');
		}
		let size; // The number of elements in the specified range.
		if (((delta > 0) && (limit < start)) || ((delta < 0) && (limit > start))) {
			size = 0;
		} else {
			size = Math.ceil(Math.abs((limit - start) / delta));
			if (size > INT32_MAX) {
				throw new Error(`Requires ((limit - start) / delta) <= ${INT32_MAX}`);
			}
		}
		rtNestedSplits[row + 1] = rtNestedSplits[row] + size;
	}
	const nVals = rtNestedSplits[nRows];
	// Construct the rtDenseValues tensor.
	const rtDenseValues = getArrayFromDType(startsDType, nVals);
	let valueIndex = 0;
	for (let row = 0; row < nRows; ++row) {
		const rowSize = rtNestedSplits[row + 1] - rtNestedSplits[row];
		let value = broadcastStarts ? starts[0] : starts[row];
		const delta = broadcastDeltas ? deltas[0] : deltas[row];
		for (let i = 0; i < rowSize; ++i) {
			rtDenseValues[valueIndex++] = value;
			value += delta;
		}
	}
	return [rtNestedSplits, rtDenseValues];
}

/**
 * @license
 * Copyright 2022 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
var RowPartitionType = RowPartitionType$1;
// Based on
// https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/ragged_tensor_to_tensor_op.cc
class RaggedTensorToTensorOp {
	constructor(shape, shapeShape, values, valuesShape, valuesDType, defaultValue, defaultValueShape, rowPartitionValues, rowPartitionValuesShapes, rowPartitionTypeStrings) {
		this.shape = shape;
		this.shapeShape = shapeShape;
		this.values = values;
		this.valuesShape = valuesShape;
		this.valuesDType = valuesDType;
		this.defaultValue = defaultValue;
		this.defaultValueShape = defaultValueShape;
		this.rowPartitionValues = rowPartitionValues;
		this.rowPartitionValuesShapes = rowPartitionValuesShapes;
		this.rowPartitionTypes =
			getRowPartitionTypesHelper(rowPartitionTypeStrings);
		this.raggedRank = getRaggedRank(this.rowPartitionTypes);
	}

	getRowPartitionTypeByDimension(dimension) {
		if (this.rowPartitionTypes[0] === RowPartitionType.FIRST_DIM_SIZE) {
			return this.rowPartitionTypes[dimension + 1];
		} else {
			return this.rowPartitionTypes[dimension];
		}
	}

	// Returns the relationship between dimension and dimension + 1.
	getRowPartitionTensor(dimension) {
		if (this.rowPartitionTypes[0] === RowPartitionType.FIRST_DIM_SIZE) {
			return this.rowPartitionValues[dimension + 1];
		} else {
			return this.rowPartitionValues[dimension];
		}
	}

	getMaxWidth(dimension) {
		const rowPartitionTensor = this.getRowPartitionTensor(dimension - 1);
		switch (this.getRowPartitionTypeByDimension(dimension - 1)) {
			case RowPartitionType.VALUE_ROWIDS:
				return RaggedTensorToTensorOp.getMaxWidthValueRowID(rowPartitionTensor);
			case RowPartitionType.ROW_SPLITS:
				return RaggedTensorToTensorOp.getMaxWidthRowSplit(rowPartitionTensor);
			default:
				throw new Error(`Cannot handle partition type ${RowPartitionType[this.getRowPartitionTypeByDimension(dimension - 1)]}`);
		}
	}

	static getMaxWidthRowSplit(rowSplit) {
		const tensorLength = rowSplit.length;
		if (tensorLength === 0 || tensorLength === 1) {
			return 0;
		}
		let maxWidth = 0;
		for (let i = 0; i < tensorLength - 1; ++i) {
			const currentWidth = rowSplit[i + 1] - rowSplit[i];
			if (currentWidth > maxWidth) {
				maxWidth = currentWidth;
			}
		}
		return maxWidth;
	}

	static getMaxWidthValueRowID(valueRowIds) {
		const indexLength = valueRowIds.length;
		if (indexLength === 0) {
			return 0;
		}
		let firstEqualIndex = 0;
		let firstEqualIndexValue = valueRowIds[0];
		let maxWidth = 0;
		for (let i = 1; i < indexLength; ++i) {
			const value = valueRowIds[i];
			if (value !== firstEqualIndexValue) {
				firstEqualIndexValue = value;
				maxWidth = Math.max(i - firstEqualIndex, maxWidth);
				firstEqualIndex = i;
			}
		}
		return Math.max(indexLength - firstEqualIndex, maxWidth);
	}

	tensorShapeFromTensor(t, tShape, isPartial = true) {
		if (tShape.length === 0) {
			if (t[0] === -1) {
				return [];
			}
			throw new Error(`The only valid scalar shape tensor is the fully unknown shape specified as -1.`);
		}
		// MakePartialShape/MakeShapeHelper.
		return makeShape(t, isPartial);
	}

	calculateOutputSize(firstDim) {
		const valueShape = this.valuesShape;
		const defaultValueShape = this.defaultValueShape;
		validateDefaultValueShape(defaultValueShape, valueShape);
		const shape = this.tensorShapeFromTensor(this.shape, this.shapeShape);
		const outputShape = combineRaggedTensorToTensorShapes(this.raggedRank, shape, valueShape);
		const result = outputShape;
		if (result[0] < 0) {
			result[0] = firstDim;
		}
		for (let i = 1; i <= this.raggedRank; ++i) {
			if (result[i] < 0) {
				result[i] = this.getMaxWidth(i);
			}
		}
		return result;
	}

	/**
	 * The outputIndex represents the index in the output tensor
	 * where the first element of a particular dimension would be written.
	 * If it is -1, it indicates that the index is out of scope.
	 * Example, given firstDimension = 10, firstDimensionOutput = 6,
	 * and outputIndexMultiplier = 100:
	 * result = [0 100 200 300 400 500 -1 -1 -1 -1]
	 * If firstDimensionOutput = 11 instead, then:
	 * result = [0 100 200 300 400 500 600 700 800 900]
	 */
	calculateFirstParentOutputIndex(firstDimension, outputIndexMultiplier, firstDimensionOutput) {
		const minDimension = Math.min(firstDimension, firstDimensionOutput);
		const result = [];
		let currentOutputIndex = 0;
		for (let i = 0; i < minDimension; ++i, currentOutputIndex += outputIndexMultiplier) {
			result.push(currentOutputIndex);
		}
		for (let i = minDimension; i < firstDimension; ++i) {
			result.push(-1);
		}
		assert(result.length === firstDimension, () => 'Final length of result must be equal to firstDimension.');
		return result;
	}

	calculateOutputIndexRowSplit(rowSplit, parentOutputIndex, outputIndexMultiplier, outputSize) {
		const rowSplitSize = rowSplit.length;
		const result = [];
		for (let i = 0; i < rowSplitSize - 1; ++i) {
			const rowLength = rowSplit[i + 1] - rowSplit[i];
			let realLength = Math.min(outputSize, rowLength);
			let parentOutputIndexCurrent = parentOutputIndex[i];
			if (parentOutputIndexCurrent === -1) {
				realLength = 0;
			}
			for (let j = 0; j < realLength; ++j) {
				result.push(parentOutputIndexCurrent);
				parentOutputIndexCurrent += outputIndexMultiplier;
			}
			for (let j = 0; j < rowLength - realLength; ++j) {
				result.push(-1);
			}
		}
		if (rowSplitSize > 0 && result.length !== rowSplit[rowSplitSize - 1]) {
			throw new Error('Invalid row split size.');
		}
		return result;
	}

	// Calculate the output index of the first element of a list.
	// The parentOutputIndex is the same computation for the previous list.
	// -1 indicates an element or list that is out of range.
	// The outputIndexMultiplier is the number of output indices one moves
	// forward for each column.
	// E.g., given:
	// valueRowIds:[0 1 2 2 2 3 5 5 6]
	// parentOutputIndex:[1000 1100 2000 2100 -1 3000 4000]
	// outputIndexMultiplier: 10
	// outputSize: 2
	// You get:
	// result = [1000 1100 2000 2010 -1 2100 -1 -1 3000]
	// result[0] = parentOutputIndex[valueRowIds[0]]
	// result[1] = parentOutputIndex[valueRowIds[1]]
	// result[2] = parentOutputIndex[valueRowIds[2]]
	// result[3] = parentOutputIndex[valueRowIds[2] + 10]
	// result[4] = -1 because it is the third element the size is 2.
	// result[5] = parentOutputIndex[valueRowIds[3]]
	// result[6] = -1 because parentOutputIndex[valueRowIds[6]] == -1
	// result[7] = -1 because parentOutputIndex[valueRowIds[6]] == -1
	// result[8] = parentOutputIndex[valueRowIds[7]]
	calculateOutputIndexValueRowID(valueRowIds, parentOutputIndex, outputIndexMultiplier, outputSize) {
		const indexSize = valueRowIds.length;
		const result = [];
		if (indexSize === 0) {
			return [];
		}
		let currentOutputColumn = 0;
		let currentValueRowId = valueRowIds[0];
		if (currentValueRowId >= parentOutputIndex.length) {
			throw new Error(`Got currentValueRowId=${currentValueRowId}, which is not less than ${parentOutputIndex.length}`);
		}
		let currentOutputIndex = parentOutputIndex[currentValueRowId];
		result.push(currentOutputIndex);
		for (let i = 1; i < indexSize; ++i) {
			const nextValueRowId = valueRowIds[i];
			if (nextValueRowId === currentValueRowId) {
				if (currentOutputIndex >= 0) {
					++currentOutputColumn;
					if (currentOutputColumn < outputSize) {
						currentOutputIndex += outputIndexMultiplier;
					} else {
						currentOutputIndex = -1;
					}
				}
			} else {
				currentOutputColumn = 0;
				currentValueRowId = nextValueRowId;
				if (nextValueRowId >= parentOutputIndex.length) {
					throw new Error(`Got nextValueRowId=${nextValueRowId} which is not less than ${parentOutputIndex.length}`);
				}
				currentOutputIndex = parentOutputIndex[nextValueRowId];
			}
			result.push(currentOutputIndex);
		}
		if (result.length !== valueRowIds.length) {
			throw new Error('Invalid row ids.');
		}
		return result;
	}

	calculateOutputIndex(dimension, parentOutputIndex, outputIndexMultiplier, outputSize) {
		const rowPartitionTensor = this.getRowPartitionTensor(dimension);
		const partitionType = this.getRowPartitionTypeByDimension(dimension);
		switch (partitionType) {
			case RowPartitionType.VALUE_ROWIDS:
				return this.calculateOutputIndexValueRowID(rowPartitionTensor, parentOutputIndex, outputIndexMultiplier, outputSize);
			case RowPartitionType.ROW_SPLITS:
				if (rowPartitionTensor.length - 1 > parentOutputIndex.length) {
					throw new Error(`Row partition size is greater than output size: ${rowPartitionTensor.length - 1} > ${parentOutputIndex.length}`);
				}
				return this.calculateOutputIndexRowSplit(rowPartitionTensor, parentOutputIndex, outputIndexMultiplier, outputSize);
			default:
				throw new Error(`Unsupported partition type: ${RowPartitionType[partitionType]}`);
		}
	}

	getFirstDimensionSize() {
		const firstPartitionTensor = this.rowPartitionValues[0];
		if (this.rowPartitionTypes.length === 0) {
			throw new Error('No row_partition_types given.');
		}
		const firstPartitionType = this.rowPartitionTypes[0];
		switch (firstPartitionType) {
			case RowPartitionType.FIRST_DIM_SIZE:
				return firstPartitionTensor[0];
			case RowPartitionType.VALUE_ROWIDS:
				throw new Error('Cannot handle VALUE_ROWIDS in first dimension.');
			case RowPartitionType.ROW_SPLITS:
				return this.rowPartitionValuesShapes[0][0] - 1;
			default:
				throw new Error(`Cannot handle type ${RowPartitionType[firstPartitionType]}`);
		}
	}

	compute() {
		const firstPartitionTensor = this.rowPartitionValues[0];
		if (firstPartitionTensor.length <= 0) {
			throw new Error('Invalid first partition input. ' +
				'Tensor requires at least one element.');
		}
		const firstDimension = this.getFirstDimensionSize();
		const outputSize = this.calculateOutputSize(firstDimension);
		const multiplier = new Array(this.raggedRank + 1);
		multiplier[multiplier.length - 1] = 1;
		for (let i = multiplier.length - 2; i >= 0; --i) {
			multiplier[i] = multiplier[i + 1] * outputSize[i + 1];
		}
		// Full size of the tensor.
		const outputShape = makeShape(outputSize, false);
		const outputTensor = getArrayFromDType(this.valuesDType, sizeFromShape(outputShape));
		const fullSize = multiplier[0] * outputSize[0];
		if (fullSize > 0) {
			let outputIndex = this.calculateFirstParentOutputIndex(firstDimension, multiplier[0], outputSize[0]);
			for (let i = 1; i <= this.raggedRank; ++i) {
				const newOutputIndex = this.calculateOutputIndex(i - 1, outputIndex, multiplier[i], outputSize[i]);
				outputIndex = newOutputIndex;
			}
			this.setOutput(this.raggedRank, outputIndex, outputTensor, outputShape);
		}
		return [outputShape, outputTensor];
	}

	setOutput(raggedRank, outputIndex, outputTensor, outputShape) {
		if (outputTensor.length === 0) {
			return;
		}
		const valuesBase = this.values;
		const outputBase = outputTensor;
		let elementShape = outputShape.slice();
		elementShape = elementShape.slice(raggedRank + 1);
		const valueElementSize = sizeFromShape(elementShape);
		const outputIndexSize = outputIndex.length;
		// Broadcast the default value to value_element_size.  (We can skip this
		// if defaultValueTensor.size == 1, since we use fill when that's true.)
		let defaultValue = this.defaultValue;
		if (defaultValue.length !== valueElementSize && defaultValue.length !== 1) {
			const srcShape = this.defaultValueShape;
			tidy(() => {
				const defaultValueTensor = reshape$1(defaultValue, srcShape);
				const bCastDefault = broadcastTo(defaultValueTensor, elementShape);
				defaultValue = bCastDefault.dataSync();
			});
		}
		// Loop through the outputIndex array, finding contiguous regions that
		// should be copied.  Once we find the end of a contiguous region, copy it
		// and add any necessary padding (with defaultValue).
		let srcStart = 0; // Start of contiguous region (in values)
		let dstStart = 0; // Destination for contiguous region (in output)
		let dstEnd = 0; // Destination for contiguous region (in output)
		for (let srcI = 0; srcI <= outputIndexSize; ++srcI) {
			// dstI is the destination where the value at srcI should be copied.
			let dstI = srcI < outputIndexSize ? outputIndex[srcI] : -1;
			// If we're still in a contiguous region, then update dstEnd go to the
			// next srcI.
			if (dstI === dstEnd) {
				++dstEnd;
				continue;
			}
			// We found the end of contiguous region.  This can be because we found
			// a gap (dstI > dstEnd), or a source value that shouldn't be copied
			// because it's out-of-bounds (dstI == -1), or the end of the tensor
			// (dstI === -1).
			if (dstStart < dstEnd) {
				// Copy the contiguous region.
				const src = valuesBase.subarray(srcStart * valueElementSize);
				const dst = outputBase.subarray(dstStart * valueElementSize);
				const nVals = (dstEnd - dstStart) * valueElementSize;
				copyArray(dst, src, nVals);
			}
			// Add any necessary padding (w/ defaultValue).
			if (srcI >= outputIndexSize) {
				// We reached the end of values: pad to the end of output.
				const outputSize = outputTensor.length;
				dstI = Math.floor(outputSize / valueElementSize);
			}
			if (dstI > dstEnd) {
				if (this.defaultValue.length === 1) {
					outputBase
						.subarray(dstEnd * valueElementSize, dstI * valueElementSize)
						.fill(this.defaultValue[0]);
					dstEnd = dstI;
				} else {
					while (dstI > dstEnd) {
						const dst = outputBase.slice(dstEnd * valueElementSize);
						copyArray(dst, defaultValue, valueElementSize);
						++dstEnd;
					}
				}
			}
			// Update indices.
			if (dstI < 0) {
				// srcI should be skipped -- leave it out of the contiguous region.
				srcStart = srcI + 1;
				dstStart = dstEnd;
			} else {
				// srcI should be copied -- include it in the contiguous region.
				srcStart = srcI;
				dstStart = dstEnd;
				dstEnd = dstStart + 1;
			}
		}
	}
}

function copyArray(dst, src, size) {
	for (let i = 0; i < size; i++) {
		dst[i] = src[i];
	}
}

function makeShape(shape, isPartial) {
	const out = [];
	for (let dim of shape) {
		if (dim < 0) {
			if (!isPartial) {
				throw new Error(`Dimension ${dim} must be >= 0`);
			}
			if (dim < -1) {
				throw new Error(`Dimension ${dim} must be >= -1`);
			}
			dim = -1;
		}
		out.push(dim);
	}
	return out;
}

function raggedTensorToTensorImpl(shape, shapesShape, values, valuesShape, valuesDType, defaultValue, defaultValueShape, rowPartitionValues, rowPartitionValuesShapes, rowPartitionTypes) {
	return new RaggedTensorToTensorOp(shape, shapesShape, values, valuesShape, valuesDType, defaultValue, defaultValueShape, rowPartitionValues, rowPartitionValuesShapes, rowPartitionTypes)
		.compute();
}

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
function rangeImpl(start, stop, step, dtype) {
	const sameStartStop = start === stop;
	const increasingRangeNegativeStep = start < stop && step < 0;
	const decreasingRangePositiveStep = stop < start && step > 1;
	if (sameStartStop || increasingRangeNegativeStep ||
		decreasingRangePositiveStep) {
		return makeZerosTypedArray(0, dtype);
	}
	const numElements = Math.abs(Math.ceil((stop - start) / step));
	const values = makeZerosTypedArray(numElements, dtype);
	if (stop < start && step === 1) {
		// Auto adjust the step's sign if it hasn't been set
		// (or was set to 1)
		step = -1;
	}
	values[0] = start;
	for (let i = 1; i < values.length; i++) {
		values[i] = values[i - 1] + step;
	}
	return values;
}

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the License);
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an AS IS BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
const rsqrtImpl = createSimpleUnaryImpl((xi) => 1 / Math.sqrt(xi));

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
function scatterImpl(indices, updates, shape, outputSize, sliceSize, numUpdates, sliceRank, strides, defaultValue, sumDupeIndices) {
	const flattenShape = [outputSize / sliceSize, sliceSize];
	const indicesData = indices.values;
	const updatesData = updates.values;
	if (outputSize === 0) {
		return buffer(shape, updates.dtype);
	}
	const outBuf = (defaultValue instanceof TensorBuffer) ?
		defaultValue :
		buffer(flattenShape, updates.dtype);
	if (typeof defaultValue === 'string') {
		outBuf.values.fill(defaultValue);
	} else if (typeof defaultValue === 'number') {
		outBuf.values.fill(defaultValue);
	} else if (typeof defaultValue === 'boolean') {
		outBuf.values.fill(+defaultValue);
	}
	for (let i = 0; i < numUpdates; i++) {
		const index = [];
		let flattenIndex = 0;
		for (let j = 0; j < sliceRank; j++) {
			const dim = indicesData[i * sliceRank + j];
			index.push(dim);
			flattenIndex += dim * strides[j];
		}
		if (flattenIndex < 0 || flattenIndex >= outputSize / sliceSize) {
			throw new Error(`Invalid indices: ${index} does not index into ${shape}`);
		}
		for (let k = 0; k < sliceSize; k++) {
			if (sumDupeIndices) {
				outBuf.values[flattenIndex * sliceSize + k] +=
					updatesData[i * sliceSize + k];
			} else {
				outBuf.values[flattenIndex * sliceSize + k] = updates.rank === 0 ?
					updatesData[0] :
					updatesData[i * sliceSize + k];
			}
		}
	}
	return outBuf;
}

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the License);
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an AS IS BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
const sigmoidImpl = createSimpleUnaryImpl((xi) => 1 / (1 + Math.exp(-xi)));
const sigmoid = unaryKernelFunc(Sigmoid, (xi) => 1 / (1 + Math.exp(-xi)));
const sigmoidConfig = {
	kernelName: Sigmoid,
	backendName: 'cpu',
	kernelFunc: sigmoid,
};

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
function sliceImpl(vals, begin, size, shape, dtype) {
	const isContinous = isSliceContinous(shape, begin, size);
	const length = sizeFromShape(size);
	const xStrides = computeStrides(shape);
	if (isContinous) {
		const flatOffset = computeFlatOffset(begin, xStrides);
		if (dtype === 'string') {
			return vals.slice(flatOffset, flatOffset + length);
		}
		return vals.subarray(flatOffset, flatOffset + length);
	}
	const decodedData = dtype === 'string' ?
		fromUint8ToStringArray(vals) :
		vals;
	const inBuf = buffer(shape, dtype, decodedData);
	const outBuf = buffer(size, dtype);
	for (let i = 0; i < outBuf.size; ++i) {
		const outLoc = outBuf.indexToLoc(i);
		const inLoc = outLoc.map((idx, j) => idx + begin[j]);
		outBuf.set(inBuf.get(...inLoc), ...outLoc);
	}
	if (dtype === 'string') {
		return fromStringArrayToUint8(outBuf.values);
	}
	return outBuf.values;
}

function slice(args) {
	const {inputs, backend, attrs} = args;
	const {x} = inputs;
	const {begin, size} = attrs;
	assertNotComplex(x, 'slice');
	const [$begin, $size] = parseSliceParams(x, begin, size);
	assertParamsValid(x, $begin, $size);
	const vals = backend.data.get(x.dataId).values;
	const outVals = sliceImpl(vals, $begin, $size, x.shape, x.dtype);
	return backend.makeTensorInfo($size, x.dtype, outVals);
}

const sliceConfig = {
	kernelName: Slice,
	backendName: 'cpu',
	kernelFunc: slice
};

/**
 * @license
 * Copyright 2021 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
function sparseFillEmptyRowsImpl(indices, indicesShape, indicesDType, values, valuesDType, denseShape, defaultValue) {
	const indicesCount = indicesShape[0];
	const denseRows = denseShape[0];
	const emptyRowIndicator = new Array(denseRows);
	const reverseIndexMap = new Array(indicesCount);
	const rank = indicesShape[1];
	if (denseRows === 0) {
		if (indicesCount !== 0) {
			throw new Error(getSparseFillEmptyRowsIndicesDenseShapeMismatch(indicesCount));
		}
		const outputIndices = getArrayFromDType(indicesDType, 0);
		const outputValues = getArrayFromDType(valuesDType, 0);
		return [
			outputIndices, [0, rank], outputValues, emptyRowIndicator, reverseIndexMap
		];
	}
	let rowsAreOrdered = true;
	let lastIndicesRow = 0;
	const csrOffset = new Array(denseRows).fill(0);
	for (let i = 0; i < indicesCount; ++i) {
		// indices is a 2d tensor with shape of [N, rank]
		const row = indices[i * rank];
		if (row < 0) {
			throw new Error(getSparseFillEmptyRowsNegativeIndexErrorMessage(i, row));
		}
		if (row >= denseRows) {
			throw new Error(getSparseFillEmptyRowsOutOfRangeIndexErrorMessage(i, row, denseRows));
		}
		++csrOffset[row];
		rowsAreOrdered = rowsAreOrdered && (row >= lastIndicesRow);
		lastIndicesRow = row;
	}
	let allRowsFull = true;
	for (let row = 0; row < denseRows; ++row) {
		// csrOffset here describes the number of elements in this dense row
		const rowEmpty = (csrOffset[row] === 0);
		emptyRowIndicator[row] = rowEmpty;
		allRowsFull = allRowsFull && !rowEmpty;
		// In filled version, each row has at least one element.
		csrOffset[row] = Math.max(csrOffset[row], 1);
		// Update csrOffset to represent the number of elements up to and
		// including denseRows + 1:
		//  csrOffset[0] == #{elements of row 0}
		//  csrOffset[1] == #{elements of row 1} + #{elements of row 0}
		//  ..
		//  csrOffset[i] == starting index for elements in row i + 1.
		if (row > 0) {
			csrOffset[row] += csrOffset[row - 1];
		}
	}
	if (allRowsFull && rowsAreOrdered) {
		const outputIndices = indices;
		const outputValues = values;
		for (let i = 0; i < indicesCount; ++i) {
			reverseIndexMap[i] = i;
		}
		return [
			outputIndices, [indicesCount, rank], outputValues, emptyRowIndicator,
			reverseIndexMap
		];
	} else {
		const fullIndicesCount = csrOffset[denseRows - 1];
		const outputIndices = getArrayFromDType(indicesDType, fullIndicesCount * rank);
		const outputValues = getArrayFromDType(valuesDType, fullIndicesCount);
		const filledCount = new Array(denseRows).fill(0);
		// Fill in values for rows that are not missing
		for (let i = 0; i < indicesCount; ++i) {
			// indices is a 2d tensor with shape of [N, rank]
			const row = indices[i * rank];
			const offset = filledCount[row];
			const outputI = ((row === 0) ? 0 : csrOffset[row - 1]) + offset;
			filledCount[row]++; // Increment the filled count for this row.
			for (let j = 0; j < rank; ++j) {
				// indices and outputIndices are 2d tensors with shape of [N, rank]
				outputIndices[outputI * rank + j] = indices[i * rank + j];
			}
			outputValues[outputI] = values[i];
			// We'll need this reverse index map to backprop correctly.
			reverseIndexMap[i] = outputI;
		}
		// Fill in values for rows that are missing
		for (let row = 0; row < denseRows; ++row) {
			const rowCount = filledCount[row];
			if (rowCount === 0) { // We haven't filled this row
				const startingIndex = (row === 0) ? 0 : csrOffset[row - 1];
				// Remaining index values were set to zero already.
				// Just need to set the row index in the right location.
				// outputIndices is a 2d tensor with shape of [N, rank]
				outputIndices[startingIndex * rank + 0] = row;
				for (let col = 1; col < rank; ++col) {
					outputIndices[startingIndex * rank + col] = 0;
				}
				outputValues[startingIndex] = defaultValue;
			}
		}
		return [
			outputIndices, [fullIndicesCount, rank], outputValues, emptyRowIndicator,
			reverseIndexMap
		];
	}
}

/**
 * @license
 * Copyright 2021 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
function sparseReshapeImpl(inputIndices, inputIndicesShape, inputDType, inputShape, targetShape) {
	const denseSize = sizeFromShape(inputShape);
	const nnz = inputIndicesShape[0];
	const outputRank = targetShape.length;
	// Compute the output shape. Determine product of specified dimensions, and
	// find the index of the unspecified one.
	const outputShape = [];
	let product = 1;
	let unknownIndex = -1;
	for (let d = 0; d < outputRank; ++d) {
		const size = targetShape[d];
		if (size === -1) {
			if (unknownIndex !== -1) {
				throw new Error(getSparseReshapeMultipleNegativeOneOutputDimErrorMessage(unknownIndex, d));
			}
			unknownIndex = d;
			outputShape.push(1);
		} else {
			if (size < 0) {
				throw new Error(getSparseReshapeNegativeOutputDimErrorMessage(d, size));
			}
			product *= size;
			outputShape.push(size);
		}
	}
	if (unknownIndex !== -1) {
		if (product <= 0) {
			throw new Error(getSparseReshapeEmptyTensorZeroOutputDimErrorMessage());
		}
		const missing = Math.trunc(denseSize / product);
		if (product * missing !== denseSize) {
			throw new Error(getSparseReshapeInputOutputMultipleErrorMessage(inputShape, outputShape));
		}
		outputShape[unknownIndex] = missing;
	}
	const outputSize = sizeFromShape(outputShape);
	if (outputSize !== denseSize) {
		throw new Error(getSparseReshapeInputOutputMismatchErrorMessage(inputShape, outputShape));
	}
	const inputRank = inputShape.length;
	const inputStrides = [];
	if (inputRank > 0) {
		inputStrides[inputRank - 1] = 1;
		for (let d = inputRank - 2; d >= 0; --d) {
			inputStrides[d] = inputStrides[d + 1] * inputShape[d + 1];
		}
	}
	const outputStrides = [];
	if (outputRank > 0) {
		outputStrides[outputRank - 1] = 1;
		for (let d = outputRank - 2; d >= 0; --d) {
			outputStrides[d] = outputStrides[d + 1] * outputShape[d + 1];
		}
	}
	const newIndices = getArrayFromDType(inputDType, nnz * outputRank);
	for (let i = 0; i < nnz; ++i) {
		let id = 0;
		for (let j = 0; j < inputRank; ++j) {
			// inputIndices is a 2d tensor with shape of [nnz, inputRank]
			id += inputIndices[i * inputRank + j] * inputStrides[j];
		}
		for (let j = 0; j < outputRank; ++j) {
			// newIndices is a 2d tensor with shape of [nnz, outputRank]
			newIndices[i * outputRank + j] = Math.trunc(id / outputStrides[j]);
			id %= outputStrides[j];
		}
	}
	return [newIndices, [nnz, outputRank], outputShape];
}

/**
 * @license
 * Copyright 2021 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
function sparseSegmentReductionImpl(input, inputShape, inputDType, indices, segmentIds, isMean = false, defaultValue = 0) {
	const numIndices = indices.length;
	// Flatten the array to two dimensions
	const inputFlat = [inputShape[0], input.length / inputShape[0]];
	const numCol = inputFlat[1];
	// Note that the current implementation assumes that segmentIds values are
	// sorted.
	const lastSegmentIdPlusOne = numIndices > 0 ? segmentIds[numIndices - 1] + 1 : 0;
	const outputRows = lastSegmentIdPlusOne;
	if (outputRows < 0) {
		throw new Error(getSparseSegmentReductionNegativeSegmentIdsErrorMessage());
	}
	const outputShape = inputShape.slice();
	outputShape[0] = outputRows;
	const outputLength = outputShape.reduce((product, value) => product * value, 1);
	// Output array is initialized with the value 0 by default.
	const output = getArrayFromDType(inputDType, outputLength);
	// Note that we do not initialize the output buffer with a default value, so
	// we need to explicitly set missing indices to the default value.
	if (numIndices === 0) {
		if (outputRows > 0) {
			output.fill(defaultValue);
		}
		return [output, outputShape];
	}
	if (outputRows <= 0) {
		throw new Error(getSparseSegmentReductionNegativeSegmentIdsErrorMessage());
	}
	let start = 0, end = 1;
	// Index from which the output is not initialized.
	let uninitializedIndex = 0;
	let outIndex = segmentIds[start];
	while (true) {
		// We initialize nextIndex to 0 to avoid may be uninitialized warning
		let nextIndex = 0;
		if (end < numIndices) {
			nextIndex = segmentIds[end];
			if (outIndex === nextIndex) {
				++end;
				continue;
			}
			// We have a new segment here.  Verify that the segment ids are growing.
			if (outIndex >= nextIndex) {
				throw new Error(getSparseSegmentReductionNonIncreasingSegmentIdsErrorMessage());
			}
		}
		if (outIndex < 0 || outIndex >= outputRows) {
			throw new Error(getSparseSegmentReductionSegmentIdOutOfRangeErrorMessage(outIndex, outputRows));
		}
		// If there is a gap between two indices, we need to set that gap to the
		// default value.
		if (outIndex > uninitializedIndex) {
			output.fill(defaultValue, uninitializedIndex * numCol, outIndex * numCol);
		}
		for (let i = start; i < end; ++i) {
			const index = indices[i];
			if (index < 0 || index >= inputFlat[0]) {
				throw new Error(getSparseSegmentReductionIndicesOutOfRangeErrorMessage(i, indices[i], inputFlat[0]));
			}
			for (let j = 0; j < numCol; j++) {
				output[outIndex * numCol + j] += input[index * numCol + j];
			}
		}
		if (isMean) {
			for (let j = 0; j < numCol; j++) {
				output[outIndex * numCol + j] /= end - start;
			}
		}
		start = end;
		++end;
		uninitializedIndex = outIndex + 1;
		outIndex = nextIndex;
		if (end > numIndices) {
			break;
		}
	}
	// Fill the gap at the end with the default value.
	if (uninitializedIndex < outputRows) {
		output.fill(defaultValue, uninitializedIndex * numCol, outputRows * numCol);
	}
	return [output, outputShape];
}

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the License);
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an AS IS BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
const sqrtImpl = createSimpleUnaryImpl((xi) => Math.sqrt(xi));
const sqrt = unaryKernelFunc(Sqrt, (xi) => Math.sqrt(xi));
const sqrtConfig = {
	kernelName: Sqrt,
	backendName: 'cpu',
	kernelFunc: sqrt,
};

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
const squaredDifferenceImpl = createSimpleBinaryKernelImpl(((a, b) => {
	const diff = a - b;
	return diff * diff;
}));

/**
 * @license
 * Copyright 2023 Google LLC.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
const staticRegexReplaceImpl = createSimpleUnaryImpl((x, attrs) => {
	const {pattern, replaceGlobal, rewrite} = attrs;
	// TODO(mattSoulanille): Don't create a regex each time.
	return x.replace(new RegExp(pattern, replaceGlobal ? 'g' : ''), rewrite);
});

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
function stridedSliceImpl(outShape, xBuf, strides, begin) {
	const outBuf = buffer(outShape, xBuf.dtype);
	for (let i = 0; i < outBuf.size; i++) {
		const loc = outBuf.indexToLoc(i);
		const newLoc = new Array(loc.length);
		for (let j = 0; j < newLoc.length; j++) {
			newLoc[j] = loc[j] * strides[j] + begin[j];
		}
		outBuf.set(xBuf.get(...newLoc), ...loc);
	}
	return outBuf;
}

/**
 * @license
 * Copyright 2021 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * The StringNGramsOp class creates ngrams from ragged string data.
 * The constructor contains all attributes related to the operation such as
 * padding widths and strings, and the compute function can be used to
 * compute the ngrams for different ragged tensor inputs.
 */
class StringNGramsOp {
	constructor(separator, nGramWidths, leftPad, rightPad, padWidth, preserveShortSequences) {
		this.separator = encodeString(separator);
		this.nGramWidths = nGramWidths;
		this.leftPad = encodeString(leftPad);
		this.rightPad = encodeString(rightPad);
		this.padWidth = padWidth;
		this.preserveShort = preserveShortSequences;
	}

	getPadWidth(nGramWidth) {
		// Ngrams can be padded with either a fixed pad width or a dynamic pad
		// width depending on the 'padWidth' arg, but in no case should the padding
		// ever be wider than 'nGramWidth' - 1.
		return Math.min(this.padWidth < 0 ? nGramWidth - 1 : this.padWidth, nGramWidth - 1);
	}

	getNumNGrams(length, nGramWidth) {
		const padWidth = this.getPadWidth(nGramWidth);
		return Math.max(0, ((length + 2 * padWidth) - nGramWidth) + 1);
	}

	createNGrams(data, splitIndex, output, outputStartIndex, numNGrams, nGramWidth) {
		for (let nGramIndex = 0; nGramIndex < numNGrams; ++nGramIndex) {
			const padWidth = this.getPadWidth(nGramWidth);
			const leftPadding = Math.max(0, padWidth - nGramIndex);
			const rightPadding = Math.max(0, padWidth - (numNGrams - (nGramIndex + 1)));
			const numTokens = nGramWidth - (leftPadding + rightPadding);
			const dataStartIndex = splitIndex + (leftPadding > 0 ? 0 : nGramIndex - padWidth);
			// Calculate the total expected size of the nGram so we can reserve the
			// correct amount of space in the string.
			let nGramSize = 0;
			// Size of the left padding.
			nGramSize += leftPadding * this.leftPad.length;
			// Size of the tokens.
			for (let n = 0; n < numTokens; ++n) {
				nGramSize += data[dataStartIndex + n].length;
			}
			// Size of the right padding.
			nGramSize += rightPadding * this.rightPad.length;
			// Size of the separators.
			const numSeparators = leftPadding + rightPadding + numTokens - 1;
			nGramSize += numSeparators * this.separator.length;
			// Build the nGram.
			output[outputStartIndex + nGramIndex] = new Uint8Array(nGramSize);
			const nGram = output[outputStartIndex + nGramIndex];
			let nextNGramIndex = 0;
			const appendToNGram = (str) => str.forEach((value) => nGram[nextNGramIndex++] = value);
			for (let n = 0; n < leftPadding; ++n) {
				appendToNGram(this.leftPad);
				appendToNGram(this.separator);
			}
			// Only output first numTokens - 1 pairs of data and separator
			for (let n = 0; n < numTokens - 1; ++n) {
				appendToNGram(data[dataStartIndex + n]);
				appendToNGram(this.separator);
			}
			// Handle case when there are no tokens or no right padding as these
			// can result in consecutive separators.
			if (numTokens > 0) {
				// If we have tokens, then output last and then pair each separator
				// with the right padding that follows, to ensure nGram ends either with
				// the token or with the right pad.
				appendToNGram(data[dataStartIndex + numTokens - 1]);
				for (let n = 0; n < rightPadding; ++n) {
					appendToNGram(this.separator);
					appendToNGram(this.rightPad);
				}
			} else {
				// If we don't have tokens, then the last item inserted into the nGram
				// has been the separator from the left padding loop above. Hence,
				// output right pad and separator and make sure to finish with a
				// padding, not a separator.
				for (let n = 0; n < rightPadding - 1; ++n) {
					appendToNGram(this.rightPad);
					appendToNGram(this.separator);
				}
				appendToNGram(this.rightPad);
			}
		}
	}

	// Data and splits together form the definition of the ragged tensor,
	// where data is 1 dimensional and contains the values of the tensor
	// and splits denotes the indices at which each row starts.
	compute(data, splits) {
		// Validate that the splits are valid indices into data, only if there are
		// splits specified.
		const inputDataSize = data.length;
		const splitsSize = splits.length;
		if (splitsSize > 0) {
			let prevSplit = splits[0];
			if (prevSplit !== 0) {
				throw new Error(`First split value must be 0, got ${prevSplit}`);
			}
			for (let i = 1; i < splitsSize; ++i) {
				let validSplits = splits[i] >= prevSplit;
				validSplits = validSplits && (splits[i] <= inputDataSize);
				if (!validSplits) {
					throw new Error(`Invalid split value ${splits[i]}, must be in [${prevSplit}, ${inputDataSize}]`);
				}
				prevSplit = splits[i];
			}
			if (prevSplit !== inputDataSize) {
				throw new Error(`Last split value must be data size. Expected ${inputDataSize}, got ${prevSplit}`);
			}
		}
		const numBatchItems = splitsSize - 1;
		const nGramsSplits = getArrayFromDType('int32', splitsSize);
		// If there is no data or size, return an empty ragged tensor.
		if (inputDataSize === 0 || splitsSize === 0) {
			const empty = new Array(inputDataSize);
			for (let i = 0; i <= numBatchItems; ++i) {
				nGramsSplits[i] = 0;
			}
			return [empty, nGramsSplits];
		}
		nGramsSplits[0] = 0;
		for (let i = 1; i <= numBatchItems; ++i) {
			const length = splits[i] - splits[i - 1];
			let numNGrams = 0;
			this.nGramWidths.forEach((nGramWidth) => {
				numNGrams += this.getNumNGrams(length, nGramWidth);
			});
			if (this.preserveShort && length > 0 && numNGrams === 0) {
				numNGrams = 1;
			}
			nGramsSplits[i] = nGramsSplits[i - 1] + numNGrams;
		}
		const nGrams = new Array(nGramsSplits[numBatchItems]);
		for (let i = 0; i < numBatchItems; ++i) {
			const splitIndex = splits[i];
			let outputStartIdx = nGramsSplits[i];
			this.nGramWidths.forEach((nGramWidth) => {
				const length = splits[i + 1] - splits[i];
				const numNGrams = this.getNumNGrams(length, nGramWidth);
				this.createNGrams(data, splitIndex, nGrams, outputStartIdx, numNGrams, nGramWidth);
				outputStartIdx += numNGrams;
			});
			// If we're preserving short sequences, check to see if no sequence was
			// generated by comparing the current output start idx to the original
			// one (nGramSplitsdata). If no ngrams were generated, then they will
			// be equal (since we increment outputStartIdx by numNGrams every
			// time we create a set of ngrams.)
			if (this.preserveShort && outputStartIdx === nGramsSplits[i]) {
				const dataLength = splits[i + 1] - splits[i];
				// One legitimate reason to not have any ngrams when this.preserveShort
				// is true is if the sequence itself is empty. In that case, move on.
				if (dataLength === 0) {
					continue;
				}
				// We don't have to worry about dynamic padding sizes here: if padding
				// was dynamic, every sequence would have had sufficient padding to
				// generate at least one nGram.
				const nGramWidth = dataLength + 2 * this.padWidth;
				const numNGrams = 1;
				this.createNGrams(data, splitIndex, nGrams, outputStartIdx, numNGrams, nGramWidth);
			}
		}
		return [nGrams, nGramsSplits];
	}
}

function stringNGramsImpl(data, dataSplits, separator, nGramWidths, leftPad, rightPad, padWidth, preserveShortSequences) {
	return new StringNGramsOp(separator, nGramWidths, leftPad, rightPad, padWidth, preserveShortSequences)
		.compute(data, dataSplits);
}

/**
 * @license
 * Copyright 2021 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
function split(str, delimiters, skipEmpty, result) {
	if (!str.length) {
		return;
	}
	// When the delimiter is empty, the input is split into individual characters.
	if (delimiters.length === 0) {
		for (let i = 0; i < str.length; ++i) {
			result.push(str.subarray(i, i + 1));
		}
		return;
	}
	// When there is one delimiter, the input is split only at that delimiter.
	if (delimiters.length === 1) {
		const delimiter = delimiters[0];
		let f = str.indexOf(delimiter);
		while (f !== -1) {
			const token = str.subarray(0, f);
			if (!skipEmpty || token.length !== 0) {
				result.push(token);
			}
			str = str.subarray(f + 1);
			f = str.indexOf(delimiter);
		}
		if (!skipEmpty || str.length !== 0) {
			result.push(str);
		}
		return;
	}
	// When there are multiple delimiters, the input is split at every instance
	// one of the delimiters appears.
	let tokenStart = 0;
	for (let i = 0; i < str.length + 1; i++) {
		if ((i === str.length) || (delimiters.indexOf(str[i]) !== -1)) {
			const token = str.subarray(tokenStart, i);
			if (!skipEmpty || token.length !== 0) {
				result.push(token);
			}
			tokenStart = i + 1;
		}
	}
}

function stringSplitImpl(input, delimiter, skipEmpty) {
	const batchSize = input.length;
	// Empty delimiter means split the input character by character.
	const tokens = [];
	let outputSize = 0;
	let maxNumEntries = 0;
	const numIndices = new Array(batchSize);
	for (let i = 0; i < batchSize; ++i) {
		const prevTokensLength = tokens.length;
		split(input[i], delimiter, skipEmpty, tokens);
		const nEntries = tokens.length - prevTokensLength;
		numIndices[i] = nEntries;
		outputSize += nEntries;
		maxNumEntries = Math.max(maxNumEntries, nEntries);
	}
	const indices = getArrayFromDType('int32', outputSize * 2);
	const values = new Array(outputSize);
	const shape = [batchSize, maxNumEntries];
	let c = 0;
	for (let i = 0; i < batchSize; ++i) {
		for (let j = 0; j < numIndices[i]; ++j) {
			// indices is a 2d tensor with shape of [outputSize, 2]
			indices[c * 2] = i;
			indices[c * 2 + 1] = j;
			values[c] = tokens[c];
			++c;
		}
	}
	return [indices, values, shape];
}

/**
 * @license
 * Copyright 2021 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
function stringToHashBucketFastImpl(input, numBuckets) {
	const output = getArrayFromDType('int32', input.length);
	for (let i = 0; i < input.length; ++i) {
		output[i] =
			fingerPrint64(input[i]).modulo(numBuckets).getLowBitsUnsigned();
	}
	return output;
}

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
const subImpl = createSimpleBinaryKernelImpl(((aValue, bValue) => aValue - bValue));
const subComplexImpl = createComplexBinaryKernelImpl(((aReal, aImag, bReal, bImag) => {
	return {real: aReal - bReal, imag: aImag - bImag};
}));
const sub = binaryKernelFunc(Sub, subImpl, subComplexImpl);
const subConfig = {
	kernelName: Sub,
	backendName: 'cpu',
	kernelFunc: sub
};

/**
 * @license
 * Copyright 2019 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/**
 * An implementation of the tile kernel shared between webgl and cpu for string
 * tensors only.
 */
function tileImpl(xBuf, reps) {
	const newShape = new Array(xBuf.rank);
	for (let i = 0; i < newShape.length; i++) {
		newShape[i] = xBuf.shape[i] * reps[i];
	}
	const result = buffer(newShape, xBuf.dtype);
	for (let i = 0; i < result.values.length; ++i) {
		const newLoc = result.indexToLoc(i);
		const originalLoc = new Array(xBuf.rank);
		for (let j = 0; j < originalLoc.length; j++) {
			originalLoc[j] = newLoc[j] % xBuf.shape[j];
		}
		const originalIndex = xBuf.locToIndex(originalLoc);
		result.values[i] = xBuf.values[originalIndex];
	}
	return result;
}

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/** An implementation of the TopK kernel shared between webgl and cpu. */
const comparePair = (a, b) => {
	const valueDiff = b.value - a.value;
	return valueDiff === 0 ? a.index - b.index : valueDiff;
};

/**
 * Partitions array where all elements smaller than the (k+1) smallest element
 * are found to the left of it, and all larger to the right of it.
 * Based on the Floyd-Rivest Algorithm, ref:
 * https://en.wikipedia.org/wiki/Floyd%E2%80%93Rivest_algorithm
 * @param array: Array to partition
 * @param left: Left index for the interval
 * @param right: Right index for the interval
 * @param k: Desired index value, where array[k] is the (k+1)th smallest element
 *           when left = 0
 */
function select$1(array, k, left = 0, right = array.length - 1) {
	while (right > left) {
		// Use select recursively to sample a smaller set of size s
		// the arbitrary constants 600 and 0.5 are used in the original
		// version to minimize execution time.
		if (right - left > 600) {
			const n = right - left + 1;
			const i = k - left + 1;
			const z = Math.log(n);
			const s = 0.5 * Math.exp(2 * z / 3);
			const sd = 0.5 * Math.sqrt(z * s * (n - s) / n) * Math.sign(i - n / 2);
			const newLeft = Math.max(left, Math.floor(k - i * s / n + sd));
			const newRight = Math.min(right, Math.floor(k + (n - i) * s / n + sd));
			select$1(array, k, newLeft, newRight);
		}
		// partition the elements between left and right around t
		const t = array[k];
		let i = left;
		let j = right;
		swap(array, left, k);
		if (comparePair(array[right], t) > 0) {
			swap(array, left, right);
		}
		while (i < j) {
			swap(array, i, j);
			i++;
			j--;
			while (comparePair(array[i], t) < 0) {
				i = i + 1;
			}
			while (comparePair(array[j], t) > 0) {
				j = j - 1;
			}
		}
		if (comparePair(array[left], t) === 0) {
			swap(array, left, j);
		} else {
			j = j + 1;
			swap(array, j, right);
		}
		// Adjust left and right towards the boundaries of the subset
		// containing the (k - left + 1)th smallest element.
		if (j <= k) {
			left = j + 1;
		}
		if (k <= j) {
			right = j - 1;
		}
	}
}

function topKImpl(x, xShape, xDtype, k, sorted) {
	// Reshape into a 2d tensor [batch, lastDim] and compute topk along lastDim.
	const lastDim = xShape[xShape.length - 1];
	const [batch, size] = [x.length / lastDim, lastDim];
	const allTopKVals = getTypedArrayFromDType(xDtype, batch * k);
	const allTopKIndices = getTypedArrayFromDType('int32', batch * k);
	for (let b = 0; b < batch; b++) {
		const offset = b * size;
		const vals = x.subarray(offset, offset + size);
		let valAndInd = new Array(vals.length);
		vals.forEach((value, index) => valAndInd[index] = {value, index});
		if (k < valAndInd.length) {
			select$1(valAndInd, k);
			valAndInd = valAndInd.slice(0, k);
		}
		if (sorted) {
			valAndInd.sort(comparePair);
		}
		const outOffset = b * k;
		const topKVals = allTopKVals.subarray(outOffset, outOffset + k);
		const topKIndices = allTopKIndices.subarray(outOffset, outOffset + k);
		for (let i = 0; i < k; i++) {
			topKVals[i] = valAndInd[i].value;
			topKIndices[i] = valAndInd[i].index;
		}
	}
	// Reshape back to the original input shape, except that the last
	// dimension is k.
	const outputShape = xShape.slice();
	outputShape[outputShape.length - 1] = k;
	return [
		buffer(outputShape, xDtype, allTopKVals),
		buffer(outputShape, 'int32', allTopKIndices)
	];
}

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
function uniqueImpl(values, axis, shape, dtype) {
	// Normalize and validate axis.
	const $axis = parseAxisParam(axis, shape)[0];
	// Calculate the new shape that is suitable for extracting data along the
	// given axis.
	//
	// The rank is 3.
	// The size of the 1st dimension is the size of all the axes < the given axis.
	// The size of the 2nd dimension is the same as the size of the given axis.
	// The size of the 3rd dimension is the size of all the axes > the given axis.
	//
	// For example, for a 4D tensor with shape=[2, 3, 5, 4] and axis=2, the
	// newShape would be: [2*3, 5, 4].
	//
	// Note that this is not the final output shape. This will be the shape for an
	// intermediate TensorBuffer (see inputBuffer below) to allow us to extract
	// values along the given axis. To demonstrate how it works, consider the
	// following example:
	//
	// Input: a 3D tensor, with shape [1, 2, 3]
	// [
	//   [
	//      [1,2,3],
	//      [4,5,6]
	//   ]
	// ]
	// Axis: 2 (the last axis).
	// Along axis 2, we expect to extract 3 tensors: [1,4], [2,5], [3,6].
	//
	// For this example, newShape would be: [2, 3, 1], where 2 is calculated from
	// 1*2. The re-shaped data would look like:
	//
	// [
	//   [
	//     [1], [2], [3]
	//   ],
	//   [
	//     [4], [5], [6]
	//   ]
	// ]
	//
	// Then, we can construct a 3-level nested loop by the following dimension
	// order to extract the values along the axis (dimension1):
	// i: dimension1       // 0,1,2 (newShape[1])
	//   m: dimension0     // 0,1   (newShape[0])
	//     n: dimension2   // 0     (newShape[2])
	//
	//                       m, i, n
	//                      ---------
	// Iteration 0: data at [0, 0, 0] => "1"
	// Iteration 1: data at [1, 0, 0] => "4"
	// We got [1,4].
	// Iteration 2: data at [0, 1, 0] => "2"
	// Iteration 3: data at [1, 1, 0] => "5"
	// We got [2,5].
	// Iteration 4: data at [0, 2, 0] => "3"
	// Iteration 5: data at [1, 2, 0] => "6"
	// We got [3,6].
	const newShape = [1, shape[0], 1];
	for (let i = 0; i < $axis; i++) {
		newShape[0] *= shape[i];
	}
	newShape[1] = shape[$axis];
	for (let i = $axis + 1; i < shape.length; i++) {
		newShape[2] *= shape[i];
	}
	// A map from unique elements (their string representations) to their values
	// in "indices" (below).
	const uniqueElements = new Map();
	// The indices of each unique element in the original tensor along the given
	// axis. It is 1D and has the same size as the given axis.
	const indices = new Int32Array(shape[$axis]);
	// Create a buffer so we can easily extract value at a given location.
	const inputBuffer = new TensorBuffer(newShape, dtype, values);
	// The indices along the given axis that have unique elements. This is a
	// de-duped version of "indices" above.
	const uniqueIndices = [];
	const is1DTensor = newShape[0] === 1 && newShape[2] === 1;
	for (let i = 0; i < shape[$axis]; i++) {
		// Extract values along the axis.
		let element;
		if (is1DTensor) {
			// Fast path for 1D tensor input.
			element = values[i].toString();
		} else {
			const axisValues = [];
			for (let m = 0; m < newShape[0]; m++) {
				for (let n = 0; n < newShape[2]; n++) {
					axisValues.push(inputBuffer.get(m, i, n));
				}
			}
			element = axisValues.join(',');
		}
		// Dedup and update various indices.
		const existingIndex = uniqueElements.get(element);
		if (existingIndex != null) {
			indices[i] = existingIndex;
		} else {
			const uniqueIndex = uniqueElements.size;
			uniqueElements.set(element, uniqueIndex);
			indices[i] = uniqueIndex;
			uniqueIndices.push(i);
		}
	}
	// Now we know where each of the unique elements are located along the axis
	// (uniqueIndices). Extract them from input buffer and store them in the
	// output buffer.
	const outputTmpShape = newShape.slice();
	outputTmpShape[1] = uniqueElements.size;
	const outputBuffer = new TensorBuffer(outputTmpShape, dtype);
	uniqueIndices.forEach((uniqueElementIndex, i) => {
		for (let m = 0; m < newShape[0]; m++) {
			for (let n = 0; n < newShape[2]; n++) {
				outputBuffer.set(inputBuffer.get(m, uniqueElementIndex, n), m, i, n);
			}
		}
	});
	// The output shape can be calculated from the input shape with the size of
	// the given axis replaced by the number of unique elements along that axis.
	const outputShape = shape.slice();
	outputShape[$axis] = outputTmpShape[1];
	return {
		outputValues: outputBuffer.values,
		outputShape,
		indices,
	};
}

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
// Shared functionality among backends.

var shared = /*#__PURE__*/Object.freeze({
	__proto__: null,
	addImpl: addImpl,
	bincountImpl: bincountImpl,
	bincountReduceImpl: bincountReduceImpl,
	bitwiseAndImpl: bitwiseAndImpl,
	castImpl: castImpl,
	ceilImpl: ceilImpl,
	concatImpl: concatImpl,
	equalImpl: equalImpl,
	expImpl: expImpl,
	expm1Impl: expm1Impl,
	floorDivImpl: floorDivImpl,
	floorImpl: floorImpl,
	gatherNdImpl: gatherNdImpl,
	gatherV2Impl: gatherV2Impl,
	greaterEqualImpl: greaterEqualImpl,
	greaterImpl: greaterImpl,
	lessEqualImpl: lessEqualImpl,
	lessImpl: lessImpl,
	linSpaceImpl: linSpaceImpl,
	logImpl: logImpl,
	maxImpl: maxImpl,
	maximumImpl: maximumImpl,
	minimumImpl: minimumImpl,
	multiplyImpl: multiplyImpl,
	negImpl: negImpl,
	notEqualImpl: notEqualImpl,
	prodImpl: prodImpl,
	raggedGatherImpl: raggedGatherImpl,
	raggedRangeImpl: raggedRangeImpl,
	raggedTensorToTensorImpl: raggedTensorToTensorImpl,
	rangeImpl: rangeImpl,
	rsqrtImpl: rsqrtImpl,
	scatterImpl: scatterImpl,
	sigmoidImpl: sigmoidImpl,
	simpleAbsImpl: simpleAbsImpl,
	sliceImpl: sliceImpl,
	sparseFillEmptyRowsImpl: sparseFillEmptyRowsImpl,
	sparseReshapeImpl: sparseReshapeImpl,
	sparseSegmentReductionImpl: sparseSegmentReductionImpl,
	sqrtImpl: sqrtImpl,
	squaredDifferenceImpl: squaredDifferenceImpl,
	staticRegexReplaceImpl: staticRegexReplaceImpl,
	stridedSliceImpl: stridedSliceImpl,
	stringNGramsImpl: stringNGramsImpl,
	stringSplitImpl: stringSplitImpl,
	stringToHashBucketFastImpl: stringToHashBucketFastImpl,
	subImpl: subImpl,
	tileImpl: tileImpl,
	topKImpl: topKImpl,
	transposeImpl: transposeImpl,
	uniqueImpl: uniqueImpl
});

/** @license See the LICENSE file. */
// This code is auto-generated, do not modify this file!
const version = '4.22.0';

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
/*
 * base.ts contains all the exports from tfjs-backend-cpu
 * without auto-kernel registration
 */
// Side effects for default initialization of MathBackendCPU
registerBackend('cpu', () => new MathBackendCPU(), 1 /* priority */);

/**
 * @license
 * Copyright 2021 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
function stringToHashBucketFast(args) {
	const {inputs, backend, attrs} = args;
	const {numBuckets} = attrs;
	const {input} = inputs;
	if (input.dtype !== 'string') {
		throw new Error('Input must be of datatype string');
	}
	if (numBuckets <= 0) {
		throw new Error(`Number of buckets must be at least 1`);
	}
	const $input = backend.data.get(input.dataId).values;
	const output = stringToHashBucketFastImpl($input, numBuckets);
	return backend.makeTensorInfo(input.shape, 'int32', output);
}

const stringToHashBucketFastConfig = {
	kernelName: StringToHashBucketFast,
	backendName: 'cpu',
	kernelFunc: stringToHashBucketFast,
};

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
function reshape(args) {
	const {inputs, backend, attrs} = args;
	const {x} = inputs;
	const {shape} = attrs;
	const xSize = sizeFromShape(x.shape);
	const $shape = inferFromImplicitShape(shape, xSize);
	const $xSize = sizeFromShape($shape);
	assert(xSize === $xSize, () => `The new shape (${$shape}) has ${$xSize} elements and the old ` +
		`shape (${x.shape}) has ${xSize} elements. The new shape and old ` +
		`shape must have the same number of elements.`);
	backend.incRef(x.dataId);
	const xData = backend.data.get(x.dataId);
	if (xData.complexTensorInfos != null) {
		const real = xData.complexTensorInfos.real;
		const imag = xData.complexTensorInfos.imag;
		real.shape = $shape;
		imag.shape = $shape;
	}
	return {dataId: x.dataId, shape: $shape, dtype: x.dtype};
}

const reshapeConfig = {
	kernelName: Reshape,
	backendName: 'cpu',
	kernelFunc: reshape
};

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
function gatherV2(args) {
	const {inputs, backend, attrs} = args;
	const {x, indices} = inputs;
	const {axis, batchDims} = attrs;
	assertNotComplex([x, indices], 'gatherV2');
	// Throw error when any index is out of bound.
	const parsedAxis = parseAxisParam(axis, x.shape)[0];
	const indicesVals = backend.data.get(indices.dataId).values;
	const axisDim = x.shape[parsedAxis];
	for (let i = 0; i < indicesVals.length; ++i) {
		const index = indicesVals[i];
		assert(index <= axisDim - 1 && index >= 0, () => `GatherV2: the index value ${index} is not in [0, ${axisDim - 1}]`);
	}
	let $batchDims = batchDims;
	if (batchDims == null) {
		$batchDims = 0;
	}
	const indicesSize = sizeFromShape(indices.shape);
	const shapeInfo = collectGatherOpShapeInfo(x, indices, parsedAxis, $batchDims);
	const flattenX = reshape({
		inputs: {x},
		backend,
		attrs: {
			shape: [
				shapeInfo.batchSize, shapeInfo.outerSize, shapeInfo.dimSize,
				shapeInfo.sliceSize
			]
		}
	});
	const flattenIndex = reshape({
		inputs: {x: indices},
		backend,
		attrs: {shape: [shapeInfo.batchSize, indicesSize / shapeInfo.batchSize]}
	});
	const flattenOutputShape = [
		shapeInfo.batchSize, shapeInfo.outerSize, indicesSize / shapeInfo.batchSize,
		shapeInfo.sliceSize
	];
	const indicesBuf = backend.bufferSync(flattenIndex);
	const xBuf = backend.bufferSync(flattenX);
	const outBuf = gatherV2Impl(xBuf, indicesBuf, flattenOutputShape);
	backend.disposeIntermediateTensorInfo(flattenX);
	backend.disposeIntermediateTensorInfo(flattenIndex);
	return backend.makeTensorInfo(shapeInfo.outputShape, outBuf.dtype, outBuf.values);
}

const gatherV2Config = {
	kernelName: GatherV2,
	backendName: 'cpu',
	kernelFunc: gatherV2
};

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the License);
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an AS IS BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
function batchMatMul(args) {
	const {inputs, backend, attrs} = args;
	const {a, b} = inputs;
	const {transposeA, transposeB} = attrs;
	assertNotComplex([a, b], 'matMul');
	const aRank = a.shape.length;
	const bRank = b.shape.length;
	const innerShapeA = transposeA ? a.shape[aRank - 2] : a.shape[aRank - 1];
	const innerShapeB = transposeB ? b.shape[bRank - 1] : b.shape[bRank - 2];
	const outerShapeA = transposeA ? a.shape[aRank - 1] : a.shape[aRank - 2];
	const outerShapeB = transposeB ? b.shape[bRank - 2] : b.shape[bRank - 1];
	const outerDimsA = a.shape.slice(0, -2);
	const outerDimsB = b.shape.slice(0, -2);
	const batchDimA = sizeFromShape(outerDimsA);
	const batchDimB = sizeFromShape(outerDimsB);
	const outShapeOuterDims = assertAndGetBroadcastShape(a.shape.slice(0, -2), b.shape.slice(0, -2));
	const outShape = outShapeOuterDims.concat([outerShapeA, outerShapeB]);
	assert(innerShapeA === innerShapeB, () => `Error in matMul: inner shapes (${innerShapeA}) and (` +
		`${innerShapeB}) of Tensors with shapes ${a.shape} and ` +
		`${b.shape} and transposeA=${transposeA}` +
		` and transposeB=${transposeB} must match.`);
	const a3dShape = transposeA ? [batchDimA, innerShapeA, outerShapeA] :
		[batchDimA, outerShapeA, innerShapeA];
	const b3dShape = transposeB ? [batchDimB, outerShapeB, innerShapeB] :
		[batchDimB, innerShapeB, outerShapeB];
	// The rest of the implementation is designed to operate on rank-3 tensors
	const a3d = reshape({inputs: {x: a}, backend, attrs: {shape: a3dShape}});
	const b3d = reshape({inputs: {x: b}, backend, attrs: {shape: b3dShape}});
	const sharedDim = transposeA ? a3d.shape[1] : a3d.shape[2];
	const leftDim = transposeA ? a3d.shape[2] : a3d.shape[1];
	const rightDim = transposeB ? b3d.shape[1] : b3d.shape[2];
	const batchDim = Math.max(batchDimA, batchDimB);
	const a3dValues = backend.data.get(a3d.dataId).values;
	const b3dValues = backend.data.get(b3d.dataId).values;
	const a3dStrides = computeStrides(a3d.shape);
	const b3dStrides = computeStrides(b3d.shape);
	const [aBatch, aOuterStep, aInnerStep] = transposeA ?
		[a3dStrides[0], 1, a3dStrides[1]] :
		[a3dStrides[0], a3dStrides[1], 1];
	const [bInnerStep, bOuterStep, bBatch] = transposeB ?
		[1, b3dStrides[1], b3dStrides[0]] :
		[b3dStrides[1], 1, b3dStrides[0]];
	const size = leftDim * rightDim;
	const result = buffer([batchDim, leftDim, rightDim], a3d.dtype);
	const resVals = result.values;
	const blockSize = backend.blockSize;
	for (let bi = 0; bi < batchDim; bi++) {
		const batchIndexA = bi % batchDimA;
		const batchIndexB = bi % batchDimB;
		for (let i0 = 0; i0 < leftDim; i0 += blockSize) {
			// for when blockSize doesn't evenly divide the input
			const iBlock = Math.min(i0 + blockSize, leftDim);
			for (let j0 = 0; j0 < rightDim; j0 += blockSize) {
				const jBlock = Math.min(j0 + blockSize, rightDim);
				for (let k0 = 0; k0 < sharedDim; k0 += blockSize) {
					const kBlock = Math.min(k0 + blockSize, sharedDim);
					for (let i = i0; i < iBlock; i++) {
						for (let j = j0; j < jBlock; j++) {
							let sum = 0.0;
							for (let k = k0; k < kBlock; k++) {
								const aVal =
									// tslint:disable-next-line: max-line-length
									a3dValues[batchIndexA * aBatch + i * aOuterStep + k * aInnerStep];
								const bVal =
									// tslint:disable-next-line: max-line-length
									b3dValues[k * bInnerStep + j * bOuterStep + batchIndexB * bBatch];
								sum += aVal * bVal;
							}
							resVals[bi * size + (i * rightDim + j)] += sum;
						}
					}
				}
			}
		}
	}
	backend.disposeIntermediateTensorInfo(a3d);
	backend.disposeIntermediateTensorInfo(b3d);
	// set correct shape on output.
	return backend.makeTensorInfo(outShape, result.dtype, result.values);
}

const batchMatMulConfig = {
	kernelName: BatchMatMul,
	backendName: 'cpu',
	kernelFunc: batchMatMul,
};

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the License);
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an AS IS BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
const relu = unaryKernelFunc(Relu, (xi) => Math.max(0, xi));
const reluConfig = {
	kernelName: Relu,
	backendName: 'cpu',
	kernelFunc: relu,
};

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
const realDivImpl = createSimpleBinaryKernelImpl((a, b) => a / b);
const div = binaryKernelFunc(RealDiv, realDivImpl);
const realDivConfig = {
	kernelName: RealDiv,
	backendName: 'cpu',
	kernelFunc: div
};

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the License);
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an AS IS BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
const elu = unaryKernelFunc(Elu, (xi) => xi >= 0 ? xi : (Math.exp(xi) - 1));

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
function leakyRelu(args) {
	const {inputs, backend, attrs} = args;
	const {x} = inputs;
	const {alpha} = attrs;
	assertNotComplex([x], 'leakyRelu');
	const xSize = sizeFromShape(x.shape);
	const xVals = backend.data.get(x.dataId).values;
	const outVals = getTypedArrayFromDType('float32', xSize);
	for (let i = 0; i < xVals.length; i++) {
		outVals[i] = xVals[i] < 0 ? alpha * xVals[i] : xVals[i];
	}
	return backend.makeTensorInfo(x.shape, 'float32', outVals);
}

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the License);
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an AS IS BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
const preluImpl = createSimpleBinaryKernelImpl((xValue, aValue) => xValue < 0 ? aValue * xValue : xValue);

function prelu(args) {
	const {inputs, backend} = args;
	const {x, alpha} = inputs;
	assertNotComplex([x, alpha], 'prelu');
	const aVals = backend.data.get(x.dataId).values;
	const bVals = backend.data.get(alpha.dataId).values;
	const [resultData, resultShape] = preluImpl(x.shape, alpha.shape, aVals, bVals, 'float32');
	return backend.makeTensorInfo(resultShape, 'float32', resultData);
}

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the License);
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an AS IS BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
const relu6 = unaryKernelFunc(Relu6, (xi) => Math.min(Math.max(0, xi), 6));

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
function applyActivation(backend, x, activation, preluActivationWeights, leakyreluAlpha) {
	if (activation === 'linear') {
		return identity({inputs: {x}, backend});
	} else if (activation === 'relu') {
		return relu({inputs: {x}, backend});
	} else if (activation === 'elu') {
		return elu({inputs: {x}, backend});
	} else if (activation === 'relu6') {
		return relu6({inputs: {x}, backend});
	} else if (activation === 'prelu') {
		return prelu({inputs: {x, alpha: preluActivationWeights}, backend});
	} else if (activation === 'leakyrelu') {
		return leakyRelu({inputs: {x}, backend, attrs: {alpha: leakyreluAlpha}});
	} else if (activation === 'sigmoid') {
		return sigmoid({inputs: {x}, backend});
	}
	throw new Error(`Activation ${activation} has not been implemented for the CPU backend.`);
}

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the License);
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an AS IS BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
function _fusedMatMul(args) {
	const {inputs, backend, attrs} = args;
	const {a, b, bias, preluActivationWeights} = inputs;
	const {transposeA, transposeB, activation, leakyreluAlpha} = attrs;
	let current;
	let addRes;
	let activationRes;
	const intermediates = [];
	const matMulRes = batchMatMul({inputs: {a, b}, attrs: {transposeA, transposeB}, backend});
	current = matMulRes;
	if (bias) {
		addRes = add({inputs: {a: current, b: bias}, backend});
		intermediates.push(current);
		current = addRes;
	}
	if (activation) {
		activationRes = applyActivation(backend, current, activation, preluActivationWeights, leakyreluAlpha);
		intermediates.push(current);
		current = activationRes;
	}
	for (const i of intermediates) {
		backend.disposeIntermediateTensorInfo(i);
	}
	return current;
}

const _fusedMatMulConfig = {
	kernelName: _FusedMatMul,
	backendName: 'cpu',
	kernelFunc: _fusedMatMul,
};

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the License);
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an AS IS BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
const clipByValue = unaryKernelFunc(ClipByValue, (xi, attrs) => {
	const clipAttrs = attrs;
	if (xi > clipAttrs.clipValueMax) {
		return clipAttrs.clipValueMax;
	}
	return xi < clipAttrs.clipValueMin ? clipAttrs.clipValueMin : xi;
});
const clipByValueConfig = {
	kernelName: ClipByValue,
	backendName: 'cpu',
	kernelFunc: clipByValue,
};

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the License);
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an AS IS BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
const log1p = unaryKernelFunc(Log1p, (xi) => Math.log1p(xi));
const log1pConfig = {
	kernelName: Log1p,
	backendName: 'cpu',
	kernelFunc: log1p,
};

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
function sum(args) {
	const {inputs, backend, attrs} = args;
	const {x} = inputs;
	const {axis, keepDims} = attrs;
	assertNotComplex(x, 'sum');
	let $x;
	if (x.dtype === 'bool') {
		$x = cast({inputs: {x}, backend, attrs: {dtype: 'int32'}});
	} else {
		$x = identity({inputs: {x}, backend});
	}
	const xRank = $x.shape.length;
	const axes = parseAxisParam(axis, $x.shape);
	const permutation = getAxesPermutation(axes, xRank);
	let reductionAxes = axes;
	let permutedX = $x;
	if (permutation != null) {
		permutedX =
			transpose({inputs: {x: $x}, backend, attrs: {perm: permutation}});
		reductionAxes = getInnerMostAxes(reductionAxes.length, xRank);
	}
	assertAxesAreInnerMostDims('sum', reductionAxes, permutedX.shape.length);
	const [outShape, reduceShape] = computeOutAndReduceShapes(permutedX.shape, reductionAxes);
	const resultDtype = upcastType(permutedX.dtype, 'int32');
	let result = zeros(backend, outShape, resultDtype);
	const reduceSize = sizeFromShape(reduceShape);
	const vals = backend.data.get(result.dataId).values;
	const aVals = backend.data.get(permutedX.dataId).values;
	for (let i = 0; i < vals.length; ++i) {
		const offset = i * reduceSize;
		let sum = 0;
		for (let j = 0; j < reduceSize; ++j) {
			sum += aVals[offset + j];
		}
		vals[i] = sum;
	}
	if (keepDims) {
		const newShape = expandShapeToKeepDim(result.shape, axes);
		const oldResult = result;
		result = reshape({inputs: {x: result}, backend, attrs: {shape: newShape}});
		backend.disposeIntermediateTensorInfo(oldResult);
	}
	backend.disposeIntermediateTensorInfo($x);
	if (permutation != null) {
		backend.disposeIntermediateTensorInfo(permutedX);
	}
	return result;
}

const sumConfig = {
	kernelName: Sum,
	backendName: 'cpu',
	kernelFunc: sum
};

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
function mean(args) {
	const {inputs, backend, attrs} = args;
	const {x} = inputs;
	const {axis, keepDims} = attrs;
	const axes = parseAxisParam(axis, x.shape);
	const shapes = computeOutAndReduceShapes(x.shape, axes);
	const reduceShape = shapes[1];
	const reduceSize = sizeFromShape(reduceShape);
	const toDispose = [];
	const reduceSizeScalar = backend.makeTensorInfo([], 'float32', new Float32Array([reduceSize]));
	toDispose.push(reduceSizeScalar);
	const $x = cast({inputs: {x}, backend, attrs: {dtype: 'float32'}});
	toDispose.push($x);
	const res = div({inputs: {a: $x, b: reduceSizeScalar}, backend});
	toDispose.push(res);
	const result = sum({inputs: {x: res}, backend, attrs: {axis, keepDims}});
	toDispose.forEach(t => backend.disposeIntermediateTensorInfo(t));
	return result;
}

const meanConfig = {
	kernelName: Mean,
	backendName: 'cpu',
	kernelFunc: mean
};

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
function fill(args) {
	const {backend, attrs} = args;
	const {shape, value, dtype} = attrs;
	const $dtype = dtype || inferDtype(value);
	const values = getArrayFromDType($dtype, sizeFromShape(shape));
	fillValues(values, value, $dtype);
	return backend.makeTensorInfo(shape, $dtype, values);
}

function fillValues(values, value, dtype) {
	if (dtype === 'string') {
		values.fill(value);
	} else {
		values.fill(value);
	}
}

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
function imag(args) {
	const {inputs, backend} = args;
	const {input} = inputs;
	const imag = backend.data.get(input.dataId).complexTensorInfos.imag;
	const imagVal = backend.data.get(imag.dataId).values;
	// When complex tensor is disposed, its underlying parts will be disposed too.
	// Make new tensor out of the imag value of the complex. This makes sure the
	// value is still accessible even if complex tensor is disposed.
	return backend.makeTensorInfo(imag.shape, imag.dtype, imagVal);
}

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
function zerosLike(args) {
	const {inputs, backend} = args;
	const {x} = inputs;
	if (x.dtype === 'string') {
		throw new Error('zerosLike is not supported for string tensors');
	} else if (x.dtype === 'complex64') {
		const realPart = real({inputs: {input: x}, backend});
		const r = zerosLike({inputs: {x: realPart}, backend});
		const imagPart = imag({inputs: {input: x}, backend});
		const i = zerosLike({inputs: {x: imagPart}, backend});
		const result = complex({inputs: {real: r, imag: i}, backend});
		backend.disposeIntermediateTensorInfo(realPart);
		backend.disposeIntermediateTensorInfo(r);
		backend.disposeIntermediateTensorInfo(imagPart);
		backend.disposeIntermediateTensorInfo(i);
		return result;
	} else {
		return fill({backend, attrs: {shape: x.shape, value: 0, dtype: x.dtype}});
	}
}

const zerosLikeConfig = {
	kernelName: ZerosLike,
	backendName: 'cpu',
	kernelFunc: zerosLike
};

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
function onesLike(args) {
	const {inputs, backend} = args;
	const {x} = inputs;
	if (x.dtype === 'string') {
		throw new Error('onesLike is not supported for string tensors');
	} else if (x.dtype === 'complex64') {
		const realPart = real({inputs: {input: x}, backend});
		const r = onesLike({inputs: {x: realPart}, backend});
		const imagPart = imag({inputs: {input: x}, backend});
		const i = zerosLike({inputs: {x: imagPart}, backend});
		const result = complex({inputs: {real: r, imag: i}, backend});
		backend.disposeIntermediateTensorInfo(realPart);
		backend.disposeIntermediateTensorInfo(r);
		backend.disposeIntermediateTensorInfo(imagPart);
		backend.disposeIntermediateTensorInfo(i);
		return result;
	} else {
		return fill({backend, attrs: {shape: x.shape, value: 1, dtype: x.dtype}});
	}
}

const onesLikeConfig = {
	kernelName: OnesLike,
	backendName: 'cpu',
	kernelFunc: onesLike
};

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the License);
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an AS IS BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
const step = unaryKernelFunc(Step, (xi, attrs) => {
	const stepAttrs = attrs;
	if (isNaN(xi)) {
		return NaN;
	} else {
		return xi > 0 ? 1 : stepAttrs.alpha;
	}
});
const stepConfig = {
	kernelName: Step,
	backendName: 'cpu',
	kernelFunc: step,
};

/**
 * @license
 * Copyright 2019 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
const squareConfig = {
	kernelName: Square,
	backendName: 'cpu',
	kernelFunc: ({inputs, backend}) => {
		const {x} = inputs;
		const cpuBackend = backend;
		assertNotComplex(x, 'square');
		const values = cpuBackend.data.get(x.dataId).values;
		const newValues = new Float32Array(values.length);
		for (let i = 0; i < values.length; ++i) {
			const value = values[i];
			newValues[i] = value * value;
		}
		const dataId = cpuBackend.write(newValues, x.shape, x.dtype);
		return {dataId, shape: x.shape, dtype: x.dtype};
	}
};

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
const logicalAndImpl = createSimpleBinaryKernelImpl((a, b) => a && b);
const logicalAnd = binaryKernelFunc(LogicalAnd, logicalAndImpl, null /* complexImpl */, 'bool');
const logicalAndConfig = {
	kernelName: LogicalAnd,
	backendName: 'cpu',
	kernelFunc: logicalAnd
};

/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
function select(args) {
	const {inputs, backend} = args;
	const {condition, t, e} = inputs;
	assertNotComplex([condition, t, e], 'select');
	const conditionRank = condition.shape.length;
	const values = backend.data.get(condition.dataId).values;
	const tValues = backend.data.get(t.dataId).values;
	const eValues = backend.data.get(e.dataId).values;
	const resultDtype = upcastType(t.dtype, e.dtype);
	const newValues = makeZerosTypedArray(sizeFromShape(t.shape), resultDtype);
	let index = 0;
	const offset = conditionRank === 0 || conditionRank > 1 || t.shape.length === 1 ?
		1 :
		sizeFromShape(t.shape.slice(1));
	for (let i = 0; i < values.length; i++) {
		for (let j = 0; j < offset; j++) {
			if (values[i] === 1) {
				newValues[index++] = tValues[i];
			} else {
				newValues[index++] = eValues[i];
			}
		}
	}
	return backend.makeTensorInfo(t.shape, resultDtype, newValues);
}

const selectConfig = {
	kernelName: Select,
	backendName: 'cpu',
	kernelFunc: select
};

/**
 * @license
 * Copyright 2025 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */

registerKernel(stringToHashBucketFastConfig);
registerKernel(reshapeConfig);
registerKernel(sliceConfig);
registerKernel(castConfig);
registerKernel(gatherV2Config);
registerKernel(identityConfig);
registerKernel(batchMatMulConfig);
registerKernel(addConfig);
registerKernel(reluConfig);
registerKernel(floorConfig);
registerKernel(realDivConfig);
registerKernel(multiplyConfig);
registerKernel(_fusedMatMulConfig);
registerKernel(sigmoidConfig);
registerKernel(clipByValueConfig);
registerKernel(subConfig);
registerKernel(logConfig);
registerKernel(absConfig);
registerKernel(negConfig);
registerKernel(expConfig);
registerKernel(log1pConfig);
registerKernel(meanConfig);
registerKernel(onesLikeConfig);
registerKernel(greaterConfig);
registerKernel(equalConfig);
registerKernel(stepConfig);
registerKernel(squareConfig);
registerKernel(greaterEqualConfig);
registerKernel(lessEqualConfig);
registerKernel(logicalAndConfig);
registerKernel(zerosLikeConfig);
registerKernel(selectConfig);
registerKernel(sumConfig);
registerKernel(sqrtConfig);

export {
	Abs,
	Acos,
	Acosh,
	AdadeltaOptimizer,
	AdagradOptimizer,
	AdamOptimizer,
	AdamaxOptimizer,
	Add,
	AddN,
	All,
	Any,
	ArgMax,
	ArgMin,
	Asin,
	Asinh,
	Atan,
	Atan2,
	Atanh,
	AvgPool,
	AvgPool3D,
	AvgPool3DGrad,
	AvgPoolGrad,
	BatchMatMul,
	BatchToSpaceND,
	Bincount,
	BitwiseAnd,
	BroadcastArgs,
	BroadcastTo,
	Cast,
	Ceil,
	ClipByValue,
	Complex,
	ComplexAbs,
	Concat,
	Conv2D,
	Conv2DBackpropFilter,
	Conv2DBackpropInput,
	Conv3D,
	Conv3DBackpropFilterV2,
	Conv3DBackpropInputV2,
	Cos,
	Cosh,
	CropAndResize,
	Cumprod,
	Cumsum,
	DataStorage,
	DenseBincount,
	DepthToSpace,
	DepthwiseConv2dNative,
	DepthwiseConv2dNativeBackpropFilter,
	DepthwiseConv2dNativeBackpropInput,
	Diag,
	Dilation2D,
	Dilation2DBackpropFilter,
	Dilation2DBackpropInput,
	Draw,
	ENV$1 as ENV,
	Einsum,
	Elu,
	EluGrad,
	Environment,
	Equal,
	Erf,
	Exp,
	ExpandDims,
	Expm1,
	FFT,
	Fill,
	FlipLeftRight,
	Floor,
	FloorDiv,
	FromPixels,
	FusedBatchNorm,
	FusedConv2D,
	FusedDepthwiseConv2D,
	GatherNd,
	GatherV2,
	Greater,
	GreaterEqual,
	IFFT,
	Identity,
	Imag,
	IsFinite,
	IsInf,
	IsNan,
	KernelBackend,
	LRN,
	LRNGrad,
	LeakyRelu,
	Less,
	LessEqual,
	LinSpace,
	Log,
	Log1p,
	LogSoftmax,
	LogicalAnd,
	LogicalNot,
	LogicalOr,
	LogicalXor,
	LowerBound,
	MathBackendCPU,
	MatrixBandPart,
	Max,
	MaxPool,
	MaxPool3D,
	MaxPool3DGrad,
	MaxPoolGrad,
	MaxPoolWithArgmax,
	Maximum,
	Mean,
	Min,
	Minimum,
	MirrorPad,
	Mod,
	MomentumOptimizer,
	Multinomial,
	Multiply,
	Neg,
	NonMaxSuppressionV3,
	NonMaxSuppressionV4,
	NonMaxSuppressionV5,
	NotEqual,
	OP_SCOPE_SUFFIX,
	OneHot,
	OnesLike,
	Optimizer,
	OptimizerConstructors,
	Pack,
	PadV2,
	Pool,
	Pow,
	Prelu,
	Prod,
	RMSPropOptimizer,
	RaggedGather,
	RaggedRange,
	RaggedTensorToTensor,
	Range,
	Rank,
	Real,
	RealDiv,
	Reciprocal,
	Reduction,
	Relu,
	Relu6,
	Reshape,
	ResizeBilinear,
	ResizeBilinearGrad,
	ResizeNearestNeighbor,
	ResizeNearestNeighborGrad,
	Reverse,
	RotateWithOffset,
	Round,
	Rsqrt,
	SGDOptimizer,
	ScatterNd,
	SearchSorted,
	Select,
	Selu,
	Sigmoid,
	Sign,
	Sin,
	Sinh,
	Slice,
	Softmax,
	Softplus,
	SpaceToBatchND,
	SparseFillEmptyRows,
	SparseReshape,
	SparseSegmentMean,
	SparseSegmentSum,
	SparseToDense,
	SplitV,
	Sqrt,
	Square,
	SquaredDifference,
	StaticRegexReplace,
	Step,
	StridedSlice,
	StringNGrams,
	StringSplit,
	StringToHashBucketFast,
	Sub,
	Sum,
	Tan,
	Tanh,
	Tensor,
	TensorBuffer,
	TensorScatterUpdate,
	Tile,
	TopK,
	Transform,
	Transpose,
	Unique,
	Unpack,
	UnsortedSegmentSum,
	UpperBound,
	Variable,
	ZerosLike,
	_FusedMatMul,
	abs$1 as abs,
	acos,
	acosh,
	add$1 as add,
	addN,
	all,
	any,
	argMax,
	argMin,
	asin,
	asinh,
	atan,
	atan2,
	atanh,
	avgPool,
	avgPool3d,
	backend,
	backend_util,
	basicLSTMCell,
	batchNorm,
	batchNorm2d,
	batchNorm3d,
	batchNorm4d,
	batchToSpaceND,
	bincount,
	bitwiseAnd,
	booleanMaskAsync,
	broadcastArgs,
	broadcastTo,
	broadcast_util,
	browser,
	buffer,
	cast$1 as cast,
	ceil,
	clipByValue$1 as clipByValue,
	clone,
	complex$1 as complex,
	concat,
	concat1d,
	concat2d,
	concat3d,
	concat4d,
	conv1d,
	conv2d$1 as conv2d,
	conv2dTranspose,
	conv3d,
	conv3dTranspose,
	copyRegisteredKernels,
	cos,
	cosh,
	cosineWindow,
	cumprod,
	cumsum,
	customGrad,
	denseBincount,
	deprecationWarn,
	depthToSpace,
	depthwiseConv2d$1 as depthwiseConv2d,
	device_util,
	diag,
	dilation2d,
	disableDeprecationWarnings,
	dispose,
	disposeVariables,
	div$1 as div,
	divNoNan,
	dot,
	dropout,
	einsum,
	elu$1 as elu,
	enableDebugMode,
	enableProdMode,
	enclosingPowerOfTwo,
	engine,
	ensureShape,
	env,
	equal$1 as equal,
	erf,
	euclideanNorm,
	exp$1 as exp,
	expandDims,
	expm1,
	eye,
	fft,
	fill$1 as fill,
	findBackend,
	findBackendFactory,
	floor$1 as floor,
	floorDiv,
	fused_ops as fused,
	gather,
	gatherND,
	gather_nd_util as gather_util,
	getBackend,
	getGradient,
	getKernel,
	getKernelsForBackend,
	grad,
	grads,
	greater$1 as greater,
	greaterEqual$1 as greaterEqual,
	ifft,
	imag$1 as imag,
	image,
	inTopKAsync,
	io,
	irfft,
	isFinite$1 as isFinite,
	isInf,
	isNaN$1 as isNaN,
	keep,
	kernel_impls,
	leakyRelu$1 as leakyRelu,
	less,
	lessEqual$1 as lessEqual,
	linalg,
	linspace,
	localResponseNormalization,
	log$1 as log,
	log1p$1 as log1p,
	logSigmoid,
	logSoftmax,
	logSumExp,
	logicalAnd$1 as logicalAnd,
	logicalNot,
	logicalOr,
	logicalXor,
	losses,
	lowerBound,
	matMul$1 as matMul,
	math,
	max,
	maxPool,
	maxPool3d,
	maxPoolWithArgmax,
	maximum,
	mean$1 as mean,
	memory,
	meshgrid,
	min,
	minimum,
	mirrorPad,
	mod,
	moments,
	movingAverage,
	mul,
	multiRNNCell,
	multinomial,
	neg$1 as neg,
	nextFrame,
	norm,
	notEqual,
	oneHot,
	ones,
	onesLike$1 as onesLike,
	op,
	outerProduct,
	pad,
	pad1d,
	pad2d,
	pad3d,
	pad4d,
	pool,
	pow,
	prelu$1 as prelu,
	print,
	prod,
	profile,
	raggedGather,
	raggedRange,
	raggedTensorToTensor,
	rand,
	randomGamma,
	randomNormal,
	randomStandardNormal,
	randomUniform,
	randomUniformInt,
	range,
	ready,
	real$1 as real,
	reciprocal,
	registerBackend,
	registerGradient,
	registerKernel,
	relu$1 as relu,
	relu6$1 as relu6,
	removeBackend,
	reshape$1 as reshape,
	reverse,
	reverse1d,
	reverse2d,
	reverse3d,
	reverse4d,
	rfft,
	round,
	rsqrt,
	scalar,
	scatterND,
	scatter_nd_util as scatter_util,
	searchSorted,
	selu,
	separableConv2d,
	serialization,
	setBackend,
	setPlatform,
	setdiff1dAsync,
	shared,
	sigmoid$1 as sigmoid,
	sign,
	signal,
	sin,
	sinh,
	slice$1 as slice,
	slice1d,
	slice2d,
	slice3d,
	slice4d,
	slice_util,
	softmax,
	softplus,
	spaceToBatchND,
	sparse,
	sparseToDense,
	spectral,
	split$1 as split,
	sqrt$1 as sqrt,
	square,
	squaredDifference,
	squeeze,
	stack,
	step$1 as step,
	stridedSlice,
	string,
	sub$1 as sub,
	sum$1 as sum,
	sumOutType,
	tan,
	tanh,
	tensor,
	tensor1d,
	tensor2d,
	tensor3d,
	tensor4d,
	tensor5d,
	tensor6d,
	tensorScatterUpdate,
	tensor_util,
	test_util,
	tidy,
	tile,
	time,
	topk,
	train,
	transpose$1 as transpose,
	truncatedNormal,
	unique,
	unregisterGradient,
	unregisterKernel,
	unsortedSegmentSum,
	unstack,
	upcastType,
	upperBound,
	util,
	valueAndGrad,
	valueAndGrads,
	variable,
	variableGrads,
	version$1 as version_core,
	version as version_cpu,
	where,
	whereAsync,
	zeros$1 as zeros,
	zerosLike$1 as zerosLike
};
